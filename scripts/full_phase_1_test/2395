Positive 2395 pI am trying to setup a develop environment for a Spark Streaming project which requires write data into Hive. I have a cluster with 1 master, 2 slaves and 1 develop machine coding in Intellij Idea 14.p  pWithin the spark shell, everything seems working fine and I am able to store data into default database in Hive via Spark 1.5 using DataFrame.write.insertIntotesttablep  pHowever when creating a scala project in IDEA and run it using same cluster with same setting, Error was thrown when creating transactional connection factory in the metastore database which suppose to be metastoredb in mysql.p  pstrongHeres the hivesite.xmlstrongp  precodeltconfigurationgt ltpropertygt   ltnamegthive.metastore.urisltnamegt   ltvaluegtthrift10.1.50.739083ltvaluegt ltpropertygt ltpropertygt   ltnamegthive.metastore.warehouse.dirltnamegt   ltvaluegtuserhivewarehouseltvaluegt ltpropertygt ltpropertygt   ltnamegtjavax.jdo.option.ConnectionURLltnamegt   ltvaluegtjdbcmysql10.1.50.733306metastoredbcreateDatabaseIfNotExisttrueltvaluegt ltpropertygt ltpropertygt   ltnamegtjavax.jdo.option.ConnectionDriverNameltnamegt   ltvaluegtcom.mysql.jdbc.Driverltvaluegt ltpropertygt ltpropertygt   ltnamegtjavax.jdo.option.ConnectionUserNameltnamegt   ltvaluegtsaltvaluegt ltpropertygt ltpropertygt   ltnamegtjavax.jdo.option.ConnectionPasswordltnamegt   ltvaluegthuanhuanltvaluegt ltpropertygt ltconfigurationgt codepre  pthe machine which I was running IDEA, can remotely login Mysql and Hive to create tables, so there should have no problem with permissions.  Heres the log4j Outputp  precodegt homestdevelopjdk1.7.079binjava Didea.launcher.port7536 Didea.launcher.bin.pathhomestdevelopideabin Dfile.encodingUTF8 classpath homestdevelopjdk1.7.079jrelibplugin.jarhomestdevelopjdk1.7.079jrelibdeploy.jarhomestdevelopjdk1.7.079jrelibjfxrt.jarhomestdevelopjdk1.7.079jrelibcharsets.jarhomestdevelopjdk1.7.079jrelibjavaws.jarhomestdevelopjdk1.7.079jrelibjfr.jarhomestdevelopjdk1.7.079jrelibjce.jarhomestdevelopjdk1.7.079jrelibjsse.jarhomestdevelopjdk1.7.079jrelibrt.jarhomestdevelopjdk1.7.079jrelibresources.jarhomestdevelopjdk1.7.079jrelibmanagementagent.jarhomestdevelopjdk1.7.079jrelibextzipfs.jarhomestdevelopjdk1.7.079jrelibextsunec.jarhomestdevelopjdk1.7.079jrelibextsunpkcs11.jarhomestdevelopjdk1.7.079jrelibextsunjceprovider.jarhomestdevelopjdk1.7.079jrelibextlocaledata.jarhomestdevelopjdk1.7.079jrelibextdnsns.jarhomestdevelopIdeaProjectsStreamingIntoHivetargetscala2.10classesroot.sbtbootscala2.10.4libscalalibrary.jarhomestdevelopSparkDllsparkassembly1.5.0hadoop2.5.2.jarhomestdevelopSparkDlldatanucleusapijdo3.2.6.jarhomestdevelopSparkDlldatanucleuscore3.2.10.jarhomestdevelopSparkDlldatanucleusrdbms3.2.9.jarhomestdevelopSparkDllmysqlconnectorjava5.1.35bin.jarhomestdevelopidealibideart.jar com.intellij.rt.execution.application.AppMain StreamingIntoHive 10.1.50.68 8080 Using Sparks default log4j profile orgapachesparklog4jdefaults.properties 150922 194318 INFO SparkContext Running Spark version 1.5.0 150922 194321 WARN NativeCodeLoader Unable to load nativehadoop library for your platform... using builtinjava classes where applicable 150922 194322 INFO SecurityManager Changing view acls to root 150922 194322 INFO SecurityManager Changing modify acls to root 150922 194322 INFO SecurityManager SecurityManager authentication disabled ui acls disabled users with view permissions Setroot users with modify permissions Setroot 150922 194326 INFO Slf4jLogger Slf4jLogger started 150922 194326 INFO Remoting Starting remoting 150922 194326 INFO Remoting Remoting started listening on addresses [akka.tcpsparkDriver10.1.50.6858070] 150922 194326 INFO Utils Successfully started service sparkDriver on port 58070. 150922 194326 INFO SparkEnv Registering MapOutputTracker 150922 194326 INFO SparkEnv Registering BlockManagerMaster 150922 194326 INFO DiskBlockManager Created local directory at tmpblockmgre7fdc896ebd24faaa9fee61bd93a9db4 150922 194326 INFO MemoryStore MemoryStore started with capacity 797.6 MB 150922 194327 INFO HttpFileServer HTTP File server directory is tmpsparkfb07a3ad807749a8bcaf12254cc90282httpd0bb434c9141849b6a51490e27cb80ab1 150922 194327 INFO HttpServer Starting HTTP Server 150922 194327 INFO Utils Successfully started service HTTP file server on port 38865. 150922 194327 INFO SparkEnv Registering OutputCommitCoordinator 150922 194329 INFO Utils Successfully started service SparkUI on port 4040. 150922 194329 INFO SparkUI Started SparkUI at http10.1.50.684040 150922 194329 INFO SparkContext Added JAR homestdevelopSparkDllmysqlconnectorjava5.1.35bin.jar at http10.1.50.6838865jarsmysqlconnectorjava5.1.35bin.jar with timestamp 1442922209496 150922 194329 INFO SparkContext Added JAR homestdevelopSparkDlldatanucleusapijdo3.2.6.jar at http10.1.50.6838865jarsdatanucleusapijdo3.2.6.jar with timestamp 1442922209498 150922 194329 INFO SparkContext Added JAR homestdevelopSparkDlldatanucleusrdbms3.2.9.jar at http10.1.50.6838865jarsdatanucleusrdbms3.2.9.jar with timestamp 1442922209534 150922 194329 INFO SparkContext Added JAR homestdevelopSparkDlldatanucleuscore3.2.10.jar at http10.1.50.6838865jarsdatanucleuscore3.2.10.jar with timestamp 1442922209564 150922 194330 WARN MetricsSystem Using default name DAGScheduler for source because spark.app.id is not set. 150922 194330 INFO AppClientClientEndpoint Connecting to master spark10.1.50.717077... 150922 194332 INFO SparkDeploySchedulerBackend Connected to Spark cluster with app ID app201509220626540004 150922 194332 INFO AppClientClientEndpoint Executor added app2015092206265400040 on worker2015092119145810.1.50.7144716 10.1.50.7144716 with 1 cores 150922 194332 INFO SparkDeploySchedulerBackend Granted executor ID app2015092206265400040 on hostPort 10.1.50.7144716 with 1 cores, 1024.0 MB RAM 150922 194332 INFO AppClientClientEndpoint Executor added app2015092206265400041 on worker2015092119145610.1.50.7336446 10.1.50.7336446 with 1 cores 150922 194332 INFO SparkDeploySchedulerBackend Granted executor ID app2015092206265400041 on hostPort 10.1.50.7336446 with 1 cores, 1024.0 MB RAM 150922 194332 INFO AppClientClientEndpoint Executor added app2015092206265400042 on worker2015092119145610.1.50.7253999 10.1.50.7253999 with 1 cores 150922 194332 INFO SparkDeploySchedulerBackend Granted executor ID app2015092206265400042 on hostPort 10.1.50.7253999 with 1 cores, 1024.0 MB RAM 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400041 is now LOADING 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400040 is now LOADING 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400042 is now LOADING 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400040 is now RUNNING 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400041 is now RUNNING 150922 194332 INFO AppClientClientEndpoint Executor updated app2015092206265400042 is now RUNNING 150922 194333 INFO Utils Successfully started service org.apache.spark.network.netty.NettyBlockTransferService on port 60161. 150922 194333 INFO NettyBlockTransferService Server created on 60161 150922 194333 INFO BlockManagerMaster Trying to register BlockManager 150922 194333 INFO BlockManagerMasterEndpoint Registering block manager 10.1.50.6860161 with 797.6 MB RAM, BlockManagerIddriver, 10.1.50.68, 60161 150922 194333 INFO BlockManagerMaster Registered BlockManager 150922 194334 INFO SparkDeploySchedulerBackend SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio 0.0 150922 194335 INFO SparkContext Added JAR homestdevelopBuildsstreamingintohive.jar at http10.1.50.6838865jarsstreamingintohive.jar with timestamp 1442922215169 150922 194339 INFO SparkDeploySchedulerBackend Registered executor AkkaRpcEndpointRefActor[akka.tcpsparkExecutor10.1.50.7240110userExecutor132020084] with ID 2 150922 194339 INFO SparkDeploySchedulerBackend Registered executor AkkaRpcEndpointRefActor[akka.tcpsparkExecutor10.1.50.7138248userExecutor1615730727] with ID 0 150922 194340 INFO BlockManagerMasterEndpoint Registering block manager 10.1.50.7237819 with 534.5 MB RAM, BlockManagerId2, 10.1.50.72, 37819 150922 194340 INFO BlockManagerMasterEndpoint Registering block manager 10.1.50.7148028 with 534.5 MB RAM, BlockManagerId0, 10.1.50.71, 48028 150922 194342 INFO HiveContext Initializing execution hive, version 1.2.1 150922 194342 INFO ClientWrapper Inspected Hadoop version 2.5.2 150922 194342 INFO ClientWrapper Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.5.2 150922 194342 INFO SparkDeploySchedulerBackend Registered executor AkkaRpcEndpointRefActor[akka.tcpsparkExecutor10.1.50.7356385userExecutor1871695565] with ID 1 150922 194343 INFO BlockManagerMasterEndpoint Registering block manager 10.1.50.7343643 with 534.5 MB RAM, BlockManagerId1, 10.1.50.73, 43643 150922 194345 INFO HiveMetaStore 0 Opening raw store with implemenation classorg.apache.hadoop.hive.metastore.ObjectStore 150922 194345 INFO ObjectStore ObjectStore, initialize called 150922 194347 INFO Persistence Property datanucleus.cache.level2 unknown  will be ignored 150922 194347 INFO Persistence Property hive.metastore.integral.jdo.pushdown unknown  will be ignored 150922 194347 WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies 150922 194348 WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies 150922 194358 INFO ObjectStore Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypesTable,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order 150922 194403 INFO Datastore The class org.apache.hadoop.hive.metastore.model.MFieldSchema is tagged as embeddedonly so does not have its own datastore table. 150922 194403 INFO Datastore The class org.apache.hadoop.hive.metastore.model.MOrder is tagged as embeddedonly so does not have its own datastore table. 150922 194410 INFO Datastore The class org.apache.hadoop.hive.metastore.model.MFieldSchema is tagged as embeddedonly so does not have its own datastore table. 150922 194410 INFO Datastore The class org.apache.hadoop.hive.metastore.model.MOrder is tagged as embeddedonly so does not have its own datastore table. 150922 194412 INFO MetaStoreDirectSql Using direct SQL, underlying DB is DERBY 150922 194412 INFO ObjectStore Initialized ObjectStore 150922 194413 WARN ObjectStore Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0 150922 194414 WARN ObjectStore Failed to get database default, returning NoSuchObjectException 150922 194415 INFO HiveMetaStore Added admin role in metastore 150922 194415 INFO HiveMetaStore Added public role in metastore 150922 194416 INFO HiveMetaStore No user is added in admin role, since config is empty 150922 194416 INFO HiveMetaStore 0 getalldatabases 150922 194416 INFO audit ugiroot  ipunknownipaddr  cmdgetalldatabases    150922 194417 INFO HiveMetaStore 0 getfunctions dbdefault pat 150922 194417 INFO audit ugiroot  ipunknownipaddr  cmdgetfunctions dbdefault pat  150922 194417 INFO Datastore The class org.apache.hadoop.hive.metastore.model.MResourceUri is tagged as embeddedonly so does not have its own datastore table. 150922 194418 INFO SessionState Created local directory tmp9ee94679df5146bcbf6f66b19f053823resources 150922 194418 INFO SessionState Created HDFS directory tmphiveroot9ee94679df5146bcbf6f66b19f053823 150922 194418 INFO SessionState Created local directory tmproot9ee94679df5146bcbf6f66b19f053823 150922 194418 INFO SessionState Created HDFS directory tmphiveroot9ee94679df5146bcbf6f66b19f053823tmpspace.db 150922 194419 INFO HiveContext default warehouse location is userhivewarehouse 150922 194419 INFO HiveContext Initializing HiveMetastoreConnection version 1.2.1 using Spark classes. 150922 194419 INFO ClientWrapper Inspected Hadoop version 2.5.2 150922 194419 INFO ClientWrapper Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.5.2 150922 194422 WARN NativeCodeLoader Unable to load nativehadoop library for your platform... using builtinjava classes where applicable 150922 194422 INFO HiveMetaStore 0 Opening raw store with implemenation classorg.apache.hadoop.hive.metastore.ObjectStore 150922 194422 INFO ObjectStore ObjectStore, initialize called 150922 194423 INFO Persistence Property datanucleus.cache.level2 unknown  will be ignored 150922 194423 INFO Persistence Property hive.metastore.integral.jdo.pushdown unknown  will be ignored 150922 194423 WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies 150922 194425 WARN HiveMetaStore Retrying creating default database after error Error creating transactional connection factory javax.jdo.JDOFatalInternalException Error creating transactional connection factory     at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusExceptionNucleusJDOHelper.java587     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfigurationJDOPersistenceManagerFactory.java788     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactoryJDOPersistenceManagerFactory.java333     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactoryJDOPersistenceManagerFactory.java202     at sun.reflect.NativeMethodAccessorImpl.invoke0Native Method     at sun.reflect.NativeMethodAccessorImpl.invokeNativeMethodAccessorImpl.java57     at sun.reflect.DelegatingMethodAccessorImpl.invokeDelegatingMethodAccessorImpl.java43     at java.lang.reflect.Method.invokeMethod.java606     at javax.jdo.JDOHelper16.runJDOHelper.java1965     at java.security.AccessController.doPrivilegedNative Method     at javax.jdo.JDOHelper.invokeJDOHelper.java1960     at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementationJDOHelper.java1166     at javax.jdo.JDOHelper.getPersistenceManagerFactoryJDOHelper.java808     at javax.jdo.JDOHelper.getPersistenceManagerFactoryJDOHelper.java701     at org.apache.hadoop.hive.metastore.ObjectStore.getPMFObjectStore.java365     at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManagerObjectStore.java394     at org.apache.hadoop.hive.metastore.ObjectStore.initializeObjectStore.java291     at org.apache.hadoop.hive.metastore.ObjectStore.setConfObjectStore.java258     at org.apache.hadoop.util.ReflectionUtils.setConfReflectionUtils.java73     at org.apache.hadoop.util.ReflectionUtils.newInstanceReflectionUtils.java133     at org.apache.hadoop.hive.metastore.RawStoreProxy.ltinitgtRawStoreProxy.java57     at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxyRawStoreProxy.java66     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.newRawStoreHiveMetaStore.java593     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.getMSHiveMetaStore.java571     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.createDefaultDBHiveMetaStore.java620     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.initHiveMetaStore.java461     at org.apache.hadoop.hive.metastore.RetryingHMSHandler.ltinitgtRetryingHMSHandler.java66     at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxyRetryingHMSHandler.java72     at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandlerHiveMetaStore.java5762     at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.ltinitgtHiveMetaStoreClient.java199     at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.ltinitgtSessionHiveMetaStoreClient.java74     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstanceMetaStoreUtils.java1521     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.ltinitgtRetryingMetaStoreClient.java86     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxyRetryingMetaStoreClient.java132     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxyRetryingMetaStoreClient.java104     at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClientHive.java3005     at org.apache.hadoop.hive.ql.metadata.Hive.getMSCHive.java3024     at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabasesHive.java1234     at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctionsHive.java174     at org.apache.hadoop.hive.ql.metadata.Hive.ltclinitgtHive.java166     at org.apache.hadoop.hive.ql.session.SessionState.startSessionState.java503     at org.apache.spark.sql.hive.client.ClientWrapper.ltinitgtClientWrapper.scala171     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree11IsolatedClientLoader.scala183     at org.apache.spark.sql.hive.client.IsolatedClientLoader.ltinitgtIsolatedClientLoader.scala179     at org.apache.spark.sql.hive.HiveContext.metadataHivelzycomputeHiveContext.scala227     at org.apache.spark.sql.hive.HiveContext.metadataHiveHiveContext.scala186     at org.apache.spark.sql.hive.HiveContext.setConfHiveContext.scala393     at org.apache.spark.sql.hive.HiveContext.defaultOverridesHiveContext.scala175     at org.apache.spark.sql.hive.HiveContext.ltinitgtHiveContext.scala178     at StreamingIntoHive.mainStreamingIntoHive.scala42     at StreamingIntoHive.mainStreamingIntoHive.scala     at sun.reflect.NativeMethodAccessorImpl.invoke0Native Method     at sun.reflect.NativeMethodAccessorImpl.invokeNativeMethodAccessorImpl.java57     at sun.reflect.DelegatingMethodAccessorImpl.invokeDelegatingMethodAccessorImpl.java43     at java.lang.reflect.Method.invokeMethod.java606     at com.intellij.rt.execution.application.AppMain.mainAppMain.java140 NestedThrowablesStackTrace java.lang.reflect.InvocationTargetException     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtensionNonManagedPluginRegistry.java631     at org.datanucleus.plugin.PluginManager.createExecutableExtensionPluginManager.java325     at org.datanucleus.store.AbstractStoreManager.registerConnectionFactoryAbstractStoreManager.java282     at org.datanucleus.store.AbstractStoreManager.ltinitgtAbstractStoreManager.java240     at org.datanucleus.store.rdbms.RDBMSStoreManager.ltinitgtRDBMSStoreManager.java286     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtensionNonManagedPluginRegistry.java631     at org.datanucleus.plugin.PluginManager.createExecutableExtensionPluginManager.java301     at org.datanucleus.NucleusContext.createStoreManagerForPropertiesNucleusContext.java1187     at org.datanucleus.NucleusContext.initialiseNucleusContext.java356     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfigurationJDOPersistenceManagerFactory.java775     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactoryJDOPersistenceManagerFactory.java333     at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactoryJDOPersistenceManagerFactory.java202     at sun.reflect.NativeMethodAccessorImpl.invoke0Native Method     at sun.reflect.NativeMethodAccessorImpl.invokeNativeMethodAccessorImpl.java57     at sun.reflect.DelegatingMethodAccessorImpl.invokeDelegatingMethodAccessorImpl.java43     at java.lang.reflect.Method.invokeMethod.java606     at javax.jdo.JDOHelper16.runJDOHelper.java1965     at java.security.AccessController.doPrivilegedNative Method     at javax.jdo.JDOHelper.invokeJDOHelper.java1960     at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementationJDOHelper.java1166     at javax.jdo.JDOHelper.getPersistenceManagerFactoryJDOHelper.java808     at javax.jdo.JDOHelper.getPersistenceManagerFactoryJDOHelper.java701     at org.apache.hadoop.hive.metastore.ObjectStore.getPMFObjectStore.java365     at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManagerObjectStore.java394     at org.apache.hadoop.hive.metastore.ObjectStore.initializeObjectStore.java291     at org.apache.hadoop.hive.metastore.ObjectStore.setConfObjectStore.java258     at org.apache.hadoop.util.ReflectionUtils.setConfReflectionUtils.java73     at org.apache.hadoop.util.ReflectionUtils.newInstanceReflectionUtils.java133     at org.apache.hadoop.hive.metastore.RawStoreProxy.ltinitgtRawStoreProxy.java57     at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxyRawStoreProxy.java66     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.newRawStoreHiveMetaStore.java593     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.getMSHiveMetaStore.java571     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.createDefaultDBHiveMetaStore.java620     at org.apache.hadoop.hive.metastore.HiveMetaStoreHMSHandler.initHiveMetaStore.java461     at org.apache.hadoop.hive.metastore.RetryingHMSHandler.ltinitgtRetryingHMSHandler.java66     at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxyRetryingHMSHandler.java72     at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandlerHiveMetaStore.java5762     at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.ltinitgtHiveMetaStoreClient.java199     at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.ltinitgtSessionHiveMetaStoreClient.java74     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstanceMetaStoreUtils.java1521     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.ltinitgtRetryingMetaStoreClient.java86     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxyRetryingMetaStoreClient.java132     at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxyRetryingMetaStoreClient.java104     at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClientHive.java3005     at org.apache.hadoop.hive.ql.metadata.Hive.getMSCHive.java3024     at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabasesHive.java1234     at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctionsHive.java174     at org.apache.hadoop.hive.ql.metadata.Hive.ltclinitgtHive.java166     at org.apache.hadoop.hive.ql.session.SessionState.startSessionState.java503     at org.apache.spark.sql.hive.client.ClientWrapper.ltinitgtClientWrapper.scala171     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree11IsolatedClientLoader.scala183     at org.apache.spark.sql.hive.client.IsolatedClientLoader.ltinitgtIsolatedClientLoader.scala179     at org.apache.spark.sql.hive.HiveContext.metadataHivelzycomputeHiveContext.scala227     at org.apache.spark.sql.hive.HiveContext.metadataHiveHiveContext.scala186     at org.apache.spark.sql.hive.HiveContext.setConfHiveContext.scala393     at org.apache.spark.sql.hive.HiveContext.defaultOverridesHiveContext.scala175     at org.apache.spark.sql.hive.HiveContext.ltinitgtHiveContext.scala178     at StreamingIntoHive.mainStreamingIntoHive.scala42     at StreamingIntoHive.mainStreamingIntoHive.scala     at sun.reflect.NativeMethodAccessorImpl.invoke0Native Method     at sun.reflect.NativeMethodAccessorImpl.invokeNativeMethodAccessorImpl.java57     at sun.reflect.DelegatingMethodAccessorImpl.invokeDelegatingMethodAccessorImpl.java43     at java.lang.reflect.Method.invokeMethod.java606     at com.intellij.rt.execution.application.AppMain.mainAppMain.java140 Caused by java.lang.OutOfMemoryError PermGen space     at java.lang.ClassLoader.defineClass1Native Method     at java.lang.ClassLoader.defineClassClassLoader.java800     at java.security.SecureClassLoader.defineClassSecureClassLoader.java142     at java.net.URLClassLoader.defineClassURLClassLoader.java449     at java.net.URLClassLoader.access100URLClassLoader.java71     at java.net.URLClassLoader1.runURLClassLoader.java361     at java.net.URLClassLoader1.runURLClassLoader.java355     at java.security.AccessController.doPrivilegedNative Method     at java.net.URLClassLoader.findClassURLClassLoader.java354     at java.lang.ClassLoader.loadClassClassLoader.java425     at org.apache.spark.sql.hive.client.IsolatedClientLoaderanon1.doLoadClassIsolatedClientLoader.scala165     at org.apache.spark.sql.hive.client.IsolatedClientLoaderanon1.loadClassIsolatedClientLoader.scala153     at java.lang.ClassLoader.loadClassClassLoader.java358     at org.datanucleus.store.rdbms.connectionpool.DBCPConnectionPoolFactory.createConnectionPoolDBCPConnectionPoolFactory.java59     at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSourcesConnectionFactoryImpl.java238     at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSourcesConnectionFactoryImpl.java131     at org.datanucleus.store.rdbms.ConnectionFactoryImpl.ltinitgtConnectionFactoryImpl.java85     at sun.reflect.NativeConstructorAccessorImpl.newInstance0Native Method     at sun.reflect.NativeConstructorAccessorImpl.newInstanceNativeConstructorAccessorImpl.java57     at sun.reflect.DelegatingConstructorAccessorImpl.newInstanceDelegatingConstructorAccessorImpl.java45     at java.lang.reflect.Constructor.newInstanceConstructor.java526     at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtensionNonManagedPluginRegistry.java631     at org.datanucleus.plugin.PluginManager.createExecutableExtensionPluginManager.java325     at org.datanucleus.store.AbstractStoreManager.registerConnectionFactoryAbstractStoreManager.java282     at org.datanucleus.store.AbstractStoreManager.ltinitgtAbstractStoreManager.java240   Process finished with exit code 1 codepre  p Can anyone help me to find out the reason Thanks.p
Positive 2395 pInstalled Spark 1.5 spark1.5.0binhadoop2.6 on my local machine. Ran   .binsparkshell Tried, following the doc at a hrefhttpspark.apache.orgdocslatestsqlprogrammingguide.htmldatasources relnofollowhttpspark.apache.orgdocslatestsqlprogrammingguide.htmldatasourcesa to create a table, getting thisp  precodegt SQL context available as sqlContext. gt  gt scalagt sqlContext.sqlCREATE TABLE IF NOT EXISTS src key INT, value gt STRING 150922 221813 ERROR DDLTask gt org.apache.hadoop.hive.ql.metadata.HiveException gt MetaExceptionmessagefileuserhivewarehousesrc is not a directory gt or unable to create one  at gt org.apache.hadoop.hive.ql.metadata.Hive.createTableHive.java720 codepre  pTried passing the hive parameter for this, but didnt workp  precodegt   .binsparkshell conf hive.metastore.warehouse.dir. Warning gt Ignoring nonspark config property hive.metastore.warehouse.dir. codepre  pFinally tried the CLI itself, but getting the same issue. Where do i change the hive warehouse parameter location  I dont have Hadoop installed at the moment, nor hive.p  pthanks, Mattp
Positive 2395 pI written a Java code to access Apache Hive tables.p  precodeimport java.sql.  public class HiveQL       private static String drivername  org.apache.hive.jdbc.HiveDriver      public static void mainString[] args throws SQLException              try         Class.forNamedrivername         catchClassNotFoundException e                      e.printStackTrace             System.exit1                  Connection con  DriverManager.getConnectionjdbchive2xxx.xxx.xxx.xxx10000db, ,           Statement stmt  con.createStatement          ResultSet res  stmt.executeQueryselect distinct ClientName from leadgenmain          System.out.printlnOutput          whileres.next                      System.out.printlnres.getString1                  con.close       codepre  pFor the above program I am getting the following errorp  precodelog4jWARN No appenders could be found for logger org.apache.hive.jdbc.Utils. log4jWARN Please initialize the log4j system properly. log4jWARN See httplogging.apache.orglog4j1.2faq.htmlnoconfig for more info. SLF4J Failed to load class org.slf4j.impl.StaticLoggerBinder. SLF4J Defaulting to nooperation NOP logger implementation SLF4J See httpwww.slf4j.orgcodes.htmlStaticLoggerBinder for further details. Exception in thread main java.sql.SQLException Error while processing statement FAILED Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask     at org.apache.hive.jdbc.HiveStatement.executeHiveStatement.java279     at org.apache.hive.jdbc.HiveStatement.executeQueryHiveStatement.java375     at com.dwi.java.hive.HiveQL.mainHiveQL.java22 codepre  pFor normal queries which are not required mapreduce, are working fine, But when i used this type queries, which perform mapreduce tasks, i am getting the above error.p  pPlease help on this.p
Positive 2395 pI have a Hive table in parquet format that was generated usingp  precodecreate table myTable var1 int, var2 string, var3 int, var4 string, var5 arrayltstructltaint,bstringgtgt stored as parquet codepre  pI am able to verify that it was filled  here is a sample valuep  precode[1, abcdef, 2, ghijkl, ArrayBuffer[1, hello]] codepre  pI wish to put this into a Spark RDD of the formp  precode1,abcdef, 2,ghijkl, Set1,hello codepre  pNow, using sparkshell I get the same problem in sparksubmit, I made a test RDD with these valuesp  precodescalagt val tempRDD  sc.parallelizeSeq1,abcdef,2,ghijkl, ArrayBuffer[Int,String]1,hello tempRDD org.apache.spark.rdd.RDD[Int, String, Int, String, scala.collection.mutable.ArrayBuffer[Int, String]]  ParallelCollectionRDD[44] at parallelize at ltconsolegt85 codepre  pusing an iterator, I can cast the ArrayBuffer as a HashSet in the following new RDDp  precodescalagt val tempRDD2  tempRDD.mapa gt a.1, a.2.1,  var tempHashSet  new HashSet[Int,String] a.2.2.foreacha gt tempHashSet  tempHashSet  HashSeta tempHashSet   tempRDD2 org.apache.spark.rdd.RDD[Int, String, Int, String, scala.collection.immutable.HashSet[Int, String]]  MapPartitionsRDD[46] at map at ltconsolegt87  scalagt tempRDD2.collect.foreachprintln 1,abcdef,2,ghijkl,Set1,hello codepre  pBut when I attempt to do the EXACT SAME THING with a DataFrame with a HiveContext  SQLContext, I get the following errorp  precodescalagt val hc  new HiveContextsc scalagt import hc. scalagt import hc.implicits.  scalagt val tempHiveQL  hc.sqlselect var1, var2, var3, var4, var5 from myTable  scalagt val tempRDDfromHive  tempHiveQL.mapa gt a0.toString.toInt, a1.toString, a2.toString.toInt, a3.toString, a4.asInstanceOf[ArrayBuffer[Int,String]]   scalagt val tempRDD3  tempRDDfromHive.mapa gt a.1, a.2.1,  var tempHashSet  new HashSet[Int,String] a.2.2.foreacha gt tempHashSet  tempHashSet  HashSeta tempHashSet   tempRDD3 org.apache.spark.rdd.RDD[Int, String, Int, String, scala.collection.immutable.HashSet[Int, String]]  MapPartitionsRDD[47] at map at ltconsolegt91  scalagt tempRDD3.collect.foreachprintln org.apache.spark.SparkException Job aborted due to stage failure Task 1 in stage 14.0 failed 1 times, most recent failure Lost task 1.0 in stage 14.0 TID 5211, localhost java.lang.ClassCastException org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to scala.Tuple2        at iwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCanonfun1anonfunapply1.applyltconsolegt91        at scala.collection.mutable.ResizableArrayclass.foreachResizableArray.scala59        at scala.collection.mutable.ArrayBuffer.foreachArrayBuffer.scala47        at iwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCanonfun1.applyltconsolegt91        at iwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCiwCanonfun1.applyltconsolegt91        at scala.collection.Iteratoranon11.nextIterator.scala328        at scala.collection.Iteratorclass.foreachIterator.scala727        at scala.collection.AbstractIterator.foreachIterator.scala1157        at scala.collection.generic.Growableclass.pluspluseqGrowable.scala48        at scala.collection.mutable.ArrayBuffer.pluspluseqArrayBuffer.scala103        at scala.collection.mutable.ArrayBuffer.pluspluseqArrayBuffer.scala47        at scala.collection.TraversableOnceclass.toTraversableOnce.scala273        at scala.collection.AbstractIterator.toIterator.scala1157        at scala.collection.TraversableOnceclass.toBufferTraversableOnce.scala265        at scala.collection.AbstractIterator.toBufferIterator.scala1157        at scala.collection.TraversableOnceclass.toArrayTraversableOnce.scala252        at scala.collection.AbstractIterator.toArrayIterator.scala1157        at org.apache.spark.rdd.RDDanonfun17.applyRDD.scala813        at org.apache.spark.rdd.RDDanonfun17.applyRDD.scala813        at org.apache.spark.SparkContextanonfunrunJob5.applySparkContext.scala1503        at org.apache.spark.SparkContextanonfunrunJob5.applySparkContext.scala1503        at org.apache.spark.scheduler.ResultTask.runTaskResultTask.scala61        at org.apache.spark.scheduler.Task.runTask.scala64        at org.apache.spark.executor.ExecutorTaskRunner.runExecutor.scala203        at java.util.concurrent.ThreadPoolExecutor.runWorkerThreadPoolExecutor.java1145        at java.util.concurrent.ThreadPoolExecutorWorker.runThreadPoolExecutor.java615        at java.lang.Thread.runThread.java724  Driver stacktrace        at org.apache.spark.scheduler.DAGScheduler.orgapachesparkschedulerDAGSchedulerfailJobAndIndependentStagesDAGScheduler.scala1203        at org.apache.spark.scheduler.DAGScheduleranonfunabortStage1.applyDAGScheduler.scala1192        at org.apache.spark.scheduler.DAGScheduleranonfunabortStage1.applyDAGScheduler.scala1191        at scala.collection.mutable.ResizableArrayclass.foreachResizableArray.scala59        at scala.collection.mutable.ArrayBuffer.foreachArrayBuffer.scala47        at org.apache.spark.scheduler.DAGScheduler.abortStageDAGScheduler.scala1191        at org.apache.spark.scheduler.DAGScheduleranonfunhandleTaskSetFailed1.applyDAGScheduler.scala693        at org.apache.spark.scheduler.DAGScheduleranonfunhandleTaskSetFailed1.applyDAGScheduler.scala693        at scala.Option.foreachOption.scala236        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailedDAGScheduler.scala693        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceiveDAGScheduler.scala1393        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceiveDAGScheduler.scala1354        at org.apache.spark.util.EventLoopanon1.runEventLoop.scala48 codepre  pNote that I get this same error GenericRowWithSchema cannot be cast to scala.Tuple2 when I run this in a compiled program using sparksubmit.  The program crashes at RUN TIME when it encounters the conversion step, and I had no compiler errors.p  pIt seems very strange to me that my artificially generated RDD tempRDD would work with the conversion, whereas the Hive query DataFrameRDD did not.  I checked, and both of the RDDs have the same formp  precodescalagt tempRDD org.apache.spark.rdd.RDD[Int, String, Int, String, scala.collection.mutable.ArrayBuffer[Int, String]]  MapPartitionsRDD[21] at map at DataFrame.scala776  scalagt tempRDDfromHive org.apache.spark.rdd.RDD[Int, String, Int, String, scala.collection.mutable.ArrayBuffer[Int, String]]  ParallelCollectionRDD[25] at parallelize at ltconsolegt70 codepre  pthe only difference is where their last step originated.  I even tried persisting, checkpointing, and materializing these RDDs before running the steps for tempRDD2 and tempRDD3.  All got the same error message.p  pI also read though related stackoverflow questions and Apache Spark Jira issues, and from those I attempted casting the ArrayBuffer as an Iterator instead, but that also failed on the second step with the same error.p  pDoes anyone know how to properly convert ArrayBuffers to HashSets for DataFrames originating from Hive tables  Since the error seems to be only for the Hive table version, Im tempted to think that this is an issue with SparkHive integration in SparkQL.p  pAny ideasp  pThanks in advance.p  p[edited] BTW, my Spark version is 1.3.0 CDH.p  p[edited  here are the printSchema results]p  precodescalagt tempRDDfromHive.printSchema root   var1 integer nullable  true   var2 string nullable  true   var3 integer nullable  true   var4 string nullable  true   var5 array nullable  true       element struct containsNull  true           a integer nullable  true           b string nullable  true codepre
Positive 2395 pIm issuing the following query to transcode an ORC formatted table for export by sqoopp  pre classlangsql prettyprintoverridecodecreate table cleancsv.user ROW FORMAT SERDE org.apache.hadoop.hive.serde2.OpenCSVSerde WITH SERDEPROPERTIES     separatorChar  t,    quoteChar      ,    escapeChar       STORED AS TEXTFILE tblproperties serialization.null.formatNULL as select  from cleaning.user codepre  pResults are the same when using codecode instead of codecodep  pIn both cases, my hive tables are stored in files with fields enclosed in double quotes, and separated by commas, codelike,this,,example of four fieldscodep
Positive 2395 pI am running query on hive ....and it is failing with casting error ...record in error logs belongs to table [strongcustomerdemographicsstrong]p  pcddemosk1082653,cdgenderM,cdmaritalstatusS,cdeducationstatus2 yr Degree         ,cdpurchaseestimate3500,cdcreditratingLow Risk  ,cddepcount4,cddepemployedcount6,cddepcollegecount3 p  pI want to understand what is causing casting error   how to debug and resolve it p  pstrongQuery strongp  precodeSELECT cdgender, cdmaritalstatus, cdeducationstatus, COUNT   AS  cnt1, cdpurchaseestimate, COUNT   AS cnt2, cdcreditrating, COUNT   AS cnt3, cddepcount, COUNT   AS cnt4, cddepemployedcount, COUNT   AS cnt5, cddepcollegecount, COUNT   AS cnt6   FROM  customer    AS c   ,  customeraddress    AS ca   ,  customerdemographics      WHERE c.ccurrentaddrsk  ca.caaddresssk  AND cacounty  IN Polk County,Harmon County,Cobb County,Mahoning County,Cedar County   AND cddemosk  c.ccurrentcdemosk   AND NOT  EXISTS   SELECT   FROM  storesales     ,  datedim     WHERE c.ccustomersk  sscustomersk AND sssolddatesk  ddatesk  AND dyear  2000  AND   dmoy BETWEEN 2 AND  2  3        GROUP BY cdgender,cdmaritalstatus,cdeducationstatus,cdpurchaseestimate,cdcreditrating,cddepcount,cddepemployedcount,cddepcollegecount     ORDER BY  cdgender , cdmaritalstatus , cdeducationstatus , cdpurchaseestimate , cdcreditrating , cddepcount , cddepemployedcount , cddepcollegecount  codepre  pError logs p  precodeQuery ID  root201509222014144fd38df2ac704d45a5a8a90ac78b75e5 Total jobs  10 Stage23 is selected by condition resolver. Stage6 is filtered out by condition resolver. .... Launching Job 5 out of 10 Number of reduce tasks is set to 0 since theres no reduce operator Starting Job  job144179479516213556, Tracking URL  httpmyhost8035proxyapplication144179479516213556 Kill Command  optheshadoophadoop2.6.0binhadoop job  kill job144179479516213556 Hadoop job information for Stage13 number of mappers 2 number of    reducers 0 20150922 201527,234 Stage13 map  0,  reduce  0 20150922 201545,490 Stage13 map  100,  reduce  0 Ended Job  job144179479516213556 with errors Error during job, obtaining debugging information... Examining task ID task144179479516213556m000001 and more from job job144179479516213556  Task with the most failures4   Task ID   task144179479516213556m000001  URL    httpmyhost8088taskdetails.jspjobidjob144179479516213556amptipidtask144179479516213556m000001  Diagnostic Messages for this Task Error java.lang.RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException Hive Runtime Error while processing row cddemosk1082653,cdgenderM,cdmaritalstatusS,cdeducationstatus2 yr Degree         ,cdpurchaseestimate3500,cdcreditratingLow Risk  ,cddepcount4,cddepemployedcount6,cddepcollegecount3 at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.mapExecMapper.java185 at org.apache.hadoop.mapred.MapRunner.runMapRunner.java54 at org.apache.hadoop.mapred.MapTask.runOldMapperMapTask.java450 at org.apache.hadoop.mapred.MapTask.runMapTask.java343 at org.apache.hadoop.mapred.YarnChild2.runYarnChild.java163 at java.security.AccessController.doPrivilegedNative Method at javax.security.auth.Subject.doAsSubject.java415 at org.apache.hadoop.security.UserGroupInformation.doAsUserGroupInformation.java1628 at org.apache.hadoop.mapred.YarnChild.mainYarnChild.java158     Caused by org.apache.hadoop.hive.ql.metadata.HiveException Hive Runtime     Error while processing row cddemosk1082653,cdgenderM,cdmaritalstatusS,cdeducationstatus2 yr Degree         ,cdpurchaseestimate3500,cdcreditratingLow Risk  ,cddepcount4,cddepemployedcount6,cddepcollegecount3 at org.apache.hadoop.hive.ql.exec.MapOperator.processMapOperator.java503 at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.mapExecMapper.java176 ... 8 more Caused by org.apache.hadoop.hive.ql.metadata.HiveException Unexpected exception org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOpMapJoinOperator.java311 at org.apache.hadoop.hive.ql.exec.Operator.forwardOperator.java815 at org.apache.hadoop.hive.ql.exec.FilterOperator.processOpFilterOperator.java120 at org.apache.hadoop.hive.ql.exec.Operator.forwardOperator.java815 at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOpTableScanOperator.java95 at org.apache.hadoop.hive.ql.exec.MapOperatorMapOpCtx.forwardMapOperator.java157 at org.apache.hadoop.hive.ql.exec.MapOperator.processMapOperator.java493 ... 9 more Caused by java.lang.ClassCastException org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectI  MapReduce Jobs Launched  StageStage15 Map 2   Cumulative CPU 5.87 sec   HDFS Read 4548921 HDFS Write 23410 SUCCESS StageStage16 Map 2   Cumulative CPU 42.8 sec   HDFS Read 122983367 HDFS Write 2153276 SUCCESS StageStage13 Map 2   HDFS Read 0 HDFS Write 0 FAIL Total MapReduce CPU Time Spent 48 seconds 670 msec codepre  pbut it works when I remove below condition from where clausep  precodecddemosk  c.ccurrentcdemosk codepre
Positive 2395 pI am new to hadoop and mapreduce framework. I was going through some of the serialization formats . One of them is Avro. which seems to be very efficient and compact format. p  pNow lets say I have some text data in HDFS , normally I write the mapreduce job to read that data and generate the output  or I can run hive queries  . p  pI wanted to know when will i use this in my custom applications  mapreduce jobs or hive   From which point onwards data ingestionprocessing in real world  applications , avro will come into picture. p
Positive 2395 pWanted to take something like this a hrefhttpsgithub.comfitzscottAirQualityblobmasterHiveDataTypeGuesser.java relnofollowhttpsgithub.comfitzscottAirQualityblobmasterHiveDataTypeGuesser.javaa and create a Hive UDAF to create an aggregate function that returns a data type guess.p  pDoes Spark have something like this already builtin Would be very useful for new wide datasets to explore data. Would be helpful for ML too, e.g. to decide categorical vs numerical variables.p  pHow do you normally determine data types in Sparkp  pP.S. Frameworks like h2o automatically determine data type scanning a sample of data, or whole dataset. So then one can decide e.g. if a variable should be a categorical variable or numerical.p  pP.P.S. Another use case is if you get an arbitrary data set we get them quite often, and want to save as a Parquet table. Providing correct data types make parquet more space effiecient and probably more querytime performant, e.g.  better parquet bloom filters than just storing everything as stringvarchar.p
Positive 2395 pI am running query on hive ....seems from logs , it is failing because of erroneous record in table [datedim] causing casting exception ....when I looked for other records in [datedim], nothing different in particular recordsee sample records at last.p  pI may be wrong pointing to particular record...trying to understand why this casting error and how this can be resolved p  pCan you please help me understand why error for this particular record  how it can be resolved  Any help is highly appreciated p  pstrongHive query strongp  precodeSELECT store AS channel, sscdemosk AS colname, dyear, dqoy,  icategory, ssextsalesprice AS extsalesprice  FROM  storesales     ,  item     ,  datedim     WHERE sscdemosk IS NULL   AND sssolddatesk  ddatesk   AND ssitemsk  iitemsk  codepre  pstrongHive logs strongp  precodeQuery ID  root20150922151717a94d4679224b41f38336a799d4ebedab Total jobs  4 ....   Launching Job 3 out of 4 Number of reduce tasks is set to 0 since theres no reduce operator Starting Job  job144179479516213426, Tracking URL  httpmyhost8035proxyapplication144179479516213426 Kill Command  optheshadoophadoop2.6.0binhadoop job  kill job144179479516213426 Hadoop job information for Stage6 number of mappers 1 number of    reducers 0 20150922 151834,291 Stage6 map  0,  reduce  0 20150922 151854,541 Stage6 map  100,  reduce  0 Ended Job  job144179479516213426 with errors Error during job, obtaining debugging information... Examining task ID task144179479516213426m000000 and more from job job144179479516213426  Task with the most failures4 Task IDtask144179479516213426m000000 URL  myhost8088taskdetails.jspjobidjob144179479516213426amptipidtask144179479516213426m000000  Diagnostic Messages for this Task Error java.lang.RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException Hive Runtime Error while processing row ddatesk2452538,ddateidAAAAAAAAKDMGFCAA,ddate20020920,dmonthseq1232,dweekseq5360,dquarterseq412,dyear2002,ddow5,dmoy9,ddom20,dqoy3,dfyyear2002,dfyquarterseq412,dfyweekseq5360,ddaynameFriday   ,dquarternameN,dholiday2002Q3,dweekendN,dfollowingholidayY,dfirstdom2452761,dlastdom2452519,dsamedayly2452447,dsamedaylq2452173,dcurrentdayN,dcurrentweekN,dcurrentmonthN,dcurrentquarterN,dcurrentyearN at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.mapExecMapper.java185 at org.apache.hadoop.mapred.MapRunner.runMapRunner.java54 at org.apache.hadoop.mapred.MapTask.runOldMapperMapTask.java450 at org.apache.hadoop.mapred.MapTask.runMapTask.java343 at org.apache.hadoop.mapred.YarnChild2.runYarnChild.java163 at java.security.AccessController.doPrivilegedNative Method at javax.security.auth.Subject.doAsSubject.java415 at org.apache.hadoop.security.UserGroupInformation.doAsUserGroupInformation.java1628 at org.apache.hadoop.mapred.YarnChild.mainYarnChild.java158 Caused by org.apache.hadoop.hive.ql.metadata.HiveException Hive Runtime Error while processing row ddatesk2452538,ddateidAAAAAAAAKDMGFCAA,ddate20020920,dmonthseq1232,dweekseq5360,dquarterseq412,dyear2002,ddow5,dmoy9,ddom20,dqoy3,dfyyear2002,dfyquarterseq412,dfyweekseq5360,ddaynameFriday   ,dquarternameN,dholiday2002Q3,dweekendN,dfollowingholidayY,dfirstdom2452761,dlastdom2452519,dsamedayly2452447,dsamedaylq2452173,dcurrentdayN,dcurrentweekN,dcurrentmonthN,dcurrentquarterN,dcurrentyearN at org.apache.hadoop.hive.ql.exec.MapOperator.processMapOperator.java503 at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.mapExecMapper.java176 ... 8 more Caused by org.apache.hadoop.hive.ql.metadata.HiveException Unexpected exception org.apache.hadoop.hive.serde2.io.HiveDecimalWritable cannot be cast to org.apache.hadoop.io.IntWritable at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOpMapJoinOperator.java311 at org.apache.hadoop.hive.ql.exec.Operator.forwardOperator.java815 at org.apache.hadoop.hive.ql.exec.FilterOperator.processOpFilterOperator.java120 at org.apache.hadoop.hive.ql.exec.Operator.forwardOperator.java815 at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOpTableScanOperator.java95 at org.apache.hadoop.hive.ql.exec.MapOperatorMapOpCtx.forwardMapOperator.java157 at org.apache.hadoop.hive.ql.exec.MapOperator.processMapOperator.java493 ... 9 more Caused by java.lang.ClassCastException org.apache.hadoop.hive.serde2.io.HiveDecimalWritable cannot be cast to org.apache.hadoop.io.IntWritable at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.getWritableIntObjectInspector.java36 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.evaluateGenericUDFOPEqual.java84 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.evaluateExprNodeGenericFuncEvaluator.java185 at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluateExprNodeEvaluator.java77 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluatorDeferredExprObject.getExprNodeGenericFuncEvaluator.java86 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluateGenericUDFOPAnd.java68 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.evaluateExprNodeGenericFuncEvaluator.java185 at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluateExprNodeEvaluator.java77 at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluateExprNodeEvaluator.java65 at org.apache.hadoop.hive.ql.exec.FilterOperator.processOpFilterOperator.java106 at org.apache.hadoop.hive.ql.exec.Operator.forwardOperator.java815 at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForwardCommonJoinOperator.java638 at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObjectCommonJoinOperator.java651 at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObjectCommonJoinOperator.java654 at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObjectCommonJoinOperator.java750 at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOpMapJoinOperator.java299 ... 15 more   FAILED Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask ATTEMPT Execute BackupTask org.apache.hadoop.hive.ql.exec.mr.MapRedTask ....... StageStage8 Map 2   Cumulative CPU 42.59 sec   HDFS Read 122983367 HDFS Write 5267230 SUCCESS StageStage6 Map 1   HDFS Read 0 HDFS Write 0 FAIL StageStage2 Map 2  Reduce 1   Cumulative CPU 8.78 sec   HDFS Read 15648221 HDFS Write 8106 SUCCESS Total MapReduce CPU Time Spent 51 seconds 370 msec OK channel    colname    dyear    dqoy    icategory    extsalesprice store    sscdemosk    1998    1    Sports                                                NULL ..... . . . .. codepre  pstrongSample data of table from datedim  including erroneous record strongp  phive  select  from datedim p  precodedatedim.ddatesk    datedim.ddateid    datedim.ddate    datedim.dmonthseq    datedim.dweekseq    datedim.dquarterseq    datedim.dyear    datedim.ddow    datedim.dmoy    datedim.ddom    datedim.dqoy   datedim.dfyyear    datedim.dfyquarterseq    datedim.dfyweekseq    datedim.ddayname    datedim.dquartername    datedim.dholiday    datedim.dweekend    datedim.dfollowingholiday    datedim.dfirstdom    datedim.dlastdom    datedim.dsamedayly    datedim.dsamedaylq    datedim.dcurrentday    datedim.dcurrentweek    datedim.dcurrentmonth    datedim.dcurrentquarter    datedim.dcurrentyear  2452538    AAAAAAAAKDMGFCAA    20020920    1232    5360    412    2002    5    9    20    3    2002    412    5360    Friday       N    2002Q3    N    Y    2452761    2452519    2452447    2452173    N    N    N    NN  2431208    AAAAAAAAIOIBFCAA          19440427    531      2313    178    1944    4     4    27    2    1944    178    2313    Thursday     N    1944Q2    N    N    2431272    2431182    2431117    2430842    N    N    N    NN  2456494    AAAAAAAAOKLHFCAA       20130720    1362    5925    455    2013     6    7    20    3    2013    455    5925    Saturday     N    2013Q3    N    Y    2456655    2456475    2456403    2456129    N    N    N    NN  2481780    AAAAAAAAEHONFCAA       20821012    2193    9537    732    2082    1    10    12    4    2082    732   9537    Monday       N    2082Q4    N    N    2482041    2481769    2481688    2481415    N    N    N    NN codepre  pstronghive describe storesales strongp  precodesssolddatesk         int                                          sssoldtimesk         int                                          ssitemsk              int                                          sscustomersk          int                                          sscdemosk             int                                          sshdemosk             int                                          ssaddrsk              int                                          ssstoresk             int                                          sspromosk             int                                          ssticketnumber        int                                          ssquantity             int                                          sswholesalecost       decimal7,2                                 sslistprice           decimal7,2                                 sssalesprice          decimal7,2                                 ssextdiscountamt     decimal7,2                                 ssextsalesprice      decimal7,2                                 ssextwholesalecost   decimal7,2                                 ssextlistprice       decimal7,2                                 ssexttax              decimal7,2                                 sscouponamt           decimal7,2                                 ssnetpaid             decimal7,2                                 ssnetpaidinctax     decimal7,2                                 ssnetprofit           decimal7,2                                 codepre  pstronghive describe item strongp  precodeiitemsk               int                                          iitemid               string                                       irecstartdate        date                                         irecenddate          date                                         iitemdesc             string                                       icurrentprice         decimal7,2                                 iwholesalecost        decimal7,2                                 ibrandid              int                                          ibrand                 string                                       iclassid              int                                          iclass                 string                                       icategoryid           int                                          icategory              string                                       imanufactid           int                                          imanufact              string                                       isize                  string                                       iformulation           string                                       icolor                 string                                       iunits                 string                                       icontainer             string                                       imanagerid            int                                          iproductname          string                                       codepre  pstronghive describe datedim strongp  precodeddatesk               int                                          ddateid               string                                       ddate                  date                                         dmonthseq             int                                          dweekseq              int                                          dquarterseq           int                                          dyear                  int                                          ddow                   int                                          dmoy                   int                                          ddom                   int                                          dqoy                   int                                          dfyyear               int                                          dfyquarterseq        int                                          dfyweekseq           int                                          ddayname              string                                       dquartername          string                                       dholiday               string                                       dweekend               string                                       dfollowingholiday     string                                       dfirstdom             int                                          dlastdom              int                                          dsamedayly           int                                          dsamedaylq           int                                          dcurrentday           string                                       dcurrentweek          string                                       dcurrentmonth         string                                       dcurrentquarter       string                                       dcurrentyear          string                                       codepre
Positive 2395 pI have a spark application which will successfully connect to hive and query on hive tables using spark engine. p  pTo build this, I just added codehivesite.xmlcode to classpath of the application and spark will read the codehivesite.xmlcode to connect to its metastore. This method was suggested in sparks mailing list.p  pSo far so good. Now I want to connect to two hive stores and I dont think adding another codehivesite.xmlcode to my classpath will be helpful. I referred quite a few articles and spark mailing lists but could not find anyone doing this.p  pCan anyone suggest how I can achieve thisp  pThanks.p  pDocs referredp  ul lipa hrefhttpscwiki.apache.orgconfluencedisplayHiveHiveonSpark3AGettingStartedHive on Sparkapli lipa hrefhttpspark.apache.orgdocslatestsqlprogrammingguide.htmlcompatibilitywithapachehiveSpak docsapli lipa hrefhttpsspark.apache.orgdocs1.3.0apijavaorgapachesparksqlhiveHiveContext.htmlHiveContextapli ul
Negative 2395 pI have seen so many websites where when open any page there is no any numeric Id or some thing like this in the URL, Just the URL String with hyphens in between the words. It means that the Page is retrieving the data on the basis of string from the database. And some URLs are very long. It means there could be no any Index on that field in the database as it is so long. So, Is it an efficient way, If not, Why so many sites, use this methodp
Negative 2395 pIm using the following code to send emails in railsp  precodeclass InvoiceMailer lt ActionMailerBase    def invoiceinvoice     from          CONFIG[email]     recipients    invoice.email     subject       Bevestiging Inschrijving invoice.course.name     contenttype  multipartalternative      part texthtml do p       p.body  rendermessage invoicehtml, invoice gt invoice     end      part textplain do p       p.body  rendermessage invoiceplain, invoice gt invoice     end      pdf  PrawnDocument.newpagesize gt A4     PDFRenderer.renderinvoicepdf, invoice     attachment contenttype gt applicationpdf, body gt pdf.render, filename gt factuur.pdf      invoice.course.coursefiles.each do file       attachment contenttype gt file.contenttype, body gt File.readfile.fullpath, filename gt file.filename     end   end  end codepre  pIt seems fine to me, and the emails also show up like they should in the Gmail webinterface. In Mail the Apple program, however, I get just 1 attachment where there should be 2 and there is no text. I just cant seem to figure out whats causing it.p  pI copied the email from the logsp  pre  Sent mail to xxxgmail.com  From yyygmail.com To xxxgmail.com Subject Bevestiging Inschrijving Authentiek Spreken MimeVersion 1.0 ContentType multipartalternative boundarymimepart4a5b035ea0d4769515bbca0ce9b412a   mimepart4a5b035ea0d4769515bbca0ce9b412a ContentType texthtml charsetutf8 ContentTransferEncoding Quotedprintable ContentDisposition inline                 pDear sirp      mimepart4a5b035ea0d4769515bbca0ce9b412a ContentType textplain charsetutf8 ContentTransferEncoding Quotedprintable ContentDisposition inline  Dear sir   Foo  mimepart4a5b035ea0d4769515bbca0ce9b412a ContentType applicationpdf namefactuur.pdf ContentTransferEncoding Base64 ContentDisposition attachment filenamefactuur.pdf  JVBERi0xLjMKwoxIDAgb2JqCjw8IC9DcmVhdG9yIChQcmF3bikKL1By b2R1Y2VyIChQcmF3bikKPj4KZW5kb2JqCjIgMCBvYmoKPDwgL0NvdW50IDEK ... ... ... MCBuIAp0cmFpbGVyCjw8IC9JbmZvIDEgMCBSCi9TaXplIDExCi9Sb290IDMg MCBSCj4CnN0YXJ0eHJlZgo4Nzc1CiUlRU9GCg  mimepart4a5b035ea0d4769515bbca0ce9b412a ContentType applicationpdf nameSpelregels.pdf ContentTransferEncoding Base64 ContentDisposition attachment filenameSpelregels.pdf  JVBERi0xLjQNJeLjz9MNCjYgMCBvYmoNPDwvTGluZWFyaXplZCAxL0wgMjEx NjYvTyA4L0UgMTY5NTIvTiAxL1QgMjEwMDAvSCBbIDg3NiAxOTJdPj4NZW5k ... ... ... MDIwNzQ4IDAwMDAwIG4NCnRyYWlsZXINCjw8L1NpemUgNj4DQpzdGFydHhy ZWYNCjExNg0KJSVFT0YNCg  mimepart4a5b035ea0d4769515bbca0ce9b412a  pre
Negative 2395 pI havent found any get equivalent to appendBytes. If Ive understood getBytes correctly, that method always returns bytes from the emstartem of the buffer. It will not increment.p  pWhats the reason for thisp
Negative 2395 pI tried to find an example for that, but I failed. Can someone tell me what method to use to get the current text selection. I know its possible in .NET 4.. but I want to make it using winapi so I can use it in .NET 2.p  pEDIT This is only possible with UI Autonation.p
Negative 2395 pI have string likep  precodeltli classvideodescriptiongtltstronggtDescriptionltstronggt hello world, This is test description.ltligt codepre  pAnd i want string like, hello world, This is test description. That string willbe dynamic everytime. p  pSo, how i can use pregmatch option herep
Negative 2395 pIm developing an Android game somewhat similar to Star Control.p  pIn this game, planet gravity plays a huge role, and so the players ship is affected by various planets. In this game, there are multiple planets which move orbit.p  pMy main issue at the moment is handing collision response. When the player hits a planet, i want the player to have a small bounce. I used this a hrefhttpswww.youtube.comwatchvymgbDdO8hKI relnofollowhttpswww.youtube.comwatchvymgbDdO8hKIa as a source to program this, and it works great for planets which dont move.p  pHowever, when the planet does move, the algorithm works great when the planet is moving away from the player, but if the player is in the planets path i.e., the planet is moving towards the player, the planet sortv eats the player.p  pI know I need to compensate somehow for the planets velocity. Ive tried different variations, such as add the planets vector to the overall resulting vector of the bounce, but nothing seems to give a good result... the planet always seems to eat up the player when the planet moves towards the player.p  pIf you guys would like me to post code samples, let me know, although Im looking for more of a concept solution, like the video linked above provides.p  pThanks in advancep
Negative 2395 pI am currently using QLPreviewController to view PDFs 250MB However it cannot deal with real large files. Either I get the info that not the whole file has been loaded or the whole app just dies.  I also need to customize the view which is nit possible using QLPreviewController.  What should I do Use UIWebView instead Or will I have to use CGContextDrawPDFPage Using the latter, how will I get zooming implementedp
Negative 2395 pI have an application using Google Maps API v1. I need to make changes but I dont know what to change in Google Maps API v2 to continue using the Map View from v1.p  pI added the metadata with the api key as a child of application. And I removed it from the Map View codeltcom.google.android.maps.MapViewcodep  pIts too difficult to use Map Fragment and change everything because theres too much code using Map View.p  pI found this link but it doesnt seem to help much.p  pa hrefhttpsdevelopers.google.commapsdocumentationandroidreferencecomgoogleandroidgmsmapsMapView relnofollowhttpsdevelopers.google.commapsdocumentationandroidreferencecomgoogleandroidgmsmapsMapViewap
Negative 2395 pIn my PostgreSQL 9.1 database I have multiple tables and one trigger function. p  pRight now I am creating the trigger for each table by using that trigger function.p  pThis methodology working fine. My boss has asked me to create the trigger commonly only one time by reusing that trigger function. That one trigger function should get used by all the tables in my database. p
Negative 2395 pI have a Django model p  precodeclass UserProfilemodels.Model     user  models.ForeignKeyUser, uniqueTrue     firstname  models.CharFieldmaxlength255     lastname  models.CharFieldmaxlength255 codepre  pNow I want to search for user. Problem is when I do p  precodeQfirstnameicontainssearchstring  Qlastnameicontainssearchstring  codepre  pit only searches in firstname and lastname respectively. But if someone types the whole name in search bar it wont give any results as the whole namefirstnamelastname is not contained in firstname or lastname. I dont want to change my model but dynamically query in combination of fieldsfirstname lastname on search, is there any way to do that  p
