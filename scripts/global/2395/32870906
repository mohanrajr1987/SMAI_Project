<p>I can use sqoop directly to create a hive table and import data from an Oracle table into the hive table in one command. But the metadata of the hive table only has two data types instead of the 7 or 8 data types from the Oracle table.</p>  <p>Then I use Embarcadero ER/Studio data modeling tool to generate the hive table ddl, which has many nice data types. I also use sqoop to just import data into HDFS files by doing this:</p>  <blockquote>   <p>sqoop import  --connect jdbc:oracle:thin:hadoop_demo/hadoop_demo@mto01oda-scan:1530/bigdata_sqoop -m 1 --table HADOOP_DEMO.APE1_RATED_EVENT --as-avrodatafile --warehouse-dir=/user/hive/warehouse --fetch-size 10 </p> </blockquote>  <p>I then tried to create a hive external table:</p>  <pre><code>CREATE EXTERNAL TABLE APE1_RATED_EVENT(     CYCLE_CODE DECIMAL,     ......     L9_SOURCE_SYSTEM VARCHAR(48) ) STORED AS AVRO LOCATION 'hdfs:///user/hive/warehouse/HADOOP_DEMO.APE1_RATED_EVENT'; </code></pre>  <p>The hive table is created, but the data in every column is null from the Metastore Manager. The properties are also weird:</p>  <pre><code>totalSize   0 numRows -1 rawDataSize -1} viewOriginalText    null viewExpandedText    null tableType   EXTERNAL_TABLE </code></pre>