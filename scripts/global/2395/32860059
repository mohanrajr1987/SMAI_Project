<p>I am running a large job that consolidates about 55 streams (tags) of samples (one sample per record) at irregular times over two years into 15-minute averages. There are about 1.1 billion records in 23k streams in the raw dataset, and these 55 streams make up about 33 million of those records. I calculated a 15-minute index and am grouping by that to get the average value, however I seem to have am exceeded the max dynamic partitions on my hive job in spite of cranking it way up to 20k. I can increase it further I suppose, but it already takes awhile to fail (about 6 hours, although I reduced it to 2 by reducing the number of streams to consider), and I donâ€™t actually know how to calculate how many I really need. </p>  <p>Here is the code:</p>  <pre><code>SET hive.exec.dynamic.partition = true;  SET hive.exec.dynamic.partition.mode = nonstrict;  SET hive.exec.max.dynamic.partitions=50000; SET hive.exec.max.dynamic.partitions.pernode=20000;    DROP TABLE IF EXISTS sensor_part_qhr;    CREATE TABLE sensor_part_qhr (     tag  STRING,     tag0 STRING,     tag1 STRING,     tagn_1  STRING,     tagn  STRING,      timestamp  STRING,     unixtime INT,     qqFr2013 INT,      quality  INT,     count  INT,     stdev  double,     value    double )   PARTITIONED BY (bld STRING);  INSERT INTO TABLE sensor_part_qhr PARTITION (bld)  SELECT  tag,         min(tag),          min(tag0),          min(tag1),          min(tagn_1),          min(tagn),          min(timestamp),         min(unixtime),           qqFr2013,          min(quality),     count(value),     stddev_samp(value),          avg(value) FROM    sensor_part_subset      WHERE   tag1='Energy' GROUP BY tag,qqFr2013; </code></pre>  <p>And here is the error message:</p>  <pre><code>    Error during job, obtaining debugging information...     Examining task ID: task_1442824943639_0044_m_000008 (and more) from job job_1442824943639_0044     Examining task ID: task_1442824943639_0044_r_000000 (and more) from job job_1442824943639_0044      Task with the most failures(4):      -----     Task ID:       task_1442824943639_0044_r_000000      URL:       http://headnodehost:9014/taskdetails.jsp?jobid=job_1442824943639_0044&amp;tipid=task_1442824943639_0044_r_000000     -----     Diagnostic Messages for this Task:     Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveFatalException: [Error 20004]: Fatal error occurred when node tried to create too many dynamic partitions. The maximum number of dynamic partitions is controlled by hive.exec.max.dynamic.partitions and hive.exec.max.dynamic.partitions.pernode. Maximum was set to: 20000         at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:283)         at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)         at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)         at java.security.AccessController.doPrivileged(Native Method)         at javax.security.auth.Subject.doAs(Subject.java:415)         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)         at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)     Caused by: org.apache.hadoop.hive.ql.metadata.HiveFatalException:      [Error 20004]: Fatal error occurred when node tried to create too many dynamic partitions.      The maximum number of dynamic partitions is controlled by hive.exec.max.dynamic.partitions and hive.exec.max.dynamic.partitions.pernode.      Maximum was set to: 20000          at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:747)         at org.apache.hadoop.hive.ql.exec.FileSinkOperator.startGroup(FileSinkOperator.java:829)         at org.apache.hadoop.hive.ql.exec.Operator.defaultStartGroup(Operator.java:498)         at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:521)         at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:232)         ... 7 more      Container killed by the ApplicationMaster.     Container killed on request. Exit code is 137     Container exited with a non-zero exit code 137       FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask     MapReduce Jobs Launched:      Job 0: Map: 520  Reduce: 140   Cumulative CPU: 7409.394 sec   HDFS Read: 0 HDFS Write: 393345977 SUCCESS     Job 1: Map: 9  Reduce: 1   Cumulative CPU: 87.201 sec   HDFS Read: 393359417 HDFS Write: 0 FAIL     Total MapReduce CPU Time Spent: 0 days 2 hours 4 minutes 56 seconds 595 msec </code></pre>  <p>Can anyone give some ideas as to how to calculate how many of these dynamic nodes I might need for a job like this? </p>  <p>Or maybe I should be doing this differently?  I am running Hive 0.13 by the way on Azure HDInsight. </p>  <p><strong>Update:</strong></p>  <ul> <li>Corrected some of the numbers above.</li> <li>Reduced it to 3 streams operating on 211k records and it finally succeeded.</li> <li>Started experimenting, reduced the partitions per node to 5k, and then 1k, and it still succeeded.</li> </ul>  <p>So I am not blocked anymore, but I am thinking I would have needed millions of nodes to do the whole dataset in one go (which is what I really wanted to do).</p>