<p>downloaded spark 1.5.0 pre-built and run via pyspark this simple code</p>  <pre><code>from pyspark.sql import Row l = [('Alice', 1)] sqlContext.createDataFrame(l).collect </code></pre>  <p>Yields error:</p>  <pre><code>15/09/30 06:48:48 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so do es not have its own datastore table. Traceback (most recent call last):   File "&lt;stdin&gt;", line 1, in &lt;module&gt;   File "c:\bigdata\spark-1.5\spark-1.5.0\python\pyspark\sql\context.py", line 408, in createDataFrame     jdf = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())   File "c:\bigdata\spark-1.5\spark-1.5.0\python\pyspark\sql\context.py", line 660, in _ssql_ctx     "build/sbt assembly", e) Exception: ("You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly", Py4JJavaError(u'An error occurred  while calling None.org.apache.spark.sql.hive.HiveContext.\n', JavaObject id=o28)) </code></pre>  <p>so tried to compile it myself</p>  <pre><code>c:\bigdata\spark-1.5\spark-1.5.0&gt;.\build\apache-maven-3.3.3\bin\mvn  -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests -Phive -Phive-t </code></pre>  <p>hriftserver clean package</p>  <p>but still get the same error on the compiled version. </p>  <p>Any suggestion?</p>