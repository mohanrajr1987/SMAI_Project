<p>I'm working on a long request to a django app (nginx reverse proxy, mysql db, celery-rabbitMQ-redis set) and have some doubts about the solution i should apply :</p>  <p><strong>Functionning :</strong> One functionality of the app allows users to migrate thousands of objects from one system to another. Each migration is logged into a db, and the users are provided the possibility to get in a csv format the history of the migration : which objects have been migrated, which status (success, errors, ...)</p>  <p>To get the history, a get request is sent to a django view, which returns, after serialization and rendering into csv, the download response.</p>  <p><strong>Problem :</strong> the serialisation and rendering processes, for a large set of objects (e.g. 160 000) are quite long and the request times out.</p>  <p>Some solutions I was thinking about/found thanks to pervious search are :</p>  <ul> <li>Increasing the amount of time before timeout : easy, but I saw everywhere that this is a global nginx setting and would affect every requests on the server.</li> <li>Using an asynchronous task handled by celery : the concept would be to make an initial request to the server, which would launch the serializing and rendering task with celery, and give a special httpresponse to the client. Then the client would regularly ask the server if the job is done, and the server would deliver the history at the end of processing. I like this one but I'm not sure about how to technically implement that.</li> <li>Creating and temporarily storing the csv file on the server, and give the user a way to access it &amp; to download it. I'm not a big fan of that one.</li> </ul>  <p><strong>So my question is :</strong> has anyone already faced a similar question ? Do you have advises for the technical implementation of the solution (#2), or a better solution to propose me ?</p>  <p>Thqnks !</p>