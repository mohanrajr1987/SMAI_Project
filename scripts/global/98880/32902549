<p>I have a 20GB file and a 400MB file which I'm mapping each to project 6 attributes each. I then create a K, V RDD by creating a hash with part of the attributes (first 2 letters of firstname and first 4 letters of surname).</p>  <p>So I now have <code>a: RDD[K,V]</code> and <code>b: RDD[K,V]</code> with a common key so I want to join them </p>  <p><code>a.join(b).map(x=&gt; [check commonality in the attributes]).SaveAsTextFile(fileout)</code></p>  <p>The strange part is that I run this on HDFS on my 16GB Macbook and it works in around 16 mins. When I put it on our 3 worker node cluster with 96GB each I get repeated FetchFailed exceptions.</p>  <p>Can this really be down to the HDFS on my mac all being same SSD and the absence of network IO or is there something else I can look at?</p>  <p>I'm using Cloudera 5.3.1 and running spark on Yarn, the executor logs have limited information I've not worked out how to adjust the logging level of executors to get more info. Any idea how to do this?</p>