<p>I'm getting a NPE when trying to coalesce and save out an RDD.  </p>  <p>Code works locally, <em>and</em> works on the cluster in the scala shell, but throws the error when submitting it as a job to the cluster.</p>  <p>I've tried printing out using a take() to see if the rdd contains some null data, but this throws the same error - pain because it works ok in the shell.</p>  <p>I'm saving out to HDFS and have the full url path in the variable - model saves with this method fine during MLLib training phase.</p>  <p>Any ideas much appreciated!</p>  <p>Scala Code (Whole Prediction Func):</p>  <pre><code>val gridFile = "hdfs://ec2-XX-XX-XX-XX.eu-west-1.compute.amazonaws.com:9000/root/data/iow_grid_001dd.csv" val featuresFile = "hdfs://ec2-XX-XX-XX-XX.eu-west-1.compute.amazonaws.com:9000/root/data/map_osm_xml_spark.csv" val modelPath = "hdfs://ec2-XX-XX-XX-XX.eu-west-1.compute.amazonaws.com:9000/root/data/map_osm_xml_spark_randforest_model" val outCoords = "hdfs://ec2-XX-XX-XX-XX.eu-west-1.compute.amazonaws.com:9000/root/data/map_osm_xml_spark_grid_01dd_coords"   val outPreds = "hdfs://ec2-XX-XX-XX-XX.eu-west-1.compute.amazonaws.com:9000/root/data/map_osm_xml_spark_grid_01dd_preds"    //Spark Setup  val conf = new SparkConf().setAppName("ML4LP")  val sc = new SparkContext(conf)   def dist(x1: Double, y1: Double, x2: Double, y2: Double): Double = {    val dX = x2-x1    val dY = y2-y1    sqrt((dX*dX)+(dY*dY))  }   //Start mapping and reducing into the correct format val gridDots = sc.textFile(gridFile, 2) val gridDotsTuple = gridDots.map(line =&gt; line.split("\n")) val gridArrS = gridDotsTuple.map(a =&gt; (a(0).split(",")))  //Remember for each grid cell we dont have a class - but want to keep the cell coords //We'll keep things same for now - with class as 0 val gridMap = gridArrS.map(line =&gt; (line(2), (line(0).toDouble, line(1).toDouble)))  //Group all the keys together - [class,[coords]] val gridRed = gridMap.groupByKey()   //Now load / build in the features file val featDots = sc.textFile(featuresFile, 2) val featDotsTuple = featDots.map(line =&gt; line.split("\n")) val featArrS = featDotsTuple.map(a =&gt; (a(0).split(","))) val featMap = featArrS.map(line =&gt; (line(2), (line(0).toDouble, line(1).toDouble)))  //Group all the keys together - [class,[coords]] val featRed = featMap.groupByKey()   //Cartesian pair wise combine val cartMap = gridMap.cartesian(featRed).map(line =&gt; (line._1._1, (line._1._2), (line._2._1, line._2._2.map(y =&gt; dist(line._1._2._1, line._1._2._2, y._1, y._2)).reduce((a,b)=&gt;(a+b)/line._2._2.size))))  //Now we have grid points paired with average distance for each class - grid points are duplicated still //Array[(String, (Double, Double), (String, Double))] = Array((0,(-1.6,50.52),(8,15.791085338956579)), (0,(-1.6,50.52),(6,11.200668398145806)))  //Group by grid coord - gets rid of the grid point duplication val cartMapSwap = cartMap.map(line =&gt; (line._1, line._2) -&gt; (line._3)).groupByKey()  //Prepare for sorting by class in the vector - so they are comparable for trg val distArr = cartMapSwap.map(line =&gt; (line._1 -&gt; line._2.map(x =&gt; (x._1, (x._2))).toMap))  //Do the sort and derive a list instead - Preparing for dense vector val distSortArr = distArr.map(line =&gt; (line._1 -&gt; line._2.toList.sortBy{_._1.toInt})).map(line =&gt; (line._1 -&gt; (line._2.map(x =&gt; x._2))))  //Now convert to the dense vector  val distVects = distSortArr.map(line =&gt; (line._1 -&gt; (Vectors dense(line._2.toArray))))  //Zip to get unique id then keep the coords seperate - so we can zip back together later in case order changes val coords = distVects.zipWithUniqueId().map(line =&gt; (line._2 -&gt; line._1._1._2))    //Also now wrap as labelled Points keeping the unique id we made previously val labDistVect = distVects.zipWithUniqueId().map(line =&gt; (LabeledPoint(line._2, line._1._2)))  //Now we the average distance to classes for each grid cell we can make predictions for these vectors using the model  /* //Naive Bayes - Load the model val nBayesModel = NaiveBayesModel.load(sc, modelPath)  //Predict probabilities for test data - this gives a probability of each class val nbPreds = labDistVect.map(p =&gt; (nBayesModel.predictProbabilities(p.features), p.label)) *  */  //Load the Random Forest val rfModel = RandomForestModel.load(sc, modelPath)  //Make the predictions - Here the label is the unique ID of the point val rfPreds = labDistVect.map(p =&gt; (p.label, rfModel.predict(p.features)))   //Collect and save println("Done Modelling, now saving preds") val outP = rfPreds.coalesce(1,true).saveAsTextFile(outPreds) println("Done Modelling, now saving coords") val outC = coords.coalesce(1,true).saveAsTextFile(outCoords) </code></pre>  <p>Stack Trace:</p>  <pre><code>    Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 40, XX.XX.XX.XX): java.lang.NullPointerException     at GeoDistPredict1$$anonfun$38.apply(GeoDist1.scala:340)     at GeoDistPredict1$$anonfun$38.apply(GeoDist1.scala:340)     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)     at scala.collection.Iterator$$anon$10.next(Iterator.scala:312)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1298)     at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1298)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1839)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1839)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)     at org.apache.spark.scheduler.Task.run(Task.scala:88)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745)  Driver stacktrace:     at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)     at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)     at scala.Option.foreach(Option.scala:236)     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)     at org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)     at org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)     at org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)     at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1298)     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)     at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)     at org.apache.spark.rdd.RDD.take(RDD.scala:1272)     at GeoDistPredict1$delayedInit$body.apply(GeoDist1.scala:352)     at scala.Function0$class.apply$mcV$sp(Function0.scala:40)     at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)     at scala.App$$anonfun$main$1.apply(App.scala:71)     at scala.App$$anonfun$main$1.apply(App.scala:71)     at scala.collection.immutable.List.foreach(List.scala:318)     at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)     at scala.App$class.main(App.scala:71)     at GeoDistPredict1$.main(GeoDist1.scala:255)     at GeoDistPredict1.main(GeoDist1.scala)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:606)     at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)     at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)     at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)     at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)     at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.NullPointerException     at GeoDistPredict1$$anonfun$38.apply(GeoDist1.scala:340)     at GeoDistPredict1$$anonfun$38.apply(GeoDist1.scala:340)     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)     at scala.collection.Iterator$$anon$10.next(Iterator.scala:312)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1298)     at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1298)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1839)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1839)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)     at org.apache.spark.scheduler.Task.run(Task.scala:88)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) 15/10/03 13:24:19 INFO spark.SparkContext: Invoking stop() from shutdown hook </code></pre>