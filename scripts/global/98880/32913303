<p>We would like our Spark driver to calculate aggregation counts on business filtered events from a kinesis stream for given time frames (30 seconds- few minutes). What are some reasonable ways to achieve this with Spark? Spark pulls a significant number of duplicates from Kinesis and we need exactly-once semantics. </p>  <p>Any feedback is greatly appreciated except for recommendation to switch to Kafka Direct.</p>