<p>I have spark, hadoop, cassandra installed on a remote PC, whoses address is <code>10.211.55.3</code>, whose name is <code>ubuntu</code>. (in fact, it's a Virtual Machine on my mac). For the local PC, I don't have Spark installed, I have only Java(and of course all the necessary jar files).</p>  <p>Now, it is possible to launch a master and several slaves on the remote PC, and <code>spark-shell</code> works fine on the remote PC as well.</p>  <p>I have created a Java project on the local PC, in which I would like to connect to the master on the remote PC in order to post Spark jobs, but the connection fails, no matter if I use </p>  <pre><code>new SparkConf().setAppName("TestApp").setMaster("spark://10.211.55.3:7077"); </code></pre>  <p>or</p>  <pre><code>new SparkConf().setAppName("TestApp").setMaster("spark://ubuntu:7077"); </code></pre>  <p>I believe that I should use the version with the IP address, but it gives me these error messages:</p>  <pre><code>Could not connect to 10.211.55.3:7077: akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkMaster@10.211.55.3:7077] akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka.tcp://sparkMaster@10.211.55.3:7077/), Path(/user/Master)] </code></pre>  <p>So my question is, is it possible to execute a Java application with a Spark job on the local PC (where Spark is not installed, there is only the jar files) ? If possible, is there some options/parameters to enable this? Or it is absolutely necessary to install Spark on the local PC?</p>  <p>Thanks a lot !</p>  <p><strong>Edit 1</strong>: OK. I confess what I'm doing...I have a Mac and I've installed a Ubuntu virtual machine in it. The IP address of the virtual machine is <code>10.211.55.3</code>. </p>  <p>Besides, the master/slaves are running, I can access their state via the web browser, the ip address works as well.</p>