<p>I'm new to Spark, SparkR and generally all HDFS-related technologies. I've installed recently Spark 1.5.0 and run some simple code with SparkR:</p>  <pre><code>Sys.setenv(SPARK_HOME="/private/tmp/spark-1.5.0-bin-hadoop2.6") .libPaths("/private/tmp/spark-1.5.0-bin-hadoop2.6/R/lib") require('SparkR') require('data.table')  sc &lt;- sparkR.init(master="local") sqlContext &lt;- sparkRSQL.init(sc) hiveContext &lt;- sparkRHive.init(sc)  n = 1000 x = data.table(id = 1:n, val = rnorm(n))  Sys.time() xs &lt;- createDataFrame(sqlContext, x) Sys.time() </code></pre>  <p>The code executes immediately. However when I change it to <code>n = 1000000</code> it takes about 4 minutes (time between two <code>Sys.time()</code> calls). When I check these jobs in console on port :4040, job for <code>n = 1000</code> has duration 0.2s, and job for <code>n = 1000000</code> 0.3s. Am I doing something wrong?</p>