<p>Having <code>Spark-1.5.0</code> installed on my Mac machine, I'm trying to initialise spark context with <code>com.databricks:-csv_2.11:1.2.0</code> package in rStudio, as:</p>  <pre><code>Sys.setenv('SPARKR_SUBMIT_ARGS'='"--packages" "com.databricks:-csv_2.11:1.2.0" "sparkr-shell"') library(SparkR, lib.loc = "spark-1.5.0-bin-hadoop2.6/R/lib/") sc &lt;- sparkR.init(sparkHome = "spark-1.5.0-bin-hadoop2.6/") </code></pre>  <p>But I'm getting the following error message:</p>  <pre><code>[unresolved dependency: com.springml#spark-salesforce_2.10;1.0.1: not found] </code></pre>  <p>Why does that happen?</p>  <p>P.s., the initiliztation works fine when I use <code>com.databricks:spark-csv_2.10:1.0.3</code>. </p>  <blockquote>   <p>UPDATE</p> </blockquote>  <p>I tried to use the version com.databricks:spark-csv_2.10:1.2.0 and things work fine. </p>  <p>Now, I use this code in rStudio to load a csv file: </p>  <pre><code>sqlContext &lt;- sparkRSQL.init(sc) flights &lt;- read.df(sqlContext, "R/nycflights13.csv", "com.databricks.spark.csv", header="true") </code></pre>  <p>I get the following error message: </p>  <pre><code>Error in writeJobj(con, object) : invalid jobj 1 </code></pre>  <p>When I execute <code>sqlContext</code> I get the error:</p>  <pre><code>Error in callJMethod(x, "getClass") :    Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed. </code></pre>  <p>Session info:</p>  <pre><code>R version 3.2.0 (2015-04-16) Platform: x86_64-apple-darwin13.4.0 (64-bit) Running under: OS X 10.10.2 (Yosemite)  locale: [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8  attached base packages: [1] stats     graphics  grDevices utils     datasets  methods   base       other attached packages: [1] SparkR_1.5.0 rJava_0.9-7   loaded via a namespace (and not attached): [1] tools_3.2.0 </code></pre>  <p>Note that I don't get this error when I use Spark Shell with the same commands. </p>