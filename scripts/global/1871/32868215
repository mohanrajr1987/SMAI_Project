<p>I have to face this situation:</p>  <p>given N number of MPI nodes and given a 2D real array of [M,K] dimension</p>  <p>I have to partition it into in order to speed up calculus, giving to each node a subsection of 2D array and taking advantage of number of nodes.</p>  <p>Following Fortran way to store data in memory, arrays are indexed using the most rapidly changing variable first, every [:,i]-column of the array is "logically" separated from the others.</p>  <p>I have looked around to very illuminating questions like this one <a href="http://stackoverflow.com/questions/17508647/sending-2d-arrays-in-fortran-with-mpi-gather/">Sending 2D arrays in Fortran with MPI_Gather</a> </p>  <p>And I have reached the idea of using <code>mpi_scatterv</code> and <code>mpi_gatherv</code>, BUT I'm stuck against the fact that, since in the problem constraints, there is no possibility to guarantee that for each MPI node it is given the same amount of data, or, in pseudo code: </p>  <p><code>#Number_of_MPI_nodes != M*K</code></p>  <p>I was looking to use vectors, since each "column" has is own "independent" series of data, when I say "independent" I mean that I have to do some manipulation on the data belonging the same column, without affecting other columns.</p>  <p>Obviously, since the inequality given, some MPI nodes will have a different number of "columns" to analyze.</p>  <p>After doing some math, I need to gather back the data, using <code>mpi_gatherv</code></p>  <p>I will update the question with a working example in a few hours!</p>  <p>Thanks a lot to everybody !</p>  <p>CODE:</p>  <pre><code>program main  use mpi  implicit none  integer:: N_COLS=100, N_ROWS=200 integer:: i, j integer:: ID_mpi, COM_mpi, ERROR_mpi integer:: master = 0, SIZE_mpi=0  integer:: to_each_cpu=0, to_each_cpu_oddment=0 integer:: sub_matrix_size=0  integer:: nans=0, infs=0, array_split =0, my_type=0  integer ,dimension(:), allocatable :: elem_to_each_cpu integer ,dimension(:), allocatable :: displacements integer,parameter:: seed = 12345  character*160:: message  real :: tot_sum = 0.0  real ,dimension(:,:), allocatable:: Data_Matrix real ,dimension(:,:), allocatable:: sub_split_Data_Matrix  call srand(seed)     call MPI_INIT(ERROR_mpi) COM_mpi = MPI_COMM_WORLD call MPI_COMM_RANK(COM_mpi,ID_mpi,ERROR_mpi) call MPI_COMM_SIZE(COM_mpi,SIZE_mpi,ERROR_mpi)  !! allocation Data_Matrix  i = 1; j = 1 if (ID_mpi .eq. master) then     i = N_ROWS; j = N_COLS end if allocate(Data_Matrix(i, j))  do j = 1, N_COLS     do i = 1, N_ROWS         Data_Matrix(i, j) = rand()         tot_sum = tot_sum + Data_Matrix(i, j)     enddo enddo  write(message,*) "N_COLS:",N_COLS, "N_ROWS:", N_ROWS, " TOTAL_SUM:", tot_sum write(*,*) message  !! SINCE THERE ARE NO RESTRICTIONS ON MPI NUMBER OR CPUS OR  !! SIZE OR Data_Matrix I NEED TO DO THIS  to_each_cpu =N_COLS / SIZE_mpi to_each_cpu_oddment = N_COLS -( to_each_cpu * SIZE_mpi )  allocate(elem_to_each_cpu(SIZE_mpi)) elem_to_each_cpu = to_each_cpu allocate(displacements(SIZE_mpi)) displacements = 0  !! I CHOOSE TO SPLIT THE DATA IN THIS WAY if (ID_mpi .eq. master) then      write(message,*) "N_COLS:",N_COLS, "mpisize:", SIZE_mpi, "to_each_cpu\oddment:", to_each_cpu, " \ ", to_each_cpu_oddment     write(*,*) message      j=1     do i = 1 , to_each_cpu_oddment         elem_to_each_cpu(j) = elem_to_each_cpu(j) + 1         j = j + 1         if(j .gt. SIZE_mpi) j = 1     enddo      do j = 2, SIZE_mpi         displacements(j) = elem_to_each_cpu(j-1) + displacements(j-1)     enddo      do i = 1 , SIZE_mpi         write(message,*)i, " to_each_cpu:", &amp;         elem_to_each_cpu(i), " sub_split_buff_displ:",displacements(i), "=",elem_to_each_cpu(i)+displacements(i)         write(*,*) message     enddo  end if  call MPI_BCAST(elem_to_each_cpu, SIZE_mpi, MPI_INT, 0, COM_mpi, ERROR_mpi) call MPI_BCAST(displacements, SIZE_mpi, MPI_INT, 0, COM_mpi, ERROR_mpi)  allocate( sub_split_Data_Matrix(N_ROWS,elem_to_each_cpu(ID_mpi+1)) )  call MPI_TYPE_VECTOR(N_COLS,N_ROWS,N_ROWS,MPI_FLOAT,my_type,ERROR_mpi)  call MPI_TYPE_COMMIT(my_type, ERROR_mpi)   sub_split_Data_Matrix=0 sub_matrix_size = N_ROWS*elem_to_each_cpu(ID_mpi+1)  call MPI_scatterv( Data_Matrix,elem_to_each_cpu,displacements,&amp;     MPI_FLOAT, sub_split_Data_Matrix, sub_matrix_size ,MPI_FLOAT, &amp;     0, COM_mpi, ERROR_mpi)  !!! DOING SOME MATH ON SCATTERED MATRIX   call MPI_gatherv(&amp;     sub_split_Data_Matrix, sub_matrix_size,MPI_FLOAT ,&amp;     Data_Matrix, elem_to_each_cpu, displacements, &amp;     MPI_FLOAT, 0, COM_mpi, ERROR_mpi)  !!! DOING SOME MATH ON GATHERED MATRIX  tot_sum = 0.0 do j = 1, N_COLS     do i = 1, N_ROWS         tot_sum = tot_sum + Data_Matrix(i, j)     enddo enddo  write(message,*) "N_COLS:",N_COLS, "N_ROWS:", N_ROWS, " TOTAL_SUM:", tot_sum write(*,*) message   deallocate(Data_Matrix)  if (ID_mpi .eq. master) then     deallocate(elem_to_each_cpu )     deallocate(displacements ) endif  deallocate(sub_split_Data_Matrix)  end  </code></pre>  <p>RESULT:</p>  <p>Error occurred in MPI_Gahterv on communicator MPI_COMM_WORLD</p>  <p>Invalid memory reference</p>