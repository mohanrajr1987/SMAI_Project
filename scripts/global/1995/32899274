<p>I am loading 100m records into Spark from a textfile, then writing them as parquet partitioned by date. It fails repeatedly with </p>  <pre><code>15/10/01 16:46:14 WARN BlockReaderFactory: BlockReaderFactory(fileName=/user/vora/fact_pos/CALENDAR_DATE=2006-05-18/part-r-00318-831b6a4a-562d-45c2-bd58-d94be989b0bf.gz.parquet, block=BP-784895517-10.32.50.16-1441344668219:blk_1073765006_24182): error creating ShortCircuitReplica. java.io.IOException: Illegal seek at sun.nio.ch.FileDispatcherImpl.pread0(Native Method) </code></pre>  <p>Pyspark commands:        </p>  <pre><code>... lines = sc.textFile("file:///path/to/file/fact_pos_100m.csv")     ... schemaPeople.write.format("parquet").mode("overwrite").partitionBy("CALENDAR_DATE").save("fact_pos") p_parquet = sqlContext.read.option("mergeSchema", "true").parquet("fact_pos") </code></pre>  <p>The HDFS logs dont indicate anything awry. This is Spark 1.4.1 running on Hortonworks 2.2 (HDP 2.2.6.0-2800) on a single node cluster SLES 11.3 The process is not running out of memory (I've allocated 100gb; open_files (4096) or disk space (terabytes)). I'm loading the first 100m rows of a 3.2bn row file.</p>  <p>My question: Is there a better way to read a CSV file into spark and write it back to HDFS as a partitioned parquet file? I need to process a much larger file, and I'd prefer to do it without chopping up the file; but spark would need to pipe the data to the parquet partitions. What is the best technique to achieve this?</p>  <p>Thanks Mark Teehan Singapore</p>