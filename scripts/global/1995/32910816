<p>My configuration is :</p>  <pre><code>cluster : 1 master, 3 slaves on ec2. spark : 1.3.1 with hive hadoop : 1.0.4 </code></pre>  <p>My code :</p>  <pre><code>println("TableNames" + ss.hiveContext.tableNames().toList)   val t = ss.hiveContext.sql(s"""Select * from currency""") println("currency.count" + t.count()) </code></pre>  <p>The first line work fine I got all the tables name and currency exist, but I got this error when I try to select my table:</p>  <pre><code>WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN LoadSnappy: Snappy native library not loaded INFO Client: Retrying connect to server: ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010. Already tried 0 time(s); maxRetries=45 INFO Client: Retrying connect to server: ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010. Already tried 1 time(s); maxRetries=45 INFO Client: Retrying connect to server: ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010. Already tried 2 time(s); maxRetries=45 ....     INFO Client: Retrying connect to server: ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010. Already tried 43 time(s); maxRetries=45 INFO Client: Retrying connect to server: ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010. Already tried 44 time(s); maxRetries=45 WARN DAGScheduler: Creating new stage failed due to exception - job: 0    java.net.SocketTimeoutException: Call to ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010 failed on socket timeout exception: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com/10.123.212.217:9010] at org.apache.hadoop.ipc.Client.wrapException(Client.java:1146) at org.apache.hadoop.ipc.Client.call(Client.java:1118) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229) at com.sun.proxy.$Proxy14.getProtocolVersion(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl </code></pre>  <p>The hostname : ec2-XX-XXX-XXX-XX.compute-1.amazonaws.com is not the current master's hostname  or slaves' hostname.</p>  <p>I checked the conf <em>hive.metastore.warehouse.dir</em> of the hivecontext (with ss.hiveContext.getAllConfs.toList in my code) and the path of the directory is good. (hdfs://current_hostname:9010/warehouse...).</p>  <p>Before this 3 lines of code I get a RDD from HBase and it work fine, so my master can connect to my 3 slaves.</p>  <p>I already try to stop and to restart my cluster and also to stop and to restart hdfs and spark daemons.</p>  <p><strong>Any idea why my application try to connect to an unknown address?</strong></p>