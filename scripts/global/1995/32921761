<p>I have created a hadoop multinode cluster and also configured SSH in both master and slave nodes now i can connect to slave without password in master node</p>  <p>But when i try to start-dfs.sh in master node I'm unable to connect to slave node the execution stops at below line</p>  <p>log:</p>  <pre><code>HNname@master:~$ start-all.sh starting namenode, logging to /usr/local/hadoop/libexec/../logs/hadoop-HNname-namenode-master.out HDnode@slave's password: master: starting datanode, logging to /usr/local/hadoop/libexec/../logs/hadoop-HNname-datanode-master.out </code></pre>  <p>I pressed Enter</p>  <pre><code>slave: Connection closed by 192.168.0.2 master: starting secondarynamenode, logging to /usr/local/hadoop/libexec/../logs/hadoop-HNname-secondarynamenode-master.out jobtracker running as process 10396. Stop it first. HDnode@slave's password: master: starting tasktracker, logging to /usr/local/hadoop/libexec/../logs/hadoop-HNname-tasktracker-master.out  slave: Permission denied, please try again. HDnode@slave's password: </code></pre>  <p>after entering the slave password the connection is closed</p>  <p>Below things I have tried but no results:</p>  <ol> <li>formatted namenode in both master &amp; slave node</li> <li>created new ssh key and configured in both the nodes</li> <li>override the default HADOOP_LOG_DIR form the <a href="http://stackoverflow.com/questions/11540591/unable-to-start-daemons-using-start-dfs-sh">this</a> post  </li> </ol>