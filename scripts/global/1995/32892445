<p>I have a cronjob that that downloads zip files (200 bytes to 1MB) from a server on the internet every 5 minutes.  If I import the zip files into HDFS as is, I encounter the infamous Hadoop small file size issue.  In order to avoid the build up of small files in HDFS, process of the the text data in the zip files and convert them into avro files and wait every 6 hours to add my avro file into HDFS.  Using this method, I have managed to get avro files imported into HDFS with a file size larger than 64MB.  The files sizes range from 50MB to 400MB.  What I'm concerned about is that what happens if I start building file sizes that start getting into the 500KB avro file size range or larger.  Will this cause issues with Hadoop?  How does everyone else handle this situation?</p>