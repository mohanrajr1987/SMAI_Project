<p>So, in weakly-typed C (IIRC), 0 is evaluated as false in a boolean context (like a conditional expression) and everything else is truthy. Modern dynamic languages seem to split on whether or not zero is falsy: python, php, and javascript say yes/ruby, lua, clojure, and racket say no.</p>  <p>Question is, why? I googled for my question title and found <a href="https://news.ycombinator.com/item?id=2266083" rel="nofollow">this</a> thread on hacker news which had interesting stuff but not the answer to my question. Substituting 'lisp' for clojure in my search yielded no historical reference.</p>  <p>So to be more precise in formulating my question: is there an actual technical advantage to zero evaluating as true in a conditional expression, or did it just become a quasi-standard in dynamic languages (that weren't as influenced by C) courtesy of the early lisps? Its definitely a bit odd to a programmer used to C/python/javascript.</p>