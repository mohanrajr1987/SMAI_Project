<p>We're dealing with a huge number of shards (+70k), which makes our ES (v 1.6.0, replica 1, 5 shards per index) not so reliable. We're in the process of deleting indices, but we're noticing that there's a spike of refresh_mapping tasks after each individual delete (if it matters, these delete actions are performed via the REST api). This can be a problem, because subsequent DELETE request will be interleaved with the refresh-mapping tasks, and eventually they will timeout.</p>  <p>For example, here's the output of <code>_cat/pending_tasks</code> when deleting an index.</p>  <pre><code>3733244    1m URGENT delete-index [test.class_jump.2015-07-16] 3733245 210ms HIGH   refresh-mapping [business.bear_case1validation.2015-09-19][[bear_case1Validation]] 3733246 183ms HIGH   refresh-mapping [business.bear_case1validation.2015-09-15][[bear_case1Validation]] 3733247 156ms HIGH   refresh-mapping [search.cube_scan.2015-09-24][[cube_scan]] 3733248 143ms HIGH   refresh-mapping [business.bear_case1validation.2015-09-17][[bear_case1Validation]] 3733249 117ms HIGH   refresh-mapping [business.bear_case1validation.2015-09-22][[bear_case1Validation]] 3733250  85ms HIGH   refresh-mapping [search.santino.2015-09-25][[santino]] 3733251  27ms HIGH   refresh-mapping [search.santino.2015-09-25][[santino]] 3733252   9ms HIGH   refresh-mapping [business.output_request_finalized.2015-09-22][[output_request_finalized]] 3733253   2ms HIGH   refresh-mapping [business.bear_case1validation.2015-08-19][[bear_case1Validation]] </code></pre>  <p>There are two things which we don't understand:</p>  <ul> <li><p>Why are refresh_mappings being triggered? Maybe they are always triggered, but now visible because they are queued behind the URGENT task. Is this the case?</p></li> <li><p>Why are they triggered on "old" indices which do not change anymore? (the indices being refreshed are from one to two weeks old. The one being deleted is two weeks old as well)</p></li> </ul>  <p>Could this be caused by load rebalancing between nodes? It seems odd, but nothing else comes to mind. Moreover, seems that there are few documents (see below) in there, so load rebalancing seems an extreme longshot.</p>  <p><code>_cat/shards for test.class_jump.2015-07-16</code></p>  <pre><code>index                                                 state      docs    store  test.class_jump.2015-07-16                        2 r STARTED       0     144b 192.168.9.240 st-12  test.class_jump.2015-07-16                        2 p STARTED       0     108b 192.168.9.252 st-16  test.class_jump.2015-07-16                        0 p STARTED       0     144b 192.168.9.237 st-10  test.class_jump.2015-07-16                        0 r STARTED       0     108b 192.168.7.49  st-01  test.class_jump.2015-07-16                        3 p STARTED       1   15.5kb 192.168.7.51  st-03  test.class_jump.2015-07-16                        3 r STARTED       1   15.5kb 192.168.10.11 st-18  test.class_jump.2015-07-16                        1 r STARTED       0     144b 192.168.9.107 st-08  test.class_jump.2015-07-16                        1 p STARTED       0     144b 192.168.7.48  st-00  test.class_jump.2015-07-16                        4 r STARTED       1   15.6kb 192.168.10.65 st-19  test.class_jump.2015-07-16                        4 p STARTED       1   15.6kb 192.168.9.106 st-07  </code></pre>  <p>Is there any way in which these can be supressed? And more importantly, any way to speed up Index Deletion?</p>  <p>Thanks in advance</p>