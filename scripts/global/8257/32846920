<p>I try to use <a href="https://github.com/patriksimek/node-mssql" rel="nofollow">Node.Js connector</a> with <a href="https://github.com/Azure/node-sqlserver" rel="nofollow">Microsoft driver</a> to communicate with a SQL Server. In the connector docs I've found a good option named 'stream'. It add ability to asynchronously obtain row objects. </p>  <p>My data have some specific - some columns have large binary data (> 100 Mb). So even one row may be really large. I'm looking for ability to get each row data as a stream. It is possible in .NET driver (<a href="https://msdn.microsoft.com/library/system.data.commandbehavior(v=vs.110).aspx" rel="nofollow">CommandBehavior.SequentialAccess</a> enumeration). Is it possible in Node.js?</p>  <p>UPDATED Here is a code to demonstrate the problem:</p>  <p>Custom writable stream module:</p>  <pre><code>var stream = require('stream'); var util = require('util');  function WritableObjects() {      stream.Writable.call(         this,         {             objectMode: true         }     );  }  util.inherits( WritableObjects, stream.Writable );  WritableObjects.prototype._write = function( chunk, encoding, doneWriting ) {     console.log('write', chunk, encoding);     doneWriting(); };   module.exports = {     WritableObjects: WritableObjects }; </code></pre>  <p>and database query code:</p>  <pre><code>var sw = new wstream.WritableObjects(); var request = new sql.Request(connection); request.stream = true; request.pipe(sw); request.query('SELECT DataId, Data FROM ds.tData WHERE DataId in (1)');  sw.on('error', function(err) {     console.log('Stream err  ', err) });  sw.on('pipe', function(src) {     console.log('Stream pipe ') });  sw.on('finish', function(data) {     console.log('Stream finish') }); </code></pre>  <p>I this example <code>chunk</code> parameter of <code>_write</code> method contains the whole data of db record, not a stream. Because <code>Data</code> field contains a big varbinary data memory of node process also grows huge.</p>