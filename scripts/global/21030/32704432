<p>I'm currently working on an augmented reality application using a medical imaging program called 3DSlicer. My application runs as a module within the Slicer environment and is meant to provide the tools necessary to use an external tracking system to augment a camera feed displayed within Slicer. </p>  <p>Currently, everything is configured properly so that all that I have left to do is automate the calculation of the camera's extrinsic matrix, which I decided to do using OpenCV's <code>solvePnP()</code> function. Unfortunately this has been giving me some difficulty as I am not acquiring the correct results.</p>  <p>My tracking system is configured as follows: </p>  <ul> <li>The optical tracker is mounted in such a way that the entire scene can be viewed.</li> <li>Tracked markers are rigidly attached to a pointer tool, the camera, and a model that we have acquired a virtual representation for.</li> <li>The pointer tool's tip was registered using a pivot calibration. This means that any values recorded using the pointer indicate the position of the pointer's tip.</li> <li>Both the model and the pointer have 3D virtual representations that augment a live video feed as seen below.</li> <li>The pointer and camera (Referred to as C from hereon) markers each return a homogeneous transform that describes their position relative to the marker attached to the model (Referred to as M from hereon). The model's marker, being the origin, does not return any transformation.</li> </ul>  <p><img src="http://i.stack.imgur.com/NnKr7.jpg" alt="Depiction of correctly augmented environment. Extrinsic was manually adjusted to demonstrate process is correct, but is inaccurate."></p>  <p>I obtained two sets of points, one 2D and one 3D. The 2D points are the coordinates of a chessboard's corners in pixel coordinates while the 3D points are the corresponding world coordinates of those same corners relative to M. These were recorded using openCV's <code>detectChessboardCorners()</code> function for the 2 dimensional points and the pointer for the 3 dimensional. I then transformed the 3D points from M space to C space by multiplying them by C inverse. This was done as the <code>solvePnP()</code> function requires that 3D points be described relative to the world coordinate system of the camera, which in this case is C, not M.</p>  <p>Once all of this was done, I passed in the point sets into <code>solvePnp()</code>. The transformation I got was completely incorrect, though. I am honestly at a loss for what I did wrong. Adding to my confusion is the fact that OpenCV uses a different coordinate format from OpenGL, which is what 3DSlicer is based on. If anyone can provide some assistance in this matter I would be exceptionally grateful. </p>  <p>Also if anything is unclear, please don't hesitate to ask. This is a pretty big project so it was hard for me to distill everything to just the issue at hand. I'm wholly expecting that things might get a little confusing for anyone reading this.</p>  <p>Thank you!</p>  <p>UPDATE #1: It turns out I'm a giant idiot. I recorded colinear points only because I was too impatient to record the entire checkerboard. Of course this meant that there were nearly infinite solutions to the least squares regression as I only locked the solution to 2 dimensions! My values are much closer to my ground truth now, and in fact the rotational columns seem correct except that they're all completely out of order. I'm not sure what could cause that, but it seems that my rotation matrix was mirrored across the center column. In addition to that, my translation components are negative when they should be positive, although their magnitudes seem to be correct. So now I've basically got all the right values in all the wrong order. </p>