<p>I have 1 Source number set (or vector) with 50 integer numbers between 1 and 100 (= a profile with 50 characteristics rated between 1 and 100)</p>  <p>I have 1,000,000 Target number sets (or vectors), also each with 50 integer numbers (= 1M more of such profiles)</p>  <p>I want to order the 1,000,000 Target sets by similarity to the Source set (= I want to browse through best to worst match)</p>  <p>I want the routine to return an ordered list in &lt;1 sec (triggered from web application, returned to laptop and/or mobile device) (= Performance requirement for end user)</p>  <p>I want the routine to do this even when there are 100 similar requests being submitted at the same time for different Source number sets (= Support for concurrent users)</p>  <p>The above numbers are to give an indication of scale. I may wind up with more integers in the set (new characteristics added) or with more Target sets (more profiles) or with more concurrent requests (more concurrent users), but this is good for now</p>  <p>If it helps, I am happy with results being clustered in buckets. I.e. top 10,000 results in a top 1% bucket. Next 10,000 results in a top 2% bucket, etc. I do not necessarily care about order within each such bucket (too granular a distinction for the end user to warrant any added performance investment).</p>  <p>Equally, because an end user would never fit 1,000,000 results in their UI, I am happy if the algorithm somehow only returns a smaller subset of returns (for example - only the top matches). But further browsing should still load the rest of the list in a performant way.</p>  <p>A practical use case; imagine a dating app where an end user is asked to rate, on a slider bar, 50 characteristics for how important they are to them. And where the app returns the best matches, updated 'live' as you makes edits.</p>  <p>Which techniques / type of algorithms are best suited to tackle this requirement?</p>  <p>Initial attempts use Squared Euclidean Distance, but this proves too resource intensive given the size of data and the performance requirements.  Bigger differences for a given characteristic ideally do carry more weight (as they do when using the Squared Euclidean Distance method)</p>