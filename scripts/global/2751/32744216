<p>I have 2 GPU. GPU1 is responsible of computing a 'something' and store the result in buffer1 (located in GPU1 space memory).</p>  <p>GPU2 is responsible of coding the result in H264. To this aim, I alloc memory buffer2 in GPU2 memory space and the encoder configurations is set to use GPU2. Buffer1 is copied to buffer2 before start the coding. I tried to approaches:</p>  <ol> <li>Use cudaMemcpyPeerAsync to copy from GPU1 memory space to GPU2 memory space</li> <li>Use a simple cuMemcpyAsync</li> </ol>  <p>Approach 2 works even (apparently) it makes no sense.</p>  <p>Questions:</p>  <ul> <li>Does approach 2 works because of the Unified Memory Access? </li> <li>If this is true, should I expect a performance drop? Because GPU load is exactly the same in both cases.</li> <li>While both operations are async, which stream should I use? One created in GPU1 or GPU2?</li> </ul>