<p>In a <a href="http://stackoverflow.com/questions/32701273/perform-sum-of-vectors-in-cuda-thrust">previous question</a> we showed that it is possible to perform reduction operations (like sum) on a matrix stored in a 1D vector in thrust, realized as a "flat" vector of (ROWS*COLS) elements.</p>  <p>What I'm trying to do now is perform a transformation on the rows of the matrix. Here's the plain CUDA kernel that I've written, since I couldn't do something similar using thrust. It takes the data which is a matrix, the labels which is a vector, the weights which is also a vector and the gradients which is the matrix that gets populated through this kernel. R and C are the num of rows and columns.</p>  <pre><code>__global__ void calculate_gradients(     float * data_array_d, float * label_vector_d,     float * weights_d, float * gradients_d, int R, int C) {  // Each thread should get one data point which is one row of the matrix int thread_index = blockIdx.x * blockDim.x + threadIdx.x; int example_index = thread_index * C;  // TODO: Vector operations should be done with cuBLAS using dynamic parallelism if (thread_index &lt; R) {      // Calculate prediction     float prediction = 0.0;      for (int i = 0; i &lt; C; ++i) {         prediction += data_array_d[example_index + i] * weights_d[i];     }     // For squared loss the loss derivative is equal to the simple loss: (y_hat - y_true)     float loss_derivative = prediction - label_vector_d[thread_index];      // Scale the gradient by the loss_derivative     for (int i = 0; i &lt; C; ++i) {         // For linear models the gradient equals the features         gradients_d[example_index + i] = loss_derivative * data_array_d[example_index + i];     } } </code></pre>  <p>}</p>  <p>The question is then: How would one perform a transformation on each row of a matrix stored using a 1D vector in thrust?</p>  <p>My use-case is stochastic gradient descent, so I would like to be able to take an array of floats (<code>examples x features</code>) and dot product each row with the same weight vector to get a column vector of prediction values. The result would be a column vector with ROWS items (scalars).</p>