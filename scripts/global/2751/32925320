<p>As known: <a href="https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications" rel="nofollow">https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications</a></p>  <blockquote>   <p>For CC 3.0 - 5.2</p>      <p>Maximum number of resident threads per multiprocessor: <strong>2048</strong></p> </blockquote>  <p>Also we know: For Maxwell architecture <strong>1 SMM(multiprocessor) has 128 CUDA-Cores</strong></p>  <p>And there is written: <a href="http://devblogs.nvidia.com/parallelforall/maxwell-most-advanced-cuda-gpu-ever-made/" rel="nofollow">http://devblogs.nvidia.com/parallelforall/maxwell-most-advanced-cuda-gpu-ever-made/</a></p>  <blockquote>   <p>SMM uses a quadrant-based design with four 32-core processing blocks   each with a dedicated <strong>warp scheduler capable of dispatching two   instructions per clock</strong>.</p> </blockquote>  <p>I.e. in 1 SMM(multiprocessor) there are:</p>  <ul> <li><strong>2048 CUDA-threads</strong></li> <li>which executed on <strong>128 CUDA-Cores</strong> </li> <li>which can execute (2*128) = <strong>256 instructions per clock</strong></li> </ul>  <p>Because resident CUDA-threads per multiprocessor more than number of CUDA-Cores, then this means that Maxwell uses one of two hardware-multithreading:</p>  <ol> <li><p>or <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading" rel="nofollow">SMT (Simultaneous multithreading)</a></p></li> <li><p>or <a href="https://en.wikipedia.org/wiki/Temporal_multithreading" rel="nofollow">TM (Temporal multithreading) fine-grained</a>, not     coarse-grained</p></li> </ol>  <p>Because even 256 instructions per clock can't provide simultaneous execution of 2048 CUDA-threads, then GPU exactly uses TM (Temporal multithreading) fine-grained, when 1 Core stores the state of 8 Threads and uses a hardware thread switch.</p>  <p>If warp scheduler capable of dispatching two instructions per clock only for one thread, then this dispatching using only only for superscalar - parallelization independent instructions inside one thread. </p>  <p>But if warp scheduler capable of dispatching two instructions per clock for two threads simultaneous, then GPU uses both SMT (Simultaneous multithreading) and TM (Temporal multithreading) fine-grained.</p>  <p>Does CUDA-GPU (Maxwell) use dispatching two instructions per clock only for superscalar or for SMT, too?</p>  <p>In other words, does nVidia GPU Maxwell use any of?:</p>  <ul> <li><a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading" rel="nofollow">SMT (Simultaneous multithreading)</a></li> <li><a href="https://en.wikipedia.org/wiki/Temporal_multithreading" rel="nofollow">TM (Temporal multithreading) fine-grained</a></li> <li><a href="https://en.wikipedia.org/wiki/Temporal_multithreading" rel="nofollow">TM (Temporal multithreading) coarse-grained</a></li> <li><a href="https://en.wikipedia.org/wiki/Super-threading" rel="nofollow">Super-threading (or time-slice multithreading)</a></li> </ul>  <p>It is very important to select the way of optimization.</p>