<p>So here's the situation.</p>  <p>I have a block of threads running a while loop and I need the loop to continue if and only if some condition is met by any of these threads. To do that I use a shared variable as the continue flag, the flag is cleared by thread #0 at the beginning of each iteration, followed by a <code>__syncthreads()</code>, and can be set by any thread during the iteration if the continue condition is met. Then another call to <code>__syncthreads()</code> is placed before the checking point of next iteration to make sure threads are synchronized. The kernel is basically like this:</p>  <pre><code>__global__ void foo(void* data) {     __shared__ int blockContinueFlag;     do {         if (threadIdx.x || threadIdx.y || threadIdx.z) {             blockContinueFlag = 0;         }         __syncthreads(); //synch1         //some data manipulations...         if(some predicate) {             blockContinueFlag = true;         }         //some data manipulations...         __syncthreads(); //synch2     } while (blockContinueFlag); } </code></pre>  <p>The problem is the barrier synch2 doesn't seem to work in my code, sometimes the kernel terminates even when the continue condition is met by some threads (I know this by checking returned data on host side). To further exam this I set a break point just after the do-while loop like following code , where sometimes the <code>blockContinueFlag</code> is said <code>true</code> (I can only assume the block exited the loop prior to some threads can set <code>blockContinueFlag</code>).</p>  <pre><code>__global__ void foo(void* data) {     __shared__ int blockContinueFlag;     do {         if (threadIdx.x || threadIdx.y || threadIdx.z) {             blockContinueFlag = 0;         }         __syncthreads(); //synch1         //some data manipulations...         if(some predicate) {             blockContinueFlag = true;         }         //some data manipulations...         __syncthreads(); //synch2     } while (blockContinueFlag);     //a break point is set here } </code></pre>  <p>I remember reading from cuda manual that <code>__syncthreads()</code> is allowed in conditional clause if the predicate is evaluated same for all threads, which should be in this case.</p>  <p>I have another simplified version of code just as an illustration for this.</p>  <pre><code>__global__ void foo(int* data, int kernelSize, int threshold) {     __shared__ int blockContinueFlag;     do {         if (threadIdx.x == 0) {             blockContinueFlag = 0;         }         __syncthreads();         if (threadIdx.x &lt; kernelSize)  {             data[threadIdx.x]--;             for (int i = 0; i &lt; threadIdx.x; i++);             if (data[threadIdx.x] &gt; threshold)                 blockContinueFlag = true;         }         __syncthreads();     } while (blockContinueFlag); }  int main() {     int hostData[1024], *deviceData;     for (int i = 0; i &lt; 1024; i++)         hostData[i] = i;     cudaMalloc(&amp;deviceData, 1024 * sizeof(int));     cudaMemcpy(deviceData, hostData, 1024 * sizeof(int), cudaMemcpyHostToDevice);     foo &lt;&lt; &lt;1, 1024 &gt;&gt; &gt;(deviceData, 512, 0);     cudaDeviceSynchronize();     cudaMemcpy(hostData, deviceData, 1024 * sizeof(int), cudaMemcpyDeviceToHost);     fprintf(stderr, cudaGetErrorString(cudaGetLastError()));     return 0;  } </code></pre>  <p>The expected value for <code>hostData[]</code> would be <code>{-511, -510, -509, ..., 0, 512, 513, 514,..., 1023}</code> at the end of <code>main()</code>, which is sometimes the actual case. But in some case it produces following values in VS 2013 debug mode</p>  <pre><code>[0]: -95 [1]: -94 ... [29]: -66 [30]: -65 [31]: -64 [32]: 31 [33]: 32 [34]: 33 ... [61]: 60 [62]: 61 [63]: 62 [64]: -31 [65]: -30 [66]: -29 ... [92]: -3 [93]: -2 [94]: -1 [95]: 0 [96]: 95 [97]: 96 [98]: 97 ... </code></pre>  <p>, which suggests warps are not actually synchronized.</p>  <p>So does anyone know the reason for this and/or whether there is a way to let the thread barrier work correctly?</p>  <p>Any help would be appreciated. Thanks in advance.</p>