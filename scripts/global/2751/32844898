<p>I have a permutation <code>p</code>. Say that I wanted to sum a vector <code>v</code> and a permuted version of another vector <code>u</code>. For example, in Matlab it would be</p>  <pre><code>w = v + u(p) </code></pre>  <p>And this can be done easily in Thrust using a <code>permutation_iterator</code> and <code>thrust::transform</code>.</p>  <p>Now, how could I do the same with matrices? That is, I want to sum a matrix <code>A</code> with a matrix <code>B</code> after permuting the rows and columns of <code>B</code>. For example, in Matlab it would be</p>  <pre><code>C = A + B(p,p) </code></pre>  <p>Is there an efficient way to do this in Thrust? It's easy to do this with raw CUDA code, but as far as I understand using Thrust is more likely to result in better performance.</p>  <p>In case it helps: in the particular case I'm interested in, both <code>A</code> and <code>B</code> are square and symmetric (and therefore <code>B(p,p)</code> is also symmetric). More specifically, <code>B</code> is the pairwise distance matrix of a set of points and <code>p</code> is a relabelling of the points' indices.</p>  <p><strong>EDIT</strong>: Here's a naive CUDA kernel that would do the trick, although probably quite inefficiently.</p>  <pre><code>__global__ void matrixSumWithPermutation(float *A, float *B, float *C, int *p, const int N) {   int i = blockIdx.x * blockDim.x + threadIdx.x;   int j = blockIdx.y * blockDim.y + threadIdx.y;    if ((i &lt; N) &amp;&amp; (j &lt; N)) {     C[i*N + j] = A[i*N + j] + B[p[i]*N + p[j]];   } } </code></pre>  <p>As an example, say that both <code>A</code> and <code>B</code> are 3x3 matrices. Then <code>p</code> contains 0,1,2 in any order (using C's zero-based numbering). For example, if <code>p = (2,0,1)</code> it would result in the following permutation:</p>  <pre><code>     0  10  20                  0  20  30 B = 10   0  30    ;   B(p,p) = 20   0  10     20  30   0                 30  10   0 </code></pre>