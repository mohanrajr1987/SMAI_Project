<p>I have been experiencing a strange behaviour when I launch 2 instances of a kernel in order to run at the same time while sharing the GPU resources.</p>  <p>I have developed a CUDA kernel which aims to run in a single SM (Multiprocessor) where the threads perform an operation several times (with a loop).</p>  <p>The kernel is prepared to create only a block, therefore to use only one SM.</p>  <blockquote>   <p>simple.cu</p> </blockquote>  <pre><code>#include &lt;cuda_runtime.h&gt; #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;helper_cuda.h&gt; using namespace std;  __global__ void increment(float *in, float *out) {     int it=0, i = blockIdx.x * blockDim.x + threadIdx.x;     float a=0.8525852f;      for(it=0; it&lt;99999999; it++)              out[i] += (in[i]+a)*a-(in[i]+a); }  int main( int argc, char* argv[]) {     int i;     int nBlocks = 1;     int threadsPerBlock = 1024;     float *A, *d_A, *d_B, *B;     size_t size=1024*13;      A = (float *) malloc(size * sizeof(float));     B = (float *) malloc(size * sizeof(float));      for(i=0;i&lt;size;i++){             A[i]=0.74;             B[i]=0.36;     }      cudaMalloc((void **) &amp;d_A, size * sizeof(float));     cudaMalloc((void **) &amp;d_B, size * sizeof(float));      cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);     cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);      increment&lt;&lt;&lt;nBlocks,threadsPerBlock&gt;&gt;&gt;(d_A, d_B);      cudaDeviceSynchronize();      cudaMemcpy(B, d_B, size, cudaMemcpyDeviceToHost);      free(A);     free(B);      cudaFree(d_A);     cudaFree(d_B);      cudaDeviceReset();      return (0); } </code></pre>  <p>So if I execute the kernel:</p>  <p><code>time ./simple</code></p>  <p>I get </p>  <p><code>real   0m36.659s  user   0m4.033s  sys    0m1.124s</code></p>  <p>Otherwise, If I execute two instances:</p>  <p><code>time ./simple &amp; time ./simple</code></p>  <p>I get for each process: </p>  <p><code>real   1m12.417s  user   0m29.494s  sys    0m42.721s</code></p>  <p><code>real   1m12.440s  user   0m36.387s  sys    0m8.820s</code></p>  <p>As far as I know, the executions should run concurrently lasting as one (about 36 seconds). However, they last twice the base time. We know that the GPU has 13 SMs, each one should execute one block, thus the kernels only create 1 block.</p>  <p>Are they being executed in the same SM?</p>  <p>Shouldnâ€™t they running concurrently in different SMs?</p>  <p>EDITED</p>  <p>In order to make me clearer I will attached the profiles of the concurrent execution, obtained from nvprof:</p>  <p>Profile, first instance <a href="http://i.stack.imgur.com/tifZz.png" rel="nofollow"><img src="http://i.stack.imgur.com/tifZz.png" alt="simple.cu profile, first instance"></a></p>  <p>Profile, second instance <a href="http://i.stack.imgur.com/MoJJ7.png" rel="nofollow"><img src="http://i.stack.imgur.com/MoJJ7.png" alt="simple.cu profile, second instance"></a></p>  <p>Now, I would like to show you the behavior of the same scenario but executing concurrently two instances of matrixMul sample:</p>  <p>Profile, first instance <a href="http://i.stack.imgur.com/Unibe.png" rel="nofollow"><img src="http://i.stack.imgur.com/Unibe.png" alt="enter image description here"></a> </p>  <p>Profile, second instance <a href="http://i.stack.imgur.com/D3zDU.png" rel="nofollow"><img src="http://i.stack.imgur.com/D3zDU.png" alt="enter image description here"></a></p>  <p>As you can see, in the first scenario, a kernel wait for terminating the other. While, in the second scenario (matrixMul), kernels from both contexts are running at the same time.</p>  <p>Thank you.</p>