<p>I have been working on a scrapy web scraper that crawls through all internal links from a start url and only collects external links with <code>scrapy</code>. However, my main problem is classifying the external links and internal links. For example, when I try to filter out external links with <code>link.startswith("http") or link.startswith("ftp") or link.startswith("www")</code>, if the website links its own website with an absolute path (<code>www.my-domain.com/about</code> instead of <code>/about</code>) then, it will classify it as the external link even if it's not. The following is my code:</p>  <pre><code>import scrapy from lab_relationship.items import Links  class WebSpider(scrapy.Spider):     name = "web"     allowed_domains = ["my-domain.com"]     start_urls = (         'www.my-domain.com',     )      def parse(self, response):         """ finds all external links"""         items = []         for link in set(response.xpath('//a/@href').extract()):             item = Links()             if len(link) &gt; 1:                 if link.startswith("/") or link.startswith("."):                     # internal link                     url = response.urljoin(link)                     item['internal'] = url                     #yield scrapy.Request(url, self.parse)                 elif link.startswith("http") or link.startswith("ftp") or link.startswith("www"):                     # external link                     item['external'] = link                 else:                     # misc. links: mailto, id (#)                     item['misc'] = link                 items.append(item)         return items </code></pre>  <p>Any suggestions?</p>