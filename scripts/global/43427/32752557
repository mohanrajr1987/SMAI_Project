<p>My spider codes are:</p>  <pre><code>class TryItem(Item):     url = Field()  class BbcSpiderSpider(CrawlSpider):     name = "bbc_spider"     allowed_domains = ["www.bbc.com"]     start_urls = ['http://www.bbc.com/sport/0/tennis']      rules = (Rule(LinkExtractor(allow=['.*sport\/0\/tennis\/\d{8}']), callback='parse_item', follow=True),)      def parse_item(self, response):         Item = TryItem()         Item['url'] = response.url         yield Item </code></pre>  <p>Through this spider, I am trying to collect the urls of all the articles on tennis. I use csv code: </p>  <pre><code>scrapy crawl bbc_spier -o bbc.csv -t csv </code></pre>  <p>The output I am looking for is:</p>  <pre><code>http://www.bbc.com/sport/0/tennis/34322294 http://www.bbc.com/sport/0/tennis/14322295 ... http://www.bbc.com/sport/0/tennis/12345678 </code></pre>  <p>But, the spider also returns nonmatching urls as well, such as:</p>  <pre><code>http://www.bbc.com/sport/0/tennis/29604652?print=true http://www.bbc.com/sport/0/tennis/34252190?comments_page=11&amp;filter=none&amp;initial_page_size=10&amp;sortBy=Created&amp;sortOrder=Descending </code></pre>  <p>Any suggestion? Thanks</p>