<p>I am pretty new to Scrapy. I am looking into using it to crawl an entire website for links, in which I would output the items into multiple JSON files. So I could then upload them to Amazon Cloud Search for indexing. Is it possible to split the items into multiple files instead of having just one giant file in the end? From what I've read, the Item Exporters can only output to one file per spider. But I am only using one CrawlSpider for this task. It would be nice if I could set a limit to the number of items included in each file, like 500 or 1000. </p>  <p>Here is the code I have set up so far (based off the Dmoz.org used in the tutorial):</p>  <p>dmoz_spider.py</p>  <pre><code>import scrapy  from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from tutorial.items import DmozItem  class DmozSpider(CrawlSpider):     name = "dmoz"     allowed_domains = ["dmoz.org"]     start_urls = [         "http://www.dmoz.org/",     ]      rules = [Rule(LinkExtractor(), callback='parse_item', follow=True)]      def parse_item(self, response):        for sel in response.xpath('//ul/li'):             item = DmozItem()             item['title'] = sel.xpath('a/text()').extract()             item['link'] = sel.xpath('a/@href').extract()             item['desc'] = sel.xpath('text()').extract()             yield item </code></pre>  <p>items.py</p>  <pre><code>import scrapy  class DmozItem(scrapy.Item):     title = scrapy.Field()     link = scrapy.Field()     desc = scrapy.Field() </code></pre>  <p>Thanks for the help.</p>