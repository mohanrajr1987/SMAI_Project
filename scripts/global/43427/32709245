<p>I run my crawler (see below), but it scrapes only page given in 'start_urls'. Empirically I found out that parameter 'restrict_xpaths' doesn't work.</p>  <pre><code># -*- coding: utf-8 -*-  from scrapy.spiders import CrawlSpider, Rule from ..items import Category from scrapy import Selector from scrapy.linkextractors import LinkExtractor   class NeoSpider(CrawlSpider):     name = 'neo'     allowed_domains = ['neopoliscasa.ru']     start_urls = ['http://www.neopoliscasa.ru/catalog.html']     identifier = 1     subcategory_parent_id = None     type_parent_id = None     categories = []     rules = (         Rule(             LinkExtractor(                 allow='/catalog/[a-z-]+.html',                 restrict_xpaths='//div[contains(@class, "itemTypeIcoon n")]'),             callback='parse_subcategories'),     )      def parse(self, response):         sel = Selector(response)         category_blocks = sel.xpath(             '//div[@class="rootCatalogItem"]')         for item in category_blocks:             category = Category()             category['category'] = ''.join(item.xpath(                 'h2/a/text()').extract())             category['id'] = unicode(self.identifier)             category['parent_id'] = unicode(0)             self.subcategory_parent_id = self.identifier             self.identifier += 1             self.categories.append(category)             yield category      def parse_subcategories(self, response):         #  do anything         pass </code></pre>  <p>How can I fix it? Thanks</p>