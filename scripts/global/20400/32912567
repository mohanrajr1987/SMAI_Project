<p>I have a dataset of 15M+ training instances in form of svmlight dataset. I read these data using sklearn.datasets.load_svmlight_file(). The data itself is not sparse, so I don't mind converting it to any other dense representation (I will prefer that).</p>  <p><em>At some point in my program I need to add millions of new data records (instances) to my training data (in random positions). I used vstack and also tried converting to dense matrices but was either inefficient or failed to give correct results (details below). Is there any way to do this task efficiently?</em></p>  <p>I'm implementing DAgger algorithm and in the first iteration it is trying to add about 7M new training instances. I want to add these new instances in random positions. I tried vstack (given my data was in csr format I was expecting it not to be too inefficient at least). However after 24hours it's not done yet. </p>  <p>I tried converting my data to numpy.matrix format just after loading them in svmlight format. A sampling showed it can help me speed things up but interestingly the results I get from training on the converted dataset and the original dataset seem not to match with each other. It appears sklearn does not work with numpy matrix in the way I thought. I couldn't find anything in the sklearn documentation.</p>  <p><em>Another approach I thought was to define a larger dataset from the beginning so that it will "reserve" enough space in memory, but when I'm using sklearn train or test features I'll index my dataset to the last "true" record. In this way, I presume, vstack will not require opening up a new large space in memory which can make the whole operation take longer. Any thoughts on this?</em></p>