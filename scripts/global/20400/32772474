<p>Is there a way I can allocate memory for scipy sparse matrix functions to process large datasets?</p>  <p>Specifically, I'm attempting to use Asymmetric Least Squares Smoothing (translated into python <a href="http://stackoverflow.com/questions/29156532/python-baseline-correction-library&gt;">here</a> and the original <a href="http://zanran_storage.s3.amazonaws.com/www.science.uva.nl/ContentPages/443199618.pdf" rel="nofollow">here</a>) to perform a baseline correction on a large mass spec dataset (length of ~60,000).</p>  <p>The function (see below) uses the scipy.sparse matrix operations. </p>  <pre><code>def baseline_als(y, lam, p, niter):   L = len(y)   D = sparse.csc_matrix(np.diff(np.eye(L), 2))   w = np.ones(L)   for i in xrange(niter):     W = sparse.spdiags(w, 0, L, L)     Z = W + lam * D.dot(D.transpose())     z = spsolve(Z, w*y)     w = p * (y &gt; z) + (1-p) * (y &lt; z)   return z </code></pre>  <p>I have no problem when I pass data sets that are 10,000 or less in length:</p>  <pre><code>baseline_als(np.ones(10000),100,0.1,10) </code></pre>  <p>But when passing larger data sets, e.g.</p>  <pre><code>baseline_als(np.ones(50000), 100, 0.1, 10) </code></pre>  <p>I get a MemoryError, for the line</p>  <pre><code>  D = sparse.csc_matrix(np.diff(np.eye(L), 2)) </code></pre>