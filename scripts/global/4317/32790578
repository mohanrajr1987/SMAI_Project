<p>I am writing a parallel code (written in parts in modules) in Fortran using MPI. I am getting few runtime errors. I can't put the code (which is huge) here as it would be very difficult to explain each and every part of it (moreover it is written in a very disorganized manner), but would like to ask the following question.</p>  <p>What are possible reasons for following runtime errors: </p>  <p>1) munmap_chunk(): invalid pointer ?</p>  <p>2) Program received signal SIGABRT: Process abort signal.    Backtrace for this error:</p>  <p>3) #7  0x42BB39 in 'code_name'.f90:145 (discriminator 23)</p>  <p>I did googled about them but it wasn't very clear what is the issue. Moreover, this error come up in turns (sometimes 2 of them). Also, there are no errors (successful run) for some very few selected total number of processors.</p>  <p>Any kind of help will be highly appreciated.</p>  <p>-------------------LATER EDIT-----------------------------</p>  <p>I have actually divided a plane into small parts and small each part in a different processor. Next each parts needs updated data from nearest neighbouring parts for own calculations. So, I have defined a grid/look up table for processors. I have checked the table (named as procs_table), it is as expected. Then I do the following,</p>  <pre><code>nextprocsy = procs_table(np_idy + 1,np_idz) prevprocsy = procs_table(np_idy - 1,np_idz)  nextprocsz = procs_table(np_idy,np_idz + 1) prevprocsz = procs_table(np_idy,np_idz - 1) </code></pre>  <p>where, present slot is <code>procs_table(np_idy,np_idz)</code>. </p>  <p>Then, </p>  <pre><code>! generate data to be send call mpi_barrier(mpi_comm_world,ierr)  ! sending data to nearby processors call mpi_isend(nexty_ax_send,(3*no_of_points*avg_no_per_process),mpi_real8,nextprocsy,1,mpi_comm_world,nexty1,ierr) ......... !many exactly similar commands ! recev data from nearby processors call mpi_irecv(prevy_ax_send,(3*no_of_points*avg_no_per_process),mpi_real8,prevprocsy,1,mpi_comm_world,nexty1,ierr) ! many exactly similar commands call mpi_wait(istatus,nexty1,ierr) call mpi_barrier(mpi_comm_world,ierr) ! calculations with received data </code></pre>  <p>I do compile with -g -fbacktrace -fcheck=all and then the only line which comes up is the line where the program ends with <code>(discriminator 23)</code>.</p>  <p>Please let me know via comments your input, meanwhile I would add more to code.</p>