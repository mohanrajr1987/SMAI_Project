<p>I read a little about PAWS (Protection Against Wrapping Sequence). It's very interesting. I didn't know such complicated things are implemented to guarantee the reliability of TCP. Without PAWS, in the case of high data rate, a delayed old packet can be received and regarded as the new packet by mistake.</p>  <p>I didn't think much about this before. But now I started to wonder how long a packet can stay in network (Especially UDP packet if the type of packet matters). A packet can be delayed, temporarily stay in the network before it's delivered. But it can only stay for a short period of time, right?</p>  <p>In other words, how much time does it take to wait for a (UDP) packet before concluding that it won't come?</p>  <p>If there is an answer, then how is it determined? How to estimate it? (for writing programs related to timeout of packet.)</p>  <hr>  <p>A simplified example: A server received 2 UDP packets. Each contains an integer to indicate the order. It got No.1 and No.3. It knows No.2 is either delayed or lost. After a period of time, No.2 still doesn't come then it concludes the packet is lost. The packet doesn't exist anymore. (So it won't cause any trouble for new packets in the future, similar to the problem PAWS aims to solve.) But how long should the the server wait before concluding No.2 doesn't exist anymore?</p>