<p>I need some help. I have to classify the wikilinks in Wikipedia articles based on their relative position in the article (in best case on the rendered page). For each wikilink I would like to obtain the position in text (i.e., a numbering ascending for each article or article/section) and if the link is contained in an infobox or a navibox. I need this information for a specific revision of every article in the English Wikipedia in the zero namespace. </p>  <p>So far, I first tried  parsing the wikitext myself from a dump file, but there are  some problems with replacing the templates. For example, if a template is embedded with parameters and/or with conditions, it is a bit difficult to know what exactly is rendered. Then, I tried out several parser from <a href="https://www.mediawiki.org/wiki/Alternative_parsers" rel="nofollow">https://www.mediawiki.org/wiki/Alternative_parsers</a> that claim to handle templates, but they did not work out mainly due the same problems that I had parsing wikitext myself. As a result, I am now considering parsing the html of a wikipedia article. For that purpose, I tried to use the MediaWiki API (<a href="https://www.mediawiki.org/wiki/API:Parsing_wikitext" rel="nofollow">https://www.mediawiki.org/wiki/API:Parsing_wikitext</a>) in order to retrieve the html for an article and parse it myself. However, the API is very slow for previous revisions of an article and it will take me forever. </p>  <p>Thus, my question has two parts: 1. What is the fastest way  to get the html of an article for specific revision or what is the best tool to setup local copy of wikipedia (currently experimenting with xowa and wikitaxi) from a dump. 2. Is somebody aware of a html Wikipedia parser that could provide e.g. the position of link or a classification of the links regarding position in text (section), if a link is in a infobox and if it is in a navibox.</p>