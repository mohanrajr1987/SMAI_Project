<p>I've the following piece of code</p>  <pre><code>for i=1,A do     for j=1,B do         self.gradInput:narrow(1, self.C[i][j], 1):add(gradOutput[2][i][j])     end end </code></pre>  <p>in a custom module in torch.</p>  <p>For values of A = 18K, B=30, this loop is taking ~4 s (measured using torch.Timer) which seems extremely high to me.</p>  <p>For context, C is a FloatTensor of size (A, B) and gradOutput of (2, A, B). The processor is AMD Opteron 4386 having 16 cores each with cache size 2MB.</p>  <p>So, it seems to me that C, gradOutput, gradInput should already be cached so 4s for such a short loop seems insanely high to me.</p>  <p>Anyone have any ideas why it's so slow and if/how I can improve the performance?</p>