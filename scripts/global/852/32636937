<p>I know each GPU has its theoretical limitation about the #threads and threadsPerBlock. But in most of the cases, we would not hit that limitation because of resource limitation. Basically there are three kinds of resources that might be used by a kernel, registers, global memory and shared memory.</p>  <p>Assuming we didn't use any share memory at all, the remaining constraint would be registers and global memory. I knew CUDA would do automatically registers spilling when SM runs out of its own resources. In this way, it seems like the hard limitation for number of launched threads would be the capacity of the global memory. (Despite the potential bad performance here)</p>  <p>Is that true? </p>