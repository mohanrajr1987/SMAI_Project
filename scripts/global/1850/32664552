<p>So I have code that grabs a list of files from a directory that initially had over 14 millions files. This is a hex-core machine with 20 GB RAM running Ubuntu 14.04 desktop and just grabbing a list of files takes hours - I haven't actually timed it.</p>  <p>Over the past week or so I've run code that doesn't nothing more than gather this list of files, open each file to determine when it was created, and move it to a directory based on the month and year it was created. (The files have been both scp'd and rsync'd so the timestamp the OS provides is meaningless at this point, hence opening the file.)</p>  <p>When I first started running this loop it was moving 1000 files in about 90 seconds. Then after several hours like this that 90 seconds became 2.5 min, then 4, then 5, then 9, and eventually 15 min. So I shut it down and started over.</p>  <p>I noticed that today once it was done gathering a list of over 9 millions files that moving 1000 files took 15 min right off the bat. I just shut the process down again and rebooted the machine because the time to move 1000 files had climbed to over 90 min.</p>  <p>I had hoped to find some means of doing a <code>while + list.pop()</code> style strategy to free memory as the loop progressed. Then found a couple of SO posts that said it could be done with <code>for i in list: ... list.remove(...)</code> but that this was a terrible idea.</p>  <p>Here's the code:</p>  <pre><code>from basicconfig.startup_config import *  arc_dir = '/var/www/data/visits/'  def step1_move_files_to_archive_dirs(files):   """    :return:   """    cntr = 0   for f in files:       cntr += 1        if php_basic_files.file_exists(f) is False:           continue        try:           visit = json.loads(php_basic_files.file_get_contents(f))       except:           continue        fname = php_basic_files.basename(f)        try:           dt = datetime.fromtimestamp(visit['Entrance Time'])       except KeyError:           continue        mYr = dt.strftime("%B_%Y")        # Move the lead to Monthly archive       arc_path = arc_dir + mYr + '//'       if not os.path.exists(arc_path):           os.makedirs(arc_path, 0777)        if not os.path.exists(arc_path):           print "Directory: {} was not created".format(arc_path)       else:           # Move the file to the archive           newFile = arc_path + fname           #print "File moved to {}".format(newFile)           os.rename(f, newFile)        if cntr % 1000 is 0:           print "{} files moved ({})".format(cntr, datetime.fromtimestamp(time.time()).isoformat())  def step2_combine_visits_into_1_file():   """    :return:   """    file_dirs = php_basic_files.glob(arc_dir + '*')    for fd in file_dirs:     arc_files = php_basic_files.glob(fd + '*.raw')     arc_fname = arc_dir + php_basic_str.str_replace('/', '', php_basic_str.str_replace(arc_dir, '', fd)) + '.arc'      try:       arc_file_data = php_basic_files.file_get_contents(arc_fname)     except:       arc_file_data = {}      for f in arc_files:       uniqID = moduleName = php_adv_str.fetchBefore('.', php_basic_files.basename(f))        if uniqID not in arc_file_data:         visit = json.loads(php_basic_files.file_get_contents(f))         arc_file_data[uniqID] = visit      php_basic_files.file_put_contents(arc_fname, json.dumps(arc_file_data))   def main():   """    :return:   """    files = php_basic_files.glob('/var/www/html/ver1/php/VisitorTracking/data/raw/*')   print "Num of Files: {}".format(len(files))    step1_move_files_to_archive_dirs(files)   step2_combine_visits_into_1_file() </code></pre>  <p>Notes:</p>  <p>basicconfig is essentially a bunch of constants I have for the environment and a few commonly used libraries like all the php_basic_* libraries. (I used PHP for years before picking up Python so I built a library to mimic the more common functions I used in order to be up and running with Python faster.)</p>  <p>The step1 def is as far as the program gets so far. The step2 def could, and likely should, be run in parallel. However, I figured I/O was the bottleneck and doing even more of it in parallel would likely slow all functions down a lot more. (I have been tempted to rsync the archive directories to another machine for aggregation thus getting parallel speed without the I/O bottleneck but figured the rsync would also be quite slow.)</p>  <p>The files themselves are all 3 Kb each so not very large.</p>  <p>----- Final Thoughts -------</p>  <p>Like I said, it doesn't appear, to me at least, that any data is being stored from each file opening. Therefore memory should not be an issue. However, I do notice that only 1.2 GB of RAM is being used right now and over 12 GB of was being used before. A big chunk of that 12 could be storing 14 million file names and paths. I've only just started the processing again so for next several hours python will be gathering a list of files and that list isn't in memory yet.</p>  <p>So I was wondering if there was a garbage collection issue or something else I was missing. Why is it slowing down as it progresses through the loop?</p>