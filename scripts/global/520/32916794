<p>I am running a streaming job and want to build a lookup map incrementally( track unique items, filter duplicated incomings for example), initially I was thinking to keep one Dataframe in cache, and union it with new Dataframe created in each batch, something like this </p>  <p>items.foreachRDD((rdd: RDD[String]) => {       ...<br>         val uf =  rdd.toDF          cached_df = cached_df.unionAll(uf)          cached_df.cache          cached_df.count   // materialize the  ...      }) </p>  <p>My concern is that the cached_df seems remember all the lineages to previous RDDs appended from every batch iteration, in my case, if I don't care to recompute this cached RDD if it crashes, is that an overhead to maintain the growing DAG? </p>  <p>As an alternative, at the beginning of each batch, I load the lookup from parquet file, instead of keeping it in memory, then at the end of each batch I append the new RDD to the same parquet file:  noDuplicatedDF.write.mode(SaveMode.Append).parquet("lookup"). This works as expected, but is there straight forward way that keep the lookup in memory? </p>  <p>Thanks  Wanchun </p>