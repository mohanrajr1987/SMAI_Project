<p>I am working on a big data solution. Usually I start with one chunk of big data, process it, send the results to the next tool to process it even more, and so on. Following is a small example how such a toolchain could look like. Mostly it is not deep (2 or 3 levels) and the fanout at the root is also usually smaller than 3. But fanout in the leaves can be easily 100.</p>  <p>These tools come from a variety of sources and can be closed source. So I have no way of bringing the in- and output of all tools to a common format. Also, the tools will be written in different languages. C, Java, Python, Bash, ...</p>  <p>The final product will be run on a server with quite a lot of RAM. So I would like to go to an in-memory, shared-memory solution.</p>  <p>The root tool is a mediator (Note: This would be my idea to solve my problems, I can be wrong of course and there would be a better approach). The mediator gets as input the toolchain (the selection and order of subprocesses will be chosen by the user), calls the different subprocesses, distributes data, recieves signals from the subprocesses when they are finished and the next one can be run. Subprocesses on the same level should be run in parallel.</p>  <p>So now to my questions: First of all - is this a good design? And second: Which API/functions/programming technique is best suited for calling all these processes so they share the RAM? (So maybe sharing RAM is not always possible. This is not so crucial.)</p>  <pre><code>mediator |-- toolA |    |-- toolA1 |    |    |-- toolA11 |    |    +-- toolA12 |    +-- toolA2 |         +-- toolA21 +-- toolB     |-- toolB1     |-- toolB2     +-- toolB3 </code></pre>