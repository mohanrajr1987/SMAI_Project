<p>When I create models in core data, I'm always a little perplexed by which integer type I should chooseâ€”16, 32, 64.  I'm almost always needing something for a simple, basic number: a count of people in the household, for my present case. Probably going to be a number between 1-20.  Or, I have an incrementing case id number in another instance...can't imagine that going further than  few hundred people.  </p>  <p>And here's the deal...It's clear that true computer science folk think of numbers differently, taking into account factors like the architecture that's going to be processing the numbers, the space required to process and store the data, backwards compatibility, future proofing, etc.  When I think of numbers, I basically think of how large a value is being represented.  So when I get to that point of my process when I have to choose between three types of integers, I basically say to myself, "Well, this is going to be a small number, let's just use the Int 16 option...", or "Shoot, I could end up with a really big number here so let's use the Int 64 choice."  Basically, I pick these data types with the same sort of logic I use when ordering fries...if I'm really hungry I go for the large, if I'm feeling a big guilty I'll just get the small.  </p>  <p>I'm learning enough to know that I'm not thinking about this in the right terms, but I don't really know why, and I don't know the appropriate way to choose the best option.  What factors should I really be considering...what's the most important criteria for selecting between Int 16, Int 32, and Int 64?</p>