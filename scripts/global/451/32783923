<p>I have read in different post on stackoverflow and in the C# documentation, that converting <code>long</code> (or any other data type representing a number) to <code>double</code> loses precision. This is quite obvious due to the representation of floating point numbers.</p>  <p>My question is, how big is the loss of precision if I convert a larger number to <code>double</code>? Do I have to expect differences larger than +/- X ?</p>  <p>The reason I would like to know this, is that I have to deal with a continuous counter which is a <code>long</code>. This value is read by my application as <code>string</code>, needs to be cast and has to be divided by e.g. 10 or some other small number and is then processed further.</p>  <p>Would <code>decimal</code> be more appropriate for this task? </p>