<p>I've got an accelerometer hooked up at an FPGA that outputs data as integers between 0 and 65535. It is connected via USB to a QT program.</p>  <p>The calibration is expressed in terms of 15mg/LSB. I assume this means 15 milli-g per least significant bit -> i.e a change output by 1 means 0.015g of acceleration.</p>  <p>This premise works fine when the output is 3, 4 , 5 , 20 - I can easily get the difference between the current and previous reading and use this to find the acceleration.</p>  <p>I'm having difficulty when the output is close to zero, once it goes below zero it starts from the top i,e 65535 so ill have a series of data like 5 ,6 ,7 ,0 ,65533 , 65530 etc</p>  <p>From here I can't find the difference and use this as the LSB change as the difference between 2 and 65535 is 65533, whereas I would want a change from 2 to 65535 to represent 3.</p>  <p>What I'm searching for is a piece of logic that can interpret the change in LSB from this series that goes from 0 to 65535, but in a  loop fashion, so once it dips below zero it is actually 65535, 65534, 65533 etc</p>  <p>Here is some code:</p>  <pre><code>double x_diff = MotionData[0]-MotionData[3]; double y_diff = MotionData[1]-MotionData[4]; double z_diff = MotionData[2]-MotionData[5]; </code></pre>  <p>The first 3 elements of <code>MotionData</code> are the current x,y,z, readings from the accelerometer, the last 3 and the next x,y,z readings.</p>  <p>Any ideas?</p>  <p>Thanks!</p>