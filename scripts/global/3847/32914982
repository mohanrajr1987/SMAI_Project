<p>I am combining two different classes from two different git projects to create an RTSP streamer for an iOS live streaming application.</p>  <p><em>edit: Agree with the -1 this question is probably a shot in the dark. But, to answer the question if I am asked "why I am not using entirely the DFURTSPPlayer library?" Because I would rather use the YUV display with opengl of the second project, hackcam, rather than decode the video frames into a UIImages like DFURTS does. Hackcam does not have audio</em></p>  <p><em>Also please comment if you down vote, at least help me find an answer by telling me what I need to refine to be clear or point out if this question is inappropriate</em></p>  <p><strong>My current issue is that the audio playback has about a 1 second latency, and is out of sync with the video which is close to real time.</strong></p>  <p>I know that the audio is in sync because I've tested the RTSP streams in VLC. Something is wrong with my implementation. Mostly frankensteining these too projects together and the fact that I am not familiar with ffmpeg c library or AudioQueue for iOS.</p>  <p>Any help would be greatly appreciated!</p>  <p>I've taken the AudioStreamer class from this repository: <a href="https://github.com/durfu/DFURTSPPlayer" rel="nofollow">https://github.com/durfu/DFURTSPPlayer</a></p>  <p><a href="https://github.com/durfu/DFURTSPPlayer/blob/master/DFURTSPPlayer/DFURTSPPlayer/FFMpegDecoder/AudioStreamer.m" rel="nofollow">https://github.com/durfu/DFURTSPPlayer/blob/master/DFURTSPPlayer/DFURTSPPlayer/FFMpegDecoder/AudioStreamer.m</a></p>  <p>And I am trying to get it to work with this one: <a href="https://github.com/hackacam/ios_rtsp_player/blob/master/src/FfmpegWrapper.m" rel="nofollow">https://github.com/hackacam/ios_rtsp_player/blob/master/src/FfmpegWrapper.m</a></p>  <p>I can post more code if needed, but my main loop in FfmpegWrapper now looks like this (_audioController is reference to AudioStreamer.m):</p>  <pre><code>-(int) startDecodingWithCallbackBlock: (void (^) (AVFrameData *frame)) frameCallbackBlock                       waitForConsumer: (BOOL) wait                    completionCallback: (void (^)()) completion {     OSMemoryBarrier();     _stopDecode=false;     dispatch_queue_t decodeQueue = dispatch_queue_create("decodeQueue", NULL);     dispatch_async(decodeQueue, ^{         int frameFinished;         OSMemoryBarrier();         while (self-&gt;_stopDecode==false){             @autoreleasepool {                 CFTimeInterval currentTime = CACurrentMediaTime();                 if ((currentTime-_previousDecodedFrameTime) &gt; MIN_FRAME_INTERVAL &amp;&amp;                     av_read_frame(_formatCtx, &amp;_packetFFmpeg)&gt;=0) {                      _previousDecodedFrameTime = currentTime;                     // Is this a packet from the video stream?                     if(_packetFFmpeg.stream_index==_videoStream) {                         // Decode video frame                         avcodec_decode_video2(_codecCtx, _frame, &amp;frameFinished,                                               &amp;_packetFFmpeg);                          // Did we get a video frame?                         if(frameFinished) {                             // create a frame object and call the block;                             AVFrameData *frameData = [self createFrameData:_frame trimPadding:YES];                             frameCallbackBlock(frameData);                         }                          // Free the packet that was allocated by av_read_frame                         av_free_packet(&amp;_packetFFmpeg);                      } else if (_packetFFmpeg.stream_index==audioStream) {                          // NSLog(@"audio stream");                         [audioPacketQueueLock lock];                          audioPacketQueueSize += _packetFFmpeg.size;                         [audioPacketQueue addObject:[NSMutableData dataWithBytes:&amp;_packetFFmpeg length:sizeof(_packetFFmpeg)]];                          [audioPacketQueueLock unlock];                          if (!primed) {                             primed=YES;                             [_audioController _startAudio];                         }                          if (emptyAudioBuffer) {                             [_audioController enqueueBuffer:emptyAudioBuffer];                         }                          //av_free_packet(&amp;_packetFFmpeg);                      } else {                          // Free the packet that was allocated by av_read_frame                         av_free_packet(&amp;_packetFFmpeg);                     }                   } else{                     usleep(1000);                 }             }         }         completion();     });     return 0; } </code></pre>  <p>Enqueue Buffer in AudioStreamer:</p>  <pre><code>- (OSStatus)enqueueBuffer:(AudioQueueBufferRef)buffer {     OSStatus status = noErr;      if (buffer) {         AudioTimeStamp bufferStartTime;         buffer-&gt;mAudioDataByteSize = 0;         buffer-&gt;mPacketDescriptionCount = 0;          if (_streamer.audioPacketQueue.count &lt;= 0) {             _streamer.emptyAudioBuffer = buffer;             return status;         }          _streamer.emptyAudioBuffer = nil;          while (_streamer.audioPacketQueue.count &amp;&amp; buffer-&gt;mPacketDescriptionCount &lt; buffer-&gt;mPacketDescriptionCapacity) {             AVPacket *packet = [_streamer readPacket];              if (buffer-&gt;mAudioDataBytesCapacity - buffer-&gt;mAudioDataByteSize &gt;= packet-&gt;size) {                 if (buffer-&gt;mPacketDescriptionCount == 0) {                     bufferStartTime.mSampleTime = packet-&gt;dts * _audioCodecContext-&gt;frame_size;                     bufferStartTime.mFlags = kAudioTimeStampSampleTimeValid;                 }                  memcpy((uint8_t *)buffer-&gt;mAudioData + buffer-&gt;mAudioDataByteSize, packet-&gt;data, packet-&gt;size);                 buffer-&gt;mPacketDescriptions[buffer-&gt;mPacketDescriptionCount].mStartOffset = buffer-&gt;mAudioDataByteSize;                 buffer-&gt;mPacketDescriptions[buffer-&gt;mPacketDescriptionCount].mDataByteSize = packet-&gt;size;                 buffer-&gt;mPacketDescriptions[buffer-&gt;mPacketDescriptionCount].mVariableFramesInPacket = _audioCodecContext-&gt;frame_size;                  buffer-&gt;mAudioDataByteSize += packet-&gt;size;                 buffer-&gt;mPacketDescriptionCount++;                   _streamer.audioPacketQueueSize -= packet-&gt;size;                  av_free_packet(packet);             }             else {                  //av_free_packet(packet);                 break;             }         }          [decodeLock_ lock];         if (buffer-&gt;mPacketDescriptionCount &gt; 0) {             status = AudioQueueEnqueueBuffer(audioQueue_, buffer, 0, NULL);             if (status != noErr) {                 NSLog(@"Could not enqueue buffer.");             }         } else {             AudioQueueStop(audioQueue_, NO);             finished_ = YES;         }          [decodeLock_ unlock];     }      return status; } </code></pre>  <p>Read packet in ffmpegwrapper:</p>  <pre><code>- (AVPacket*)readPacket {     if (_currentPacket.size &gt; 0 || _inBuffer) return &amp;_currentPacket;      NSMutableData *packetData = [audioPacketQueue objectAtIndex:0];     _packet = [packetData mutableBytes];      if (_packet) {         if (_packet-&gt;dts != AV_NOPTS_VALUE) {             _packet-&gt;dts += av_rescale_q(0, AV_TIME_BASE_Q, _audioStream-&gt;time_base);         }          if (_packet-&gt;pts != AV_NOPTS_VALUE) {             _packet-&gt;pts += av_rescale_q(0, AV_TIME_BASE_Q, _audioStream-&gt;time_base);         }          [audioPacketQueueLock lock];         audioPacketQueueSize -= _packet-&gt;size;         if ([audioPacketQueue count] &gt; 0) {             [audioPacketQueue removeObjectAtIndex:0];         }         [audioPacketQueueLock unlock];          _currentPacket = *(_packet);     }      return &amp;_currentPacket; } </code></pre>