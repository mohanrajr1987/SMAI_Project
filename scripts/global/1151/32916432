<p>I have a tricky problem trying to find an efficient way of ordering a set of objects (~1000 rows) that contain a large (~5 million) number of indexed data points. In my case I need a query that allows me to order the table by a specific datapoint. Each datapoint is a 16-bit unsigned integer.</p>  <p>I am currently solving this problem by using an large array:</p>  <p><strong>Object Table:</strong></p>  <pre><code>  id serial NOT NULL,   category_id integer,   description text,   name character varying(255),   created_at timestamp without time zone NOT NULL,   updated_at timestamp without time zone NOT NULL,   data integer[], </code></pre>  <p><strong>GIST index:</strong></p>  <pre><code>  CREATE INDEX object_rdtree_idx   ON object   USING gist   (data gist__intbig_ops)   </code></pre>  <p><em>This index is not currently being used when I do a select query, and I am not certain it would help anyway.</em></p>  <p>Each day the array field is updated with a new set of ~5 million values</p>  <p>I have a webserver that needs to list all objects ordered by the value of a particular data point:</p>  <p><strong>Example Query:</strong></p>  <pre><code>SELECT name, data[3916863] as weight FROM object ORDER BY weight DESC </code></pre>  <p>Currently, it takes about 2.5 Seconds to perform this query.</p>  <p><strong>Question:</strong> Is there a better approach? I am happy for the insertion side to be slow as it happens in the background, but I need the select query to be as fast as possible. In saying this, there is a limit to how long the insertion can take.</p>  <p>I have considered creating a lookup table where every value has it's own row - but I'm not sure how the insertion/lookup time would be affected by this approach and I suspect entering 1000+ records with ~5 million data points as individual rows would be too slow.</p>  <p>Currently inserting a row takes ~30 seconds which is acceptable for now.</p>  <p>Ultimately I am still on the hunt for a scalable solution to the base problem, but for now I need this solution to work, so this solution doesn't need to scale up any further.</p>  <p><strong>Update:</strong> I was wrong to dismiss having a giant table instead of an array, while insertion time massively increased, query time is reduced to just a few milliseconds.</p>  <p>I am now altering my generation algorithm to only save a datum if it non-zero and changed from previous update. This has reduced insertions to just a few hundred thousands values which only takes a few seconds.</p>  <p><strong>New Table:</strong></p>  <pre><code>CREATE TABLE data (   object_id integer,   data_index integer,   value integer, )  CREATE INDEX index_data_on_data_index   ON data   USING btree   ("data_index"); </code></pre>  <p><strong>New Query:</strong></p>  <pre><code>SELECT name, coalesce(value,0) as weight FROM objects LEFT OUTER JOIN data on data.object_id = objects.id AND data_index = 7731363 ORDER BY weight DESC </code></pre>  <p>Insertion Time: 15,000 records/second</p>  <p>Query Time: 17ms</p>