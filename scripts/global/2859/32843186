<p>I'm trying to use custom log4j appender inside spark executor, in order to forward all logs to Apache Kafka.</p>  <p>The problem is, log4j is initialized before fatjar's classloader with appender gets registered, so I end up with following:</p>  <pre><code>log4j:ERROR Could not instantiate class [kafka.producer.KafkaLog4jAppender]. java.lang.ClassNotFoundException: kafka.producer.KafkaLog4jAppender     at java.net.URLClassLoader$1.run(URLClassLoader.java:372)     at java.net.URLClassLoader$1.run(URLClassLoader.java:361)     at java.security.AccessController.doPrivileged(Native Method)     at java.net.URLClassLoader.findClass(URLClassLoader.java:360)     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:260)     at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)     at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)     at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)     at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)     at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)     at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)     at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)     at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)     at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)     at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:127)     at org.apache.spark.Logging$class.initializeLogging(Logging.scala:122)     at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:107)     at org.apache.spark.Logging$class.log(Logging.scala:51)     at org.apache.spark.executor.CoarseGrainedExecutorBackend$.log(CoarseGrainedExecutorBackend.scala:126)     at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:137)     at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:235)     at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala) log4j:ERROR Could not instantiate appender named "KAFKA". 2015-09-29 13:10:43 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger started 2015-09-29 13:10:43 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO Remoting: Starting remoting 2015-09-29 13:10:43 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@gin3.dev:36918] 2015-09-29 13:10:43 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO Remoting: Remoting now listens on addresses: [akka.tcp://driverPropsFetcher@gin3.dev:36918] 2015-09-29 13:10:44 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon. 2015-09-29 13:10:44 [driverPropsFetcher-akka.actor.default-dispatcher-4] INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports. 2015-09-29 13:10:44 [sparkExecutor-akka.actor.default-dispatcher-3] INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger started 2015-09-29 13:10:44 [sparkExecutor-akka.actor.default-dispatcher-2] INFO Remoting: Starting remoting 2015-09-29 13:10:44 [sparkExecutor-akka.actor.default-dispatcher-2] INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@gin3.dev:40067] 2015-09-29 13:10:44 [sparkExecutor-akka.actor.default-dispatcher-2] INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@gin3.dev:40067] 2015-09-29 13:10:44 [driverPropsFetcher-akka.actor.default-dispatcher-5] INFO Remoting: Remoting shut down .... </code></pre>  <p>The problem seems to be right here: <a href="https://github.com/apache/spark/blob/v1.3.1/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L126" rel="nofollow">https://github.com/apache/spark/blob/v1.3.1/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L126</a></p>  <p>Is there any easy way to solve this? We are currently using Spark 1.3.x.</p>  <p>Thanks</p>  <p>David</p>