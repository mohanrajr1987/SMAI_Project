<p>I have a scraper which collects information perfectly, but when I try to implement rules to crawl the "next" page I get stuck. Using Scrapy 0.22 (I can't upgrade at this time).</p>  <pre><code>import re import datetime import dateutil  import urllib2  from scrapy.http import Request from scrapy.selector import Selector from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule from crawlers.spiders import BaseCrawler   class rappSpider(BaseCrawler):     name = "rapp"      base_url = "www.example.com"     start_urls = [         # "http://www.example.com/news-perspective",         # "http://www.example.com/news-perspective?f[0]=field_related_topics%3A31366",         "http://www.example/news-perspective?key=&amp;page=%d"      ]     # rules = [          # Rule(SgmlLinkExtractor(allow=r'?key=&amp;page=[0-9]'), callback='get_article_links', follow= True)      # ]      TITLE_XPATH_SELECTOR= "//div[@id='inset-content']//h1/text()"      TEXT_XPATH_SELECTOR = "//div[@class='field-item even']/p/text()"      DATETIME_XPATH_SELECTOR = "//div[@class='field-items']/div/span/text()"      def get_article_links(self, response, *args, **kwargs):         html = Selector(response)         link_extractor = SgmlLinkExtractor(allow=('http://www.example.com/news-perspective/\d{4}/\d{2}\/*\S*$',))          is_relative_path = False         yield [link.url for link in link_extractor.extract_links(response)], is_relative_path </code></pre>  <p>The scraper works for start_urls like <a href="http://www.example/news-perspective" rel="nofollow">http://www.example/news-perspective</a> which lists a number of articles on the page, then the scraper will follow the links defined by get_article_links and get the relevant information. However, I'd like to be able to go to the next page (same format on other pages, the url being</p>  <p><a href="http://www.example/news-perspective?key=&amp;page=" rel="nofollow">http://www.example/news-perspective?key=&amp;page=</a><strong>#</strong></p>  <p>How can I set this up with my existing code? Do I need two separate rules ? Or do I need to alter start_requests? </p>