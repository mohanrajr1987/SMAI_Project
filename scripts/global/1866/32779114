<p><strong>The setup</strong><br> We have setup a SolrCloud (Solr version 4.10.4) cluster consisting of 6 servers distributed over 2 datacenters (3 on each DC).</p>  <p>The cluster is setup with 3 shards and a replication factor of 2 and handles one core with 45M documents averaging at about 100GB per shard. There are 3 Zookeeper instances regulating the cluster that reside on 3 of the 6 servers (the ones in the first DC).</p>  <p>The core resides on a  6Gb/s SSD drive on all shards.  The intra-DC ping time is in the region of 0.3ms, while the inter-DC one is in the region of 3 ms.</p>  <p>The cluster is setup on Tomcat 7.0.61 and Java 7 with an allocated memory of 26GB while each server has 32GB available while each node is configured to contact the zookeeper every 30 seconds.</p>  <p>The cache configuration for each solr node is as follows</p>  <pre class="lang-xml prettyprint-override"><code>&lt;filterCache class="solr.FastLRUCache"              size="40000"              initialSize="40000"              autowarmCount="0"/&gt; &lt;queryResultCache class="solr.LRUCache"                  size="50000"                  initialSize="20000"                  autowarmCount="0"/&gt; &lt;documentCache class="solr.LRUCache"                size="2000000"                initialSize="2000000"               /&gt; &lt;fieldValueCache class="solr.FastLRUCache"                size="8"                autowarmCount="8"                showItems="8" /&gt; </code></pre>  <p>On top of that we have an API application that performs certain search operations that most of the times look like:</p>  <pre><code>q=Fragmento+de+retablo+NOT+DATA_PROVIDER%3A%22CER.ES%3A+Red+Digital+de+Colecciones+de+museos+de+Espa%C3%B1a%22&amp; rows=12&amp;start=0&amp; sort=score+desc&amp; timeAllowed=30000&amp;fl=*%2Cscore&amp;facet.mincount=1 </code></pre>  <p>We use one or at most to sort parameters (the second one being the unique id of our schema but not in this example).</p>  <p><strong>The problem</strong><br> Our API sends around 5-10 queries per second on the cluster. Even that minimal number of requests after a while overwhelms the cluster and nodes start disappearing while at the same time a lot of disk I/O is observed. We do some manual cache warming for about 10 minutes before we make the core available to the API and we notice that after a while (and before the crash of the cluster) the hit ratio on the caches is 1 for all but the <code>queryResultCache=0.67</code> and <code>documentCache=0.9</code>, while no evictions happen either. The memory consumption is around 88%.</p>  <p>Any ideas what can be wrong or where we should focus will be highly appreciated.</p>