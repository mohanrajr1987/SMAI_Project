<p>I am trying to fix a 'charmap' can't encode character error. I believe it can be fixed by encoding/decoding the url according to the charset in the source code, setting the default charset to utf-8 if it isn't specified in the pages source code. I have some code started so far. But I don't know where to go from there.  I also want to be able to transform my output into a sql file that I can then upload into MySQL.</p>  <pre><code>import urllib.request from bs4 import BeautifulSoup import re re.IGNORECASE = True from urllib.parse import urlparse  #SourceUrl url = 'http://www.imdb.com'  urls = [url]  z = urlparse(urls[0])  TopLevel = z.scheme+'://'+z.netloc  visited =[url]  robotsUrl = TopLevel +'/robots.txt'  while len(urls) &lt; 1000:         try:             htmltext = urllib.request.urlopen(urls[0]).read()             robots = urllib.request.urlopen(robotsUrl).read()             disallowList = re.findall(b'Disallow\:\s*([a-zA-Z0-9\*\-\/\_\?\.\%\:\&amp;]+)', robots)         except:             print (urls[0])         sourceCode = BeautifulSoup(htmltext, "html.parser")         urls.pop(0)         print (len(urls))         print ('urls')         for link in sourceCode.findAll('a', href=True):             if "http://" not in link['href']:                 link['href'] = urllib.parse.urljoin(TopLevel,link['href'])             in_disallow = False             for i in range(len(disallowList)):                 if (disallowList[i]).upper().decode() in link['href'].upper():                     in_disallow = True                     break             if not in_disallow:                 if link['href'] not in visited:                     urls.append(link['href'])                     visited.append(link['href'])  for i in range(len(urls)):     urls[i] = urls.decode('utf-8')  print ('these are the urls crawled') print (urls) </code></pre>