<p>I have flat files...which has pipe separated values... Also we have corresponding tables for each of these files.</p>  <p>Table/file dont have any unique/primary column.</p>  <p>To compare the data, we converting file to data table (c#,.net)</p>  <p>Take first row in DB_Datatable...  Compare it against each row in file_DataTable</p>  <p>If we could not find match in file_dataTable ...we will consider as different.</p>  <p>In this case, if both tables match...and if each tables have 10 records...there would be 100 row comparisons.</p>  <p>This logic works, but for files > 20K, it is taking huge time.</p>  <p>Could someone suggest me a better approach?</p>  <p>Note : Can we make a hash for each row(using content of each row)...and sort based on it?</p>