<p>I have to develop a code which receives single precision inputs and should return single precision outputs (all netcdf files).</p>  <p>My idea was to use for internal operations single precision variables, however I've been told that I should be using double precision instead.</p>  <p>I would have to read the single precision inputs from netcdf as double, process them and, then, report them as single again.</p>  <p>The argument was that now with 64-bits architectures, the output resulting from double precision variables has higher accuracy than single precision operations.</p>  <p>Is that right? It's not clear to me. A better explanation is very welcome.</p>