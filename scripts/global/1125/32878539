<p>I'm using the <code>math.geom2d</code> package to calculate union of a set of polygons using Spark Java API. </p>  <p>So now I have a <code>JavaRDD&lt;SimplePolygon2D&gt;</code> object that has all the existing polygons.</p>  <p>With this, I have the following reduce function to find a union polygon</p>  <pre><code>SimplePolygon2D union = polygons.reduce(new ReducePolygon()); </code></pre>  <p>The class that does this is</p>  <pre><code>class ReducePolygon implements Function2&lt;SimplePolygon2D, SimplePolygon2D, SimplePolygon2D&gt;, Serializable {      private static final long serialVersionUID = 3967706011201472920L;      public SimplePolygon2D call(SimplePolygon2D v1, SimplePolygon2D v2) throws Exception {         SimplePolygon2D polygon = (SimplePolygon2D) Polygons2D.union(v1, v2);          return polygon;     } } </code></pre>  <p>What I have noted is that it is working fine and at each call to the <code>call(SimplePolygon2D v1, SimplePolygon2D v2)</code> method, it is creating a proper polygon based on the union. But after this, it throws an exception. I have tried all different combinations but I keep getting this exception at the end of this execution.</p>  <p>This is the stack trace:</p>  <pre><code>15/09/30 19:23:13 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0) java.io.NotSerializableException: math.geom2d.polygon.SimplePolygon2D     at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)     at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)     at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)     at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)     at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)     at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)     at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)     at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     at java.lang.Thread.run(Thread.java:745) </code></pre>  <p>Please let me know how to solve this.</p>