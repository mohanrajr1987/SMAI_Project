<p>I have a csv file that is about 500MB. I am trying to read it in.</p>  <pre><code> df = pd.read_csv(My_CSV_File.csv, low_memory=False) </code></pre>  <p>I get this error: </p>  <pre><code> CParserError: Error tokenizing data. C error: out of memory </code></pre>  <p>I don't think this csv file is considered anything close to large for pandas's read_csv to handle. I've tried some other options in read_csv such as setting but none worked. What can I do in this case?</p>