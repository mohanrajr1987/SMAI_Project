<p>I'm trying to install Spark on a Linux box.  I downloaded it from <a href="http://spark.apache.org/docs/latest/building-spark.html" rel="nofollow">http://spark.apache.org/docs/latest/building-spark.html</a> and am trying to build it with this command:</p>  <pre><code>root# build/mvn -e -X -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package </code></pre>  <p>The build seems to start fine:</p>  <pre><code>Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T07:57:37-04:00) Maven home: /some_path_here/spark-1.5.0/build/apache-maven-3.3.3 Java version: 1.7.0_05, vendor: Oracle Corporation Java home: /usr/local/mytools-tools/java/jdk64/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "2.6.32-573.1.1.el6.x86_64", arch: "amd64", family: "unix" [DEBUG] Created new class realm maven.api [DEBUG] Importing foreign packages into class realm maven.api </code></pre>  <p>But then it fails:</p>  <pre><code>[debug] Recompiling all 8 sources: invalidated sources (8) exceeded 50.0% of all sources [info] Compiling 8 Java sources to /some_path_here/spark-1.5.0/launcher/target/scala-2.10/classes... [debug] Attempting to call javac directly... [debug] com.sun.tools.javac.Main not found with appropriate method signature; forking javac instead [debug] Forking javac: javac @/tmp/sbt_6c9436e4/argfile [error] Cannot run program "javac": error=20, Not a directory [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO]  [INFO] Spark Project Parent POM ........................... SUCCESS [  2.056 s] [INFO] Spark Project Launcher ............................. FAILURE [  4.832 s] </code></pre>  <p>and so on.</p>  <p>I'm pretty sure I have <code>JAVA_HOME</code> and <code>PATH</code> defined appropriately.</p>  <p>This box has multiple versions of Java installed, which might be related to the problem.</p>