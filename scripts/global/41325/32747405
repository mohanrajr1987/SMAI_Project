<p>I'm following the documentation example <a href="https://spark.apache.org/docs/latest/ml-guide.html#example-estimator-transformer-and-param" rel="nofollow">Example: Estimator, Transformer, and Param</a></p>  <p>And I got error msg</p>  <blockquote>   <p>15/09/23 11:46:51 INFO BlockManagerMaster: Registered BlockManager   Exception in thread "main" java.lang.NoSuchMethodError:   scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaUniverse$JavaMirror;     at SimpleApp$.main(hw.scala:75)</p> </blockquote>  <p>And line 75 is the code "sqlContext.createDataFrame()":</p>  <pre><code>import java.util.Random  import org.apache.log4j.Logger import org.apache.log4j.Level  import scala.io.Source  import org.apache.spark.SparkConf import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.rdd._   import org.apache.spark.ml.classification.LogisticRegression import org.apache.spark.ml.param.ParamMap import org.apache.spark.mllib.linalg.{Vector, Vectors} import org.apache.spark.mllib.recommendation.{ALS, Rating, MatrixFactorizationModel} import org.apache.spark.sql.Row import org.apache.spark.sql.SQLContext import org.apache.spark.sql.DataFrame import org.apache.spark.sql.functions._  object SimpleApp {      def main(args: Array[String]) {        val conf = new SparkConf().setAppName("Simple Application").setMaster("local[4]");        val sc = new SparkContext(conf)        val sqlContext = new SQLContext(sc)        val training = sqlContext.createDataFrame(Seq(          (1.0, Vectors.dense(0.0, 1.1, 0.1)),          (0.0, Vectors.dense(2.0, 1.0, -1.0)),          (0.0, Vectors.dense(2.0, 1.3, 1.0)),          (1.0, Vectors.dense(0.0, 1.2, -0.5))        )).toDF("label", "features")     } } </code></pre>  <p>And my sbt is like below:</p>  <pre><code>lazy val root = (project in file(".")).   settings(     name := "hello",     version := "1.0",     scalaVersion := "2.11.4"   )  libraryDependencies ++= {     Seq(         "org.apache.spark" %% "spark-core" % "1.4.1" % "provided",         "org.apache.spark" %% "spark-sql" % "1.4.1" % "provided",         "org.apache.spark" % "spark-hive_2.11" % "1.4.1",         "org.apache.spark"  % "spark-mllib_2.11" % "1.4.1" % "provided",         "org.apache.spark" %% "spark-streaming" % "1.4.1" % "provided",         "org.apache.spark" %% "spark-streaming-kinesis-asl" % "1.4.1" % "provided"     ) } </code></pre>  <p>I tried to search around and found <a href="http://stackoverflow.com/questions/29543400/nosuchmethoderror-org-apache-spark-sql-sqlcontext-applyschema">this post</a> which is very similar to my issue, and I tried to change my sbt setting for spark versions (spark-mllib_2.11 to 2.10, and spark-1.4.1 to 1.5.0), but it came even more dependency conflicts. </p>  <p>My intuition is it's some version problem but cannot figure it out myself, could anyone please help? thanks a lot.</p>