<p>I have a 4 node Slurm cluster, each with 6 cores. I would like to submit a test Python script (it spawns processes that print the hostname of the node it's being run on) utilizing Multiprocessing as follows:</p>  <pre><code>def print_something():   print gethostname()  # number of processes allowed to run on the cluster at a given time n_procs = int(environ['SLURM_JOB_CPUS_PER_NODE']) * int(environ['SLURM_JOB_NUM_NODES']) # tell Python how many processes can run at a time pool = Pool(n_procs) # spawn an arbitrary number of processes for i in range(200):     pool.apply_async(print_something) pool.close() pool.join() </code></pre>  <p>I submit this with an SBATCH script, which specifies nodes=4 and ntasks-per-node=6, but I am finding that the Python script gets executed 4*6 times. I just want the job to execute the script once, and allow Slurm to distribute the process spawns across the cluster.</p>  <p>I'm obviously not understanding something here...?</p>