<p>I have a file with regular numeric output (same format) of many arrays, each separated by a single line (containing some info). For example: </p>  <pre><code>library(gdata) nx = 150 # ncol of my arrays ny = 130 # nrow of my arrays myfile = 'bigFileWithRowsToSkip.txt' niter = 10 for (i in 1:niter) {   write(paste('This is iteration', i), myfile, append=T)   z = matrix(runif(nx*ny), nrow = ny) # random numbers with dim(nx, ny)   write.fwf(z, myfile, append=T, rownames=F, colnames=F) #write in fixed width format } </code></pre>  <p>With <code>nx=5</code> and <code>ny=2</code>, I would have a file like this:</p>  <blockquote>   <p>This is iteration 1</p>      <p>0.08051668 0.19546772 0.908230985 0.9920930408 0.386990316</p>      <p>0.57449532 0.21774728 0.273851698 0.8199024885 0.441359571</p>      <p>This is iteration 2</p>      <p>0.655215475 0.41899060 0.84615044 0.03001664 0.47584591</p>      <p>0.131544592 0.93211342 0.68300161 0.70991368 0.18837031</p>      <p>This is iteration 3</p>      <p>... </p> </blockquote>  <p>I want to read the successive arrays as fast as possible to put them in a single <code>data.frame</code> (in reality, I have thousands of them). What is the most efficient way to proceed?</p>  <p>Given the output is regular, I thought <code>readr</code> would be a good idea (?).  The only way I can think of, is to do it manually by chunks in order to eliminate the useless info lines:</p>  <pre><code>library(readr) ztot = numeric(niter*nx*ny) # allocate a vector with final size  # (the arrays will be vectorized and successively appended to each other) for (i in 1:niter) {   nskip = (i-1)*(ny+1) + 1 # number of lines to skip, including the info lines   z = read_table(myfile, skip = nskip, n_max = ny, col_names=F)   z = as.vector(t(z))   ifirst = (i-1)*ny*nx + 1 # appropriate index   ztot[ifirst:(ifirst+nx*ny-1)] = z }  # The arrays are actually spatial rasters. Compute the coordinates  # and put everything in DF for future analysis: x = rep(rep(seq(1:nx), ny), niter)  y = rep(rep(seq(1:ny), each=nx), niter)   myDF = data.frame(x=x, y=y, z=z)  </code></pre>  <p>But this is not fast enough. How can I achieve this faster?</p>  <p>Is there a way to read everything at once and delete the useless rows afterwards?</p>  <p>Alternatively, is there no reading function accepting a vector with precise locations as <code>skip</code> argument, rather than a single number of initial rows?</p>  <p><em>PS: note the reading operation is to be repeated on many files (same structure) located in different directories, in case it influences the solution...</em></p>