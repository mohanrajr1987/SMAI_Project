<p>I have like loads of data of customer verbatim for a major transportation services and I would like to analyze the frequently used phrases/detect text patterns to understand exactly what the customer is saying from the text.There is no specific list where I can use gsub to get those specific rather would want to Discover the patterns by doing something to the text. Few of the examples are below: </p>  <p>Output expected is shown with a hyphen next to the comment or approximately close to it would help</p>  <p>1) Didnt Unblock My Account  - Unblock Account 2) since the driver who had picked me up, upon figuring out that he was the wrong driver, forced me to use a different company's cab. - Wrong driver 3) The Person Who Emailed Me Just Seemed To Send Pretexted Response  - PreTexted Response 4) Keep up the great work!! - Great Work 5) Very good response time - Good Response Time 6) Support personnel did not take the time to understand the seriousness of a driver trying to take a much longer route. - Understand the seriouness 7) I emailed customer service but they responded too late. - Response too late 8) I am sending a letter to your CEO.  It is obvious that providing feedback electronically is insufficient. - Letter to your CEO 9) Did not get a refund or even a partial refund. I should not have paid as much as I did. - Partial refund</p>  <p>How can this be done? Answer may not be limited to few of ideas which did not work for me or am not sure if I have missed doing anything 1) Detecting sequential words 2) By finding the POS of the words and collect only those terms which are Verb, Adjective,Noun (assuming these carry the meaning of the sentence). I am total stuck now. I could proceed only till pre-processing of the data. This is what I have done so far. </p>  <pre><code>CSVfile = read.csv("Testfortextcsv.csv",stringsAsFactors = FALSE) TestSplit = as.data.frame(sent_detect_nlp(CSVfile$Comment)) colnames(TestSplit)[colnames(TestSplit)=="sent_detect_nlp(CSVfile$Comment)"]&lt;- "Comment" TestCorpus = Corpus(VectorSource(TestSplit$Comment)) TestCorpus = tm_map(TestCorpus, tolower) TestCorpus = tm_map(TestCorpus, PlainTextDocument) TestCorpus = tm_map(TestCorpus, removePunctuation) TestCorpus = tm_map(TestCorpus, removeWords,c("Test",stopwords("SMART"),stopwords("english"))) TestCorpus = tm_map(TestCorpus, stripWhitespace) TestCorpus = tm_map(TestCorpus, stemDocument) dtm &lt;- TermDocumentMatrix(TestCorpus) m &lt;- as.matrix(dtm) v &lt;- sort(rowSums(m),decreasing=TRUE) d &lt;- data.frame(word = names(v),freq=v) head(d, 10) findFreqTerms(dtm, lowfreq = 15) </code></pre>  <p>I am not sure how to proceed further. Please help. Please let me know if my expectation is not clear.</p>  <p>I tried to replicate this <a href="http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package">article</a> but for someone reason nothing happened.</p>