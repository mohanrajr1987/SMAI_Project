<p>I have a 4 node  hadoop distributed cluster (including hbase) set up like this.</p>  <ul> <li><strong>node1</strong>- namenode + hbase master + zookeeper</li> <li><strong>node2</strong>- resourcemanager </li> <li><strong>node3</strong>- datanode1+hbase regionserver1+nodemanager</li> <li><strong>node4</strong>- datenode2+hbase regionserver2+nodemanager</li> </ul>  <p>Cluster set up seems to be fine , as all the WEB UIs (hbase, namenode, resource manager ) are coming up . Now when I am trying to submit a mapreduce job which reads/writes hbase tables , it gets hanged. It keeps getting timeedout <strong>However same job is working fine , in case I explicitly mention hbase credentials in my mapreduce code and set them in job</strong></p>  <pre><code>Configuration conf =  HBaseConfiguration.create(); conf.set("hbase.zookeeper.quorum", "10.211.55.101"); conf.set("hbase.zookeeper.property.clientPort","2181"); conf.set("hbase.master", "10.211.55.101:60000"); //10.211.55.101 is the ipaddress of node1 </code></pre>  <p>These properties are already set in hbase configuration on node1 , node3 and node4.  Now my question is <strong>Do I need to set up anything with regards to hbase configs on node2 where only resource manager is running ? Why the same job is working fine when hbase configs are set in code explicity</strong></p>