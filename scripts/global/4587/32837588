<p>I'm using Spark for fun and to learn new things about MapReduce. So, I'm trying to write a program suggesting new friendships (i.e., a sort of recommendation system). The suggestion of a friendship between two individuals is performed if they are not connected yet and have a lot of friends in common.</p>  <p>The friendship text file has a structure similar to the following:</p>  <pre><code>1   2,4,11,12,15 2   1,3,4,5,9,10 3   2,5,11,15,20,21 4   1,2,3 5   2,3,4,15,16 ... </code></pre>  <p>where the syntax is: <code>ID_SRC1&lt;TAB&gt;ID_DST1,ID_DST2,...</code>.</p>  <p>The program should output (print or text file) something like the following:</p>  <pre><code>1   3,5 3   1 5   1 ... </code></pre>  <p>where the syntax is: <code>ID_SRC1&lt;TAB&gt;ID_SUGG1,ID_SUGG2,...</code>. Of course the program must suggest a friendship if the two individuals shares a minimum number of friends, let's say <code>3</code> in our case.</p>  <p>I've written my program, but I'd like to read better and more powerful solutions by you. Indeed, I think my code can improved a lot since it takes much time to output from an input file of 4.2 MB.</p>  <p>Below my code:</p>  <pre><code>from pyspark import SparkContext, SparkConf  def linesToDataset(line):     (src, dst_line) = line.split('\t')     src = int(src.strip())      dst_list_string = dst_line.split(',')     dst_list = [int(x.strip()) for x in dst_list_string if x != '']      return (src, dst_list)    def filterPairs(x):      # don't take into account pairs of a same node and pairs of already friends     if (x[0][0] != x[1][0]) and (not x[0][0] in x[1][1]) and (not x[1][0] in x[0][1]):         shared = len(list(set(x[0][1]).intersection(set(x[1][1]))))         return (x[0][0], [x[1][0], shared])  def mapFinalDataset(elem):     recommendations = []     src = elem[0]     dst_commons = elem[1]     for pair in dst_commons:         if pair[1] &gt; 3: # 3 is the minimum number of friends in common             recommendations.append(pair[0])     return (src, recommendations)  def main():     conf = SparkConf().setAppName("Recommendation System").setMaster("local[4]")     sc = SparkContext(conf=conf)     rdd = sc.textFile("data.txt")      dataset = rdd.map(linesToDataset)      cartesian = dataset.cartesian(dataset)     filteredDatasetRaw = cartesian.map(filterPairs)     filteredDataset = filteredDatasetRaw.filter(lambda x: x != None) #   print filteredDataset.take(10)      groupedDataset = filteredDataset.groupByKey().mapValues(list) #   print groupedDataset.take(10)      finalDataset = groupedDataset.map(mapFinalDataset)     output = finalDataset.take(100)     for (k, v) in output:         if len(v) &gt; 0:             print str(k) + ': ' + str(v)      sc.stop()   if __name__ == "__main__":     main() </code></pre>