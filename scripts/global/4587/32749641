<p>I'm running spark on Yarn but my application keeps getting Out of Memory Exception while trying to load a large RDD even though I had dynamic scheduling set to true</p>  <p>...</p>  <pre><code>.set("spark.shuffle.service.enabled", "true") .set("spark.dynamicAllocation.enabled", "true") .set("spark.default.parallelism", String.valueOf(cpu * 3)) </code></pre>  <p>To fix this, I had to specify the executor memory </p>  <p>...</p>  <pre><code>.set("spark.driver.memory", "5g") .set("spark.executor.memory", "30g") .set("spark.shuffle.service.enabled", "true") .set("spark.dynamicAllocation.enabled", "true") .set("spark.default.parallelism", String.valueOf(cpu * 3)) </code></pre>  <p>```</p>  <p>but isn't the whole point of dynamic scheduling to allocate the required resources from Yarn?</p>