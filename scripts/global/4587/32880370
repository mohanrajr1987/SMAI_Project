<p>I am trying to aggregate session data without a true session "key" in PySpark. I have data where an individual is detected in an area at a specific time, and I want to aggregate that into a duration spent in each area <strong>during a specific visit (see below).</strong></p>  <p>The tricky part here is that I want to infer the time someone exits each area as the time they are detected in the next area. This means that I will need to use the start time of the next area ID as the end time for any given area ID. Area IDs can also show up more than once for the same individual.</p>  <p>I had an implementation of this in MapReduce where I iterate over all rows and aggregate the time until a new AreaID or Individual is detected, then output the record. Is there a way to do something similar in Spark? Is there a better way to approach the problem?</p>  <p>Also of note, I do not want to output a record unless the individual has been detected in another area (e.g. IndividualY, AreaT below)</p>  <p>I have a dataset in the following format:</p>  <pre> Individual  AreaID  Datetime of Detection IndividualX AreaQ   1/7/2015 0:00 IndividualX AreaQ   1/7/2015 1:00 IndividualX AreaW   1/7/2015 3:00 IndividualX AreaQ   1/7/2015 4:00 IndividualY AreaZ   2/7/2015 4:00 IndividualY AreaZ   2/7/2015 5:00 IndividualY AreaW   2/7/2015 6:00 IndividualY AreaT   2/7/2015 7:00 </pre>  <p>I would like the desired output of:</p>  <pre> Individual  AreaID  Start_Time      End_Time        Duration (minutes) IndividualX AreaQ   1/7/2015 0:00   1/7/2015 3:00   180 IndividualX AreaW   1/7/2015 3:00   1/7/2015 4:00   60 IndividualY AreaZ   2/7/2015 4:00   2/7/2015 6:00   120 IndividualY AreaW   2/7/2015 6:00   2/7/2015 7:00   60 </pre>