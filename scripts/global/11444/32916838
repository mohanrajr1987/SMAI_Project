<p>I understand you can download Spark source code (1.5.1), or prebuilt binaries for various versions of Hadoop. As of Oct 2015, the Spark webpage <a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a>  has prebuilt binaries against Hadoop 2.6+, 2.4+, 2.3, and 1.X.</p>  <p>I'm not sure what version to download. </p>  <p>I want to run a Spark cluster in standalone mode using AWS machines. </p>  <p><code>&lt;EDIT&gt;</code></p>  <p>I will be running a 24/7 streaming process. My data will be coming from a Kafka stream. I thought about using spark-ec2, but since I already have persistent ec2 machines, I thought I might as well use them. </p>  <p>My understanding is that since my persistent workers need to perform <code>checkpoint()</code>, it needs to have access to some kind of shared file system with the master node. S3 seems like a logical choice.<br> <code>&lt;/EDIT&gt;</code></p>  <p>This means I need to access S3, but not hdfs. I do not have Hadoop installed. </p>  <p>I got a pre-built Spark for Hadoop 2.6. I can run it in local mode, such as the wordcount example. However, whenever I start it up, I get this message</p>  <pre><code>WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable </code></pre>  <p>Is this a problem? Do I need hadoop? </p>  <p><code>&lt;EDIT&gt;</code></p>  <p>It's not a show stopper but I want to make sure I understand the reason of this warning message. I was under the assumption that Spark doesn't need Hadoop, so why is it even showing up? <code>&lt;/EDIT&gt;</code></p>