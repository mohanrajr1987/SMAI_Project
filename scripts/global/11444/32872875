<p>So I'm currently building a project in spark in EMR, and have been having some latency issues with pulling from s3. I found <a href="http://tech.kinja.com/how-not-to-pull-from-s3-using-apache-spark-1704509219" rel="nofollow">this article</a> that highly suggests parallelizing the s3 keys rather than using Spark's "textfile" function. I wrote this code based on the code provided and it seems to work fine... on uncompressed data.</p>  <pre><code>def getDataFromAWSBuckets(bucket:String, prefix:String, sc:SparkContext, setMax:Boolean):RDD[String] = {  val request = new ListObjectsRequest() request.setBucketName(bucket) request.setPrefix(prefix) if(setMax)   request.setMaxKeys(100)  def s3 = new AmazonS3Client()  val objs:ObjectListing = s3.listObjects(request)  val s3FileKeys = objs.getObjectSummaries.asScala.map(_.getKey).toList sc.parallelize(s3FileKeys).flatMap {     key =&gt; Source.fromInputStream(s3.getObject(bucket, key)     .getObjectContent: InputStream).getLines } } </code></pre>  <p>Whenever I use this code on a .bz2 I get a "MalformedInputException" and I'm not sure how I could parse the data differently.</p>  <p>Here is the error output if it helps:</p>  <pre><code>java.nio.charset.MalformedInputException: Input length = 1     at java.nio.charset.CoderResult.throwException(CoderResult.java:277)     at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:338)     at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)     at java.io.InputStreamReader.read(InputStreamReader.java:184)     at java.io.BufferedReader.fill(BufferedReader.java:154)     at java.io.BufferedReader.readLine(BufferedReader.java:317)     at java.io.BufferedReader.readLine(BufferedReader.java:382)     at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.    scala:67)     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)     at org.apache.spark.scheduler.Task.run(Task.scala:64)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.    java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.    java:615)     at java.lang.Thread.run(Thread.java:745) 15/09/30 11:02:50 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 10,     localhost): java.nio.charset.MalformedInputException: Input length = 1     at java.nio.charset.CoderResult.throwException(CoderResult.java:277)     at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:338)     at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)     at java.io.InputStreamReader.read(InputStreamReader.java:184)     at java.io.BufferedReader.fill(BufferedReader.java:154)     at java.io.BufferedReader.readLine(BufferedReader.java:317)     at java.io.BufferedReader.readLine(BufferedReader.java:382)     at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.    scala:67)     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)     at org.apache.spark.scheduler.Task.run(Task.scala:64)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.    java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.    java:615)     at java.lang.Thread.run(Thread.java:745)  15/09/30 11:02:50 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times;     aborting job 15/09/30 11:02:50 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have     all completed, from pool  15/09/30 11:02:50 INFO TaskSchedulerImpl: Cancelling stage 10 15/09/30 11:02:50 INFO DAGScheduler: Stage 10 (collect at DataMillTest.scala:15)     failed in 0.581 s 15/09/30 11:02:50 INFO DAGScheduler: Job 10 failed: collect at DataMillTest.    scala:15, took 0.586724 s  Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent     failure: Lost task 0.0 in stage 10.0 (TID 10, localhost): java.nio.charset.    MalformedInputException: Input length = 1     at java.nio.charset.CoderResult.throwException(CoderResult.java:277)     at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:338)     at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)     at java.io.InputStreamReader.read(InputStreamReader.java:184)     at java.io.BufferedReader.fill(BufferedReader.java:154)     at java.io.BufferedReader.readLine(BufferedReader.java:317)     at java.io.BufferedReader.readLine(BufferedReader.java:382)     at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.    scala:67)     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)     at org.apache.spark.scheduler.Task.run(Task.scala:64)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.    java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.    java:615)     at java.lang.Thread.run(Thread.java:745)  Driver stacktrace: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage     10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10,     localhost): java.nio.charset.MalformedInputException: Input length = 1     at java.nio.charset.CoderResult.throwException(CoderResult.java:277)     at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:338)     at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)     at java.io.InputStreamReader.read(InputStreamReader.java:184)     at java.io.BufferedReader.fill(BufferedReader.java:154)     at java.io.BufferedReader.readLine(BufferedReader.java:317)     at java.io.BufferedReader.readLine(BufferedReader.java:382)     at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.    scala:67)     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)     at scala.collection.Iterator$class.foreach(Iterator.scala:727)     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)     at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)     at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)     at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)     at scala.collection.AbstractIterator.to(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)     at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)     at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)     at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.    scala:1498)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)     at org.apache.spark.scheduler.Task.run(Task.scala:64)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.    java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.    java:615)     at java.lang.Thread.run(Thread.java:745)  Driver stacktrace:     at org.apache.spark.scheduler.DAGScheduler.    org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(    DAGScheduler.scala:1204)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(    DAGScheduler.scala:1193)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(    DAGScheduler.scala:1192)     at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.    scala:59)     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply    (DAGScheduler.scala:693)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply    (DAGScheduler.scala:693)     at scala.Option.foreach(Option.scala:236)     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.    scala:693)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(    DAGScheduler.scala:1393)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(    DAGScheduler.scala:1354)     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)     </code></pre>