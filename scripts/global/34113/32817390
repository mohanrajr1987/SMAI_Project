<p>I have Cassandra installed on single node AWS. I am trying to bulkload 1mn rows(70MB) of CSV file from HDFS to Cassandra via Spark. But i noticed that cassandra only writes 4789 rows. Should i have to change any properties in Spark or Cassandra?</p>  <pre><code>$ hadoop fs -ls par.txt Picked up _JAVA_OPTIONS: -Xms1024m -Xmx1024m -rw-------   3 u**** u****     707531 2015-09-24 10:33 par.txt </code></pre>  <p>Cassandra Table:</p>  <pre><code>CREATE TABLE party1 (   id text,   country text,   email text,   first_name text,   ip_address text,   last_name text,   PRIMARY KEY (id) ) WITH   bloom_filter_fp_chance=0.010000 AND   caching='KEYS_ONLY' AND   comment='' AND   dclocal_read_repair_chance=0.100000 AND   gc_grace_seconds=864000 AND   index_interval=128 AND   read_repair_chance=0.000000 AND   populate_io_cache_on_flush='false' AND   default_time_to_live=0 AND   speculative_retry='99.0PERCENTILE' AND   memtable_flush_period_in_ms=0 AND   compaction={'class': 'SizeTieredCompactionStrategy'} AND   compression={'sstable_compression': 'LZ4Compressor'};  cqlsh:cassdb&gt; select count(*) from party1;   count -------   4789  (1 rows) </code></pre>  <p>Spark Commands:</p>  <pre><code>val cassing = sc.textFile("par.txt").map(line =&gt; line.split(",")).map(p =&gt; (p(0),p(1),p(2),p(3),p(4),p(5)))  scala&gt; cassing.saveToCassandra("cassdb", "party1", SomeColumns("id","first_name","last_name","email","country","ip_address"))  </code></pre>