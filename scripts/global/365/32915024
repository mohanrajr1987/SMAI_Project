<p>I am trying to download hundreds of files, and I am a windows guy. I searched online and found the solution, but get new questions. </p>  <p>Here is what I did:</p>  <ol> <li><p>I put all the urls into a text file, each url a seperate line. The file is called download.txt</p></li> <li><p>In command window, type in </p>  <p>wget -i download.txt</p></li> <li>I am successful in getting the files. </li> </ol>  <p>However, the server looks not very stable, and sometimes I got </p>  <pre><code>Error 500: Internal server error </code></pre>  <p>Then I have to pick out the files that are not downloaded. It is tedious work since the file names are very similar and there are hundreds of them. </p>  <p>My question: Is there any easy way to automatically pick these files out and download them again? Or is there any way to let wget download it again whenever it fails for a file?</p>  <p>Thanks for your help.</p>