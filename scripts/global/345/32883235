<p>Currently I'm working on loading data into a Titan graph with Hadoop (Titan version 0.5.4, Hadoop version 2.6.0). I'm using a single-server (pseudo-distributed) Hadoop cluster, with the purpose of extending to a full cluster with more machines of the same hardware. I'm trying to setup Hadoop in such a way that I get full core utilization. Until now, I though I had made some decent setup with good configuration parameters, but when Hadoop is executing and loads data into the Titan graph, I don't see full utilization of all cores on my machine.</p>  <p>The situation is as follows. The machine I'm using has the following hardware specifications:</p>  <ul> <li>CPU: 32 cores</li> <li>RAM: 256GB</li> <li>Swap memory: 32GB</li> <li>Drives: 8x128GB SSD, 4x2TB HDD</li> </ul>  <p>The data I'm loading into a Titan graph with Hadoop has the following specifications:</p>  <ul> <li>Total size: 848MB</li> <li>Split into four files (487MB, 142MB, 219MB and 1.6MB), each containing vertices of one single type, together with all the vertex properties and outgoing edges.</li> </ul>  <p>While setting up the Hadoop cluster, I tried to use some logic reasoning for setting the configuration parameters of Hadoop to their (what I think is the) optimal setting. See this reasoning below.</p>  <ul> <li>My machine has 32 cores, so in theory I could split up my input size into chuncks of which the size is big enough to end up with around 32 chuncks. So, for 848MB of input, I could set <code>dfs.block.size</code> to 32MB, which would lead to around (848MB / 32MB ~ ) 27 chunks.</li> <li>In order to ensure that each map task receives one chunck, I set the value of <code>mapred.min.split.size</code> to a bit less than the block size, and <code>mapred.max.split.size</code> to a bit more than the block size (for example 30MB and 34MB, respectively).</li> <li>The available memory needed per task is a bit vague for me. For example, I could set <code>mapred.child.java.opts</code> to a value of <code>-Xmx1024m</code> to give each task (e.g. each mapper/reducer) 1GB of memory. Given that my machine has 256GB memory in total - subtracting some from it to reserve for other purposes leaving me around 200GB of memory - I could end up with a total of (200GB / 1GB = ) 200 mappers and reducers. Or, when I give each task 2GB of memory, I would end up with a total of 100 mappers and reducers. The amount of memory given to each task also depends on the input size, I guess. Anyway, this leads to values for <code>mapred.tasktracker.map/reduce.tasks.maximum</code> of around 100, which might already be too much given the fact I have only 32 cores. Therefore, maybe setting this parameter to 32 for both <code>map</code> and <code>reduce</code> might be better? What do you think?</li> </ul>  <p>After these assumptions, I end up with the following configuration.</p>  <p><strong>hdfs-site.xml</strong></p>  <pre><code>&lt;configuration&gt;   &lt;property&gt;     &lt;name&gt;dfs.replication&lt;/name&gt;     &lt;value&gt;1&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;dfs.block.size&lt;/name&gt;     &lt;value&gt;33554432&lt;/value&gt;     &lt;description&gt;Specifies the sizeof data blocks in which the input dataset is split.&lt;/description&gt;   &lt;/property&gt; &lt;/configuration&gt; </code></pre>  <p><strong>mapred-site.xml</strong></p>  <pre><code>&lt;configuration&gt;   &lt;property&gt;     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;     &lt;value&gt;yarn&lt;/value&gt;     &lt;description&gt;The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapred.child.java.opts&lt;/name&gt;     &lt;value&gt;-Xmx2048m&lt;/value&gt;     &lt;description&gt;Java opts for the task tracker child processes.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;     &lt;value&gt;32&lt;/value&gt;     &lt;description&gt;The maximum number of map tasks that will be run simultaneously by a tasktracker.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;     &lt;value&gt;32&lt;/value&gt;     &lt;description&gt;The maximum number of reduce tasks that will be run simultaneously by a tasktracker.&lt;/description&gt;   &lt;/property&gt;     &lt;property&gt;     &lt;name&gt;mapred.min.split.size&lt;/name&gt;     &lt;value&gt;31457280&lt;/value&gt;     &lt;description&gt;The minimum size chunk that map input should be split into.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapred.max.split.size&lt;/name&gt;     &lt;value&gt;35651584&lt;/value&gt;     &lt;description&gt;The maximum size chunk that map input should be split into.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapreduce.job.reduces&lt;/name&gt;     &lt;value&gt;32&lt;/value&gt;     &lt;description&gt;The default number of reducers to use.&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;mapreduce.job.maps&lt;/name&gt;     &lt;value&gt;32&lt;/value&gt;     &lt;description&gt;The default number of maps to use.&lt;/description&gt;   &lt;/property&gt; &lt;/configuration&gt; </code></pre>  <p><strong>yarn-site.xml</strong></p>  <pre><code>&lt;configuration&gt;   &lt;property&gt;     &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;     &lt;value&gt;mapreduce_shuffle&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;     &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;     &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;     &lt;value&gt;2048&lt;/value&gt;     &lt;description&gt;The minimum allocation for every container request at the RM, in MBs.&lt;/description&gt;   &lt;/property&gt; &lt;/configuration&gt; </code></pre>  <p>Executing Hadoop with these settings does not give my full core utilization on my single machine. Not all cores are busy throughout all MapReduce phases. During the Hadoop execution, I also took a look at the IO throughput using the <code>iostat</code> command (<code>iostat -d -x 5 3</code> giving me three reports of 5 second intervals). A sample of such a report is shown below.</p>  <pre><code>Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util sda               0.00     0.07    0.02    0.41     0.29     2.37    12.55     0.01   16.92    5.18   17.43   2.47   0.10 sdb               0.07     2.86    4.90   10.17   585.19  1375.03   260.18     0.04    2.96   23.45    8.55   1.76   2.65 sdc               0.08     2.83    4.89   10.12   585.48  1374.71   261.17     0.07    4.89   30.35    8.12   2.08   3.13 sdd               0.07     2.83    4.89   10.10   584.79  1374.46   261.34     0.04    2.78   26.83    6.71   1.94   2.91 sde               0.00     0.00    0.00    0.00     0.05     0.80   278.61     0.00   10.74    2.55   32.93   0.73   0.00 sdf               0.00     0.00    0.00    0.00     0.05     0.80   283.72     0.00   10.30    1.94   33.09   0.68   0.00 sdg               0.00     0.00    0.00    0.00     0.05     0.80   283.83     0.00   10.24    1.99   32.75   0.68   0.00 sdh               0.00     0.00    0.00    0.00     0.05     0.80   284.13     0.00   10.29    1.96   32.99   0.69   0.00 sdi               0.00     0.00    0.00    0.00     0.05     0.80   284.87     0.00   17.89    2.35   60.33   0.74   0.00 sdj               0.00     0.00    0.00    0.00     0.05     0.80   284.05     0.00   10.30    2.01   32.96   0.68   0.00 sdk               0.00     0.00    0.00    0.00     0.05     0.80   284.44     0.00   10.20    1.99   32.62   0.68   0.00 sdl               0.00     0.00    0.00    0.00     0.05     0.80   284.21     0.00   10.50    2.00   33.71   0.69   0.00 md127             0.00     0.00    0.04    0.01     0.36     6.38   279.84     0.00    0.00    0.00    0.00   0.00   0.00 md0               0.00     0.00   14.92   36.53  1755.46  4124.20   228.57     0.00    0.00    0.00    0.00   0.00   0.00 </code></pre>  <p>I'm no expert in disk utilization, but could these value mean that I'm IO-bound somewhere, for example on disks <em>sdb</em>, <em>sbc</em> or <em>sdd</em>?</p>  <p>Edit: maybe a better indication of CPU utilization and IO throughput can be given by using the <code>sar</code> command. Here are results for 5 reports, 5 seconds aprt (<code>sar -u 5 5</code>):</p>  <pre><code>11:07:45 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle 11:07:50 AM     all     12.77      0.01      0.91      0.31      0.00     86.00 11:07:55 AM     all     15.99      0.00      1.39      0.56      0.00     82.05 11:08:00 AM     all     11.43      0.00      0.58      0.04      0.00     87.95 11:08:05 AM     all      8.03      0.00      0.69      0.48      0.00     90.80 11:08:10 AM     all      8.58      0.00      0.59      0.03      0.00     90.80 Average:        all     11.36      0.00      0.83      0.28      0.00     87.53 </code></pre>  <p>Thanks in advance for any reply!</p>