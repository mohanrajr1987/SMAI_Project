<p>I am using Apache Spark 1.5.1 and trying to connect to a local SQLite database named <code>clinton.db</code>. Creating a data frame from a table of the database works fine but when I do some operations on the created object, I get the error below which says "SQL error or missing database (Connection is closed)". Funny thing is that I get the result of the operation nevertheless. Any idea what I can do to solve the problem, i.e., avoid the error?</p>  <p>Start command for spark-shell:</p>  <pre><code>../spark/bin/spark-shell --master local[8] --jars ../libraries/sqlite-jdbc-3.8.11.1.jar --classpath ../libraries/sqlite-jdbc-3.8.11.1.jar </code></pre>  <p>Reading from the database:</p>  <pre><code>val emails = sqlContext.read.format("jdbc").options(Map("url" -&gt; "jdbc:sqlite:../data/clinton.sqlite", "dbtable" -&gt; "Emails")).load() </code></pre>  <p>Simple count (fails):</p>  <pre><code>emails.count </code></pre>  <p>Error:</p>  <p><code>15/09/30 09:06:39 WARN JDBCRDD: Exception closing statement java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (Connection is closed)     at org.sqlite.core.DB.newSQLException(DB.java:890)     at org.sqlite.core.CoreStatement.internalClose(CoreStatement.java:109)     at org.sqlite.jdbc3.JDBC3Statement.close(JDBC3Statement.java:35)     at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1.org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$anon$$close(JDBCRDD.scala:454)     at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1$$anonfun$8.apply(JDBCRDD.scala:358)     at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1$$anonfun$8.apply(JDBCRDD.scala:358)     at org.apache.spark.TaskContextImpl$$anon$1.onTaskCompletion(TaskContextImpl.scala:60)     at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:79)     at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:77)     at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)     at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:77)     at org.apache.spark.scheduler.Task.run(Task.scala:90)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     at java.lang.Thread.run(Thread.java:745) res1: Long = 7945 </code></p>