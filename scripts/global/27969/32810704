<p>I have a couple of functions and each function is creating logs specific to one transaction; It is a multi-thread application so the function entry to func1 can be random for transaction made but for single transaction it will go via func1, func2 and func3 order only.</p>  <pre><code>func1(transactionId) {      log("%d Now in func1", transactionId); }  func2(transactionId) {      log("%d Now in func2", transactionId); }  func3(transactionId) {      log("%d Now in func3", transactionId); } </code></pre>  <p>Now, I want to write in to logstash at once for each transaction ONLY at a time; that is </p>  <pre><code> 1 Now in func1 Now in func2 Now in fun3 </code></pre>  <p>and then this need to go finally to elasticsearch;</p>  <p>I was thinking of writing half transaction log to RabbitMQ temporary queue and then on completion of complete transaction, I will commit it to RabbitMQ producer queue to send the message to logstash;</p>  <p>Like</p>  <pre><code>func1(transactionId) {      add2RMQ(transactionId, "Now in func1"); }  func2(transactionId) {      add2RMQ("transactionId, "Now in func2"); }  func3(transactionId) {       add2RMQ("transactionId, "Now in func3");       /* Last point of transaction */       commit2RMQ(transactionId); } </code></pre>  <p>The time commit2RMQ execute the logstash should receive the complete message specific to the transaction to write to elasticsearch.</p>  <p>Question:</p>  <ol> <li>What is the right solution to address this problem to send data specific to a transaction at once to elasticsearch?</li> <li>Can we address this with RabbitMQ? If so, what are the right API that I need to use for this?</li> <li>Is there any way that I can achieve the same without RabbitMQ but only with logstash and elasticsearch?</li> <li>I don't want to use elasticsearch update API as it may consume lots of search operation for every log message specific to a transacation. </li> </ol>