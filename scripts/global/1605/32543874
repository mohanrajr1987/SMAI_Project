<p>I'm dealing with the age-old devil of timing resolutions on Linux as part of a homework assignment. To cut a long story short, I'm trying to find the mechanism that can give me the best resolution on an Intel i5 quad-core 3.2 GHz machine running Linux version 2.6.32.</p>  <p>I looked up different methods, and finally am comparing performances of <code>clock_gettime</code> and <code>RDTSC</code>. I'll probably post another question for the latter, but here's what I did to find the average time required for a <code>clock_gettime</code> call:</p>  <pre><code>clock_gettime(CLOCK_MONOTONIC, &amp;ts_start); for(loopindex = 0; loopindex &lt; 1000000; loopindex++)     clock_gettime(CLOCK_MONOTONIC, &amp;ts); clock_gettime(CLOCK_MONOTONIC, &amp;ts_end); </code></pre>  <p><code>ts, ts1, ts2</code> are of type <code>struct timespec</code>. I later calculate differences between <code>ts_end</code> and <code>ts_start</code> and divide by 1000000. This gives me 26ns.</p>  <p>I then thought of trying something else:</p>  <pre><code>clock_gettime(CLOCK_MONOTONIc, &amp;ts1); ts2 = ts1; while(ts1.tv_sec == ts2.tv_sec &amp;&amp; ts1.tv_nsec == ts2.tv_nsec)     clock_gettime(CLOCK_MONOTONIC, &amp;ts2); </code></pre>  <p><code>ts1</code> and <code>ts2</code> are again of type <code>struct timespec</code>. I print the difference in times when the loop terminates. Now, this gives me ~100ns.</p>  <p>I'm confused as to what the right approach is here, and what is the right value. Is there some fundamental point I'm missing in how the times should be interpreted? Also, this got me confused about the term "resolution". Does 26ns/100ns indicate that the actual resolution of <code>clock_gettime</code> on the system is 26ns/100ns?</p>  <p><strong>Edit</strong></p>  <p>I also varied the loop count of the first method and used smaller numbers. The first call takes a long time (~140ns), the second lesser and so on until it evens out at 26ns for all subsequent calls. I'm guessing this is due to caching. Does this affect the correct value?</p>