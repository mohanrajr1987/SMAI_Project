<p>I'm using the Stanford classifier to perform a classification task on tokens in sentences, where each token has to be assigned to one semantic category. </p>  <p>Everything works well at training time, but I have the following problem. When, after training, I try to classify a new instance and this instance has a feature that is never seen in the training set, the classifier will output a score of 0.0 for all the labels, therefore returning always the "first" label in the list as best. In the specific case I'm using the lemma of the word as a feature, doing like this:</p>  <pre><code>Label1    current_pos=NN current_lemma=dog other_feature=X ... Label2    current_pos=PRP current_lemma=his other_feature=Y ... </code></pre>  <p>At test time it is easy to incur in a lemma that was not contained in the training set and this error happens quite often. Is this the desired behaviour? I thought that if a feature is never seen at training time it was just going to be ignored. Indeed, if I remove that feature and pass all the other ones to the classifier the output is no longer identically zero for all labels.</p>  <p>This is the properties file I'm using to run the classifier:</p>  <pre><code>### Model (useNB=false is MaxEnt; useNB=true is NaiveBayes) ### useNB=false  ### Features ###  useClassFeature=false  # The true class labels are given in column 0 (leftmost) goldAnswerColumn=0  1.useSplitWords=true 1.splitWordsRegexp=\\s+  ### MaxEnt parameters ### prior=quadratic intern=true sigma=0.2 useQN=true QNsize=15 tolerance=1e-4 </code></pre>