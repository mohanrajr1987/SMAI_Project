<p>I am building a website (LAMP stack) with an Amazon RDS MySQL instance as the back end (type db.m3.medium).</p>  <p>I am happy with database integrity, and it works perfectly with regards to SELECT/JOIN/ETC queries (everything is normalized, indexed, and foreign keyed, all tables have id primary keys and relevant secondary keys / unique keys).</p>  <p>I have a table 'df_products' with approx half a million products in it. The products need to be updated nightly. The process involves a PHP script reading over a large products data-file and inserting data into several tables (products table, product_colours table, brands table, etc), calling either INSERT or UPDATE depending on whether or not a row already exists. This is done as one giant transaction.</p>  <p>What I am seeing is the UPDATE commands are sufficiently fast (50/sec, not exactly lightning but it should do), however the INSERT commands are super slow (1/sec) and appear to be consuming 100% of the CPU. On a dual core instance we see 50% CPU use (i.e. one full core). I assume that this is because indexes (1x PRIMARY + 5x INDEX + 1x UNIQUE + 1x FULLTEXT) are being rebuilt after every INSERT. However I though that putting the entire process into one transaction should stop indexes being rebuilt until the transaction is committed.</p>  <p>I have tried setting the following params via PHP but there is negligible performance improvement:</p>  <pre><code>$this-&gt;db-&gt;query('SET unique_checks=0'); $this-&gt;db-&gt;query('SET foreign_key_checks=0;'); </code></pre>  <p>The process will take weeks to complete at this rate so we must improve performance. Google appears to suggest using LOAD DATA. However:</p>  <ul> <li>I would have to generate five files in order to populate five tables</li> <li>The process would have to use UPDATE commands as opposed to INSERT since the tables already exist</li> <li>I would still need to loop over the products and scan the database for what values already do and don't exist</li> </ul>  <p>The database is entirely InnoDB and I don't plan to move to MyISAM (I want transactions, foreign keys, etc). This means that I cannot disable indexes. Even if I did it would probably be a big performance drain as we need to check if a row already exists before we insert it, and without an index this will be super slow.</p>  <p>I have provided the products table defition below for information. Can you please provide advice to what process we should be using to achieve faster INSERT/UPDATE on multiple large related tables? Or what optimisations we can make to our existing process?</p>  <p>Thank you,</p>  <pre><code>CREATE TABLE `df_products` (   `id` int(11) NOT NULL AUTO_INCREMENT,   `id_brand` int(11) NOT NULL,   `title` varchar(255) NOT NULL,   `id_gender` int(11) NOT NULL,   `id_colourSet` int(11) DEFAULT NULL,   `id_category` int(11) DEFAULT NULL,   `desc` varchar(500) DEFAULT NULL,   `seoAlias` varchar(255) CHARACTER SET ascii NOT NULL,   `runTimestamp` timestamp NOT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `seoAlias_UNIQUE` (`seoAlias`),   KEY `idx_brand` (`id_brand`),   KEY `idx_category` (`id_category`),   KEY `idx_seoAlias` (`seoAlias`),   KEY `idx_colourSetId` (`id_colourSet`),   KEY `idx_timestamp` (`runTimestamp`),   KEY `idx_gender` (`id_gender`),   FULLTEXT KEY `fulltext_title` (`title`),   CONSTRAINT `fk_id_colourSet` FOREIGN KEY (`id_colourSet`) REFERENCES `df_productcolours` (`id_colourSet`) ON DELETE NO ACTION ON UPDATE NO ACTION,   CONSTRAINT `fk_id_gender` FOREIGN KEY (`id_gender`) REFERENCES `df_lu_genders` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE=InnoDB AUTO_INCREMENT=285743 DEFAULT CHARSET=utf8 </code></pre>