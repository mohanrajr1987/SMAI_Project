<p>It just come across my mind that, for example, suppose we have training data with <code>N</code> points in 2 dimensions. We know that we can always, naively, build a decision tree so that we can classify each data point. (probably we are overfitting, and depth can go to 2<sup>N</sup>)</p>  <p>However, we know that if the data set is linear separable, then the decision tree can take an advantage, probably. Use the above data set as an example, can we determine the upper bound of depth for linear and nonlinear data set? Is it guaranteed that the upper bound of depth for linear case is smaller then nonlinear case?</p>