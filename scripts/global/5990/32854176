<p>I'm using scikit-learn for machine learning similarities between 2 sentences. Therefore, each example in my dataset is a pair of sentences. </p>  <p>For each sentence I calculate a set of features, which I encode in a Python dictionary for vectorization with DictVectorizer. However, I don't know how to distinguish, for each example, the feature vectors of sentence 1 and 2. </p>  <p>What I intend is a scikit learn compatible dataset (for now, I'm using SVC) such as:</p>  <p>s1, s2 -> [ [S1featureA, S1featureB, etc], [S2featureA, S2featureB, etc]]</p>  <p>using the array like notation [] to identify feature vectors and "sub feature vectors", and where: </p>  <p>a) s1 representes the first sentence, and so on; </p>  <p>b) features (may) have the same name or value on both feature vectors. Thus, it is possible that S1featureA = S2featureA, but nevertheless I'd like them separated, so the classifier "sees" 2 sentences per example, each with it's own feature vectors.</p>  <p>Currently, I am only able to design datasets as:</p>  <p>s1, s2 -> [ S1featureA, S2featureB, etc ]</p>  <p>which I vectorize as:</p>  <pre><code># list of dicts, one dict per example traindata = [{'S1featureA': 1, 'S2featureB': 0}, {...}, ...]  v = DictVectorizer() vartrainVect1 = v.fit_transform(traindata) </code></pre>  <p>I tried to design my dataset's dictionary with lists or (sub) dictionaries as values, such as </p>  <pre><code>traindata = [{s1: [{featureA: 1, featureB: 0}], s2: [{featureA: 1, featureB: 0}]}] </code></pre>  <p>and </p>  <pre><code>traindata = [{s1: {featureA: 1, featureB: 0}, s2: {featureA: 1, featureB: 0} }] </code></pre>  <p>but when appying DictVectorizer.fit_transform on these I get an exception saying only dtype convertible value types are ok (ie, strings or numbers).</p>  <p>How do I design and represent a feature vector of feature vectors, for usage with classification algorithms in scikit-learn? Is it possible to employ such a dataset in other algorithms, such as neural networks?</p>  <p>Thanks in advance,</p>  <p>Best,</p>