<p>We have some data and a probabilistic model with latent variables, we want to estimate the posterior distribution after seeing the data. Usually this p(x|z) is hard to compute, so we use variational inference, or MCMC. </p>  <p>What I don't understand is why MCMC plays an essential rule. MCMC can only draw samples. But we may want to fit the model parameters other than just draw samples. For example, for $p(x,\theta|z)$ we may want to fit the parameter $\theta$, only draw samples cannot satisfy our need.</p>  <p>My question is, since MCMC can only draw samples of the posterior, why it is important?</p>