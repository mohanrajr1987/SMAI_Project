<p>I've been trying to build a convolutional neural network that performs classification on the MNIST dataset; I think I'm pretty close to getting everything to work, but I'm still running into some issues I think come from an issue with my backprop implementation.</p>  <p>If I train a small net (conv -> max-pool -> fully-connected -> output) using stochastic gradient descent on a subset (30 images) of the MNIST dataset, it learns all the images in around 5 epochs. However, if I try training a net (conv -> max-pool -> conv -> max-pool -> fully-connected -> output) on such a subset, the  cost function gets stuck at around 2.2 and it doesn't learn.</p>  <p>The only different chunk of code that runs in the 2nd net is backpropagating the error from the 2nd conv layer to the 1st max-pool layer, which is why I suspect that the error comes from here. When I worked through some examples by hand it looked like this was done with a full convolution between the error and weight matrices, so I implemented it in that form (code is below).</p>  <pre><code>#layer is the layer after the current layer self.error = np.zeros(self.conv_output.shape)  #the error coming from the convolutional layer #should have the same shape as this layer's output s_error = np.zeros(self.output.shape)  #for each output map of the current layer, #take the error of each feature map in the next layer, convolve #with its corresponding filter, and sum the contributions to the error for i in range(s_error.shape[2]):     for j in range(layer.error.shape[2]):         s_error[:,:,i] += sig.convolve2d(layer.error[:,:,j], layer.W[:,:,j], mode='full')  #differentiate the activation function s_error = np.multiply(s_error, 1.0 - np.multiply(self.output, self.output))  #propagate errors up through the max-pooling for i in range(0, self.output.shape[0]):     for j in range(0, self.output.shape[1]):         for k in range(0, self.output.shape[2]):             # generalize mapping later             if self.max_indices[i,j,k] == 0:                 self.error[2*i,2*j,k] = s_error[i,j,k]             elif self.max_indices[i,j,k] == 1:                 self.error[2*i,2*j+1,k] = s_error[i,j,k]             elif self.max_indices[i,j,k] == 2:                 self.error[2*i+1,2*j,k] = s_error[i,j,k]             else:                 self.error[2*i+1,2*j+1,k] = s_error[i,j,k] </code></pre>  <p>Does this backpropagate the error correctly? The activations are <code>tanh</code> and the max-pooling is 2x2. </p>  <p>Any help is appreciated. Thanks!</p>