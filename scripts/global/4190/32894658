<p>I have a very large array of values that I am sorting. The array size is ~ <code>(1e9, 3)</code> in <code>float16</code> i.e. <code>dtype='f2'</code> in <code>numpy</code>. So when it is built and sitting there, it takes about 6GB of memory. </p>  <p>I then sort this large array, which runs the memory up 3-4x that level for reasons I don't quite understand. The formula I use to sort it (which should only make a single temporary copy of one column of the array from what I can gather), is the following:</p>  <p><code>arr = arr[arr[:,idx].argsort()]</code></p>  <p>This sort works and doesn't blow up my memory to the point of crashing, though it does spill into virtual memory (I have 16GB memory luckily).</p>  <p>My question is this: the large array is actually composed of 22 smaller arrays that are concatenated together in a sub-function. Is there anything to be gained from pre-sorting each smaller array before building the large array and sorting THAT?</p>