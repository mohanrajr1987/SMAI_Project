<p>My question is theoretical. Let's say that we have one relational database table in a MySQL database with millions of rows. We want to process all the rows but we know that the whole table does not fit in memory. We would choose that we iterate through the records with a general SQL query:</p>  <pre><code>select * from table skip OFFSET limit LIMIT </code></pre>  <p>Which is better/faster/cheaper (for e.g. 10 million record)</p>  <p>A) Choosing small offsets and large limit e.g. in 1000 steps we select 10.000 rows for each step) </p>  <p>B) Choosing large offsets and small limit (e.g. in 10.000 steps we select 1000 rows for each step) ?</p>  <p>I know that <code>skip</code> is expensive anyway but I don't know which strategy is better to choose.</p>  <p><strong>Update</strong>: I'd like to make this question technology independent. So no matters which database we use or what kind of technology. So the question is that how we achive better performance: </p>  <p>A) reading <em>more</em> data in one iteration and we have <em>less</em> iteration steps</p>  <p>B) reading <em>less</em> data in one iteration and we have <em>more</em> iteration steps?</p>