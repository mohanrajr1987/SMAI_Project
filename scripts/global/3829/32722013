<p>My DataFrame 3 fields are account ,month and salary.</p>  <pre><code>account month              Salary 1       201501             10000 2       201506             20000 2       201506             20000 3       201508             30000 3       201508             30000 3       201506             10000 3       201506             10000 3       201506             10000 3       201506             10000 </code></pre>  <p>I am doing groupby on Account and Month and calculating sum of salary for group. Then removing duplicates.</p>  <pre><code>MyDataFrame['salary'] = MyDataFrame.groupby(['account'], ['month'])['salary'].transform(sum) MyDataFrame = MyDataFrame.drop_duplicates() </code></pre>  <p>Expecting output like below:</p>  <pre><code>account month              Salary 1       201501             10000 2       201506             40000 3       201508             60000 3       201506             40000 </code></pre>  <p>It works well for few records. I tried same <strong>for 600 Million records and it is in progress since 4-5 hours</strong>. Initially when I loaded data using pd.read_csv() data acquired 60 GB RAM, till 1-2 hour RAM usages was in between 90 to 120 GB. After 3 hours process is taking 236 GB RAM and it is still running.</p>  <p>Please suggest if any other alternative faster way is available for this.</p>  <p>EDIT: <strong>Now 15 Minutes</strong> in df.groupby(['account', 'month'], sort=False)['Salary'].sum() </p>