<p>I wrote a Python program that fetch all hyperlinks from a webpage and store them in a .txt file.Now what i want to do is to check from that .txt file ,take one URL at a time and open it using urllib.urlopen(url) to continue parsing for hyperlinks. I hope i am asking the write question i am new to Python .</p>  <pre><code>import sys import urllib import urlparse import re from bs4 import BeautifulSoup def process(url):     proxies = {"http":"http://proxy4.nehu.ac.in:3128"}     page = urllib.urlopen(url)     text = page.read()     page.close()     soup = BeautifulSoup(text)     file=open('s.txt','w')      for tag in soup.findAll('a', href=True):         tag['href'] = urlparse.urljoin(url, tag['href'])         print tag['href']         file.write('\n')         file.write(tag['href'])         file.close()         file=open('s.txt','r')         for line in file:             if re.match(ur'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()&lt;&gt;]+|\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\))+(?:\(([^\s()&lt;&gt;]+|(\([^\s()&lt;&gt;]+\)))*\)|[^\s`!()\[\]{};:\'".,&lt;&gt;?\xab\xbb\u201c\u201d\u2018\u2019]))',line):             print line             page = urllib.urlopen(line)             text = page.read()             page.close()             soup = BeautifulSoup(text)            with open('newfile.txt','a') as file2:                 file2.write('\n ----------------- \n')                 file2.write(line)                 file2.write('\n ----------------- \n')                 for tag in soup.findAll('a', href=True):                     tag['href'] = urlparse.urljoin(line, tag['href'])                     print tag['href']                      file2.write('\n')                     file2.write(tag['href'])          file.close()    def main():       if len(sys.argv) == 1:             print 'No url !!'                   sys.exit(1)      for url in sys.argv[1:]:             process(url)   main() </code></pre>