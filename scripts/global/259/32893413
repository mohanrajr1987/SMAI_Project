<p>I'm porting following C code from disassembled x86 binary:</p>  <pre><code>(unsigned __int16) ~ (_WORD) crc32 ^ length * ~crc32; </code></pre>  <p>This is my ported code in C#:</p>  <pre><code>(uint) (~(ushort) crc32) ^ length * ~crc32) </code></pre>  <p>And below there is table with some calculated values.</p>  <pre><code>Expected   | Actual (C#) -----------+------------ 0x1082B9CB | 0x6082B9CB 0x30389AF7 | 0x20389AF7 0xD0EF1CD6 | 0xE0EF1CD6 </code></pre>  <p>As you can see, last three bytes are correct. Just the first byte is little shifted, but I can't see any pattern how it differs one from another.</p>  <p>Any ideas how to fix my C# code so it gives expected output?</p>  <hr>  <p>Example values (C# syntax):</p>  <pre><code>uint crc32 = 0x7601A9C5; int length = 17; </code></pre>  <p>They should evaluate to <code>0xD0EF1CD6</code>, but give <code>0xE0EF1CD6</code> instead.</p>  <hr>  <p>Edit: I probably forgot to mention an important thing. The output is represented as underlying raw bytes in hexadecimal notation.</p>  <p>Working code example: <a href="https://ideone.com/Z8eONJ" rel="nofollow">https://ideone.com/Z8eONJ</a></p>  <pre><code>Console.WriteLine(BitConverter.ToString(BitConverter.GetBytes(checksum))); </code></pre>