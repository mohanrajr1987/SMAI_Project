<p>So I'm implementing a <a href="https://en.wikipedia.org/wiki/Continued_fraction" rel="nofollow">continued fraction</a> library for handling a subset of <a href="https://en.wikipedia.org/wiki/Quadratic_integer" rel="nofollow">quadratic integers</a> and <a href="https://en.wikipedia.org/wiki/Rational_number" rel="nofollow">rational numbers</a>.  Continued fraction terms are represented by unsigned integers.  I've noticed the following general patterns when working with continued fractions:</p>  <ol> <li>Most terms are small single digit numbers, with 1 being the most common.</li> <li>Some terms can be enormous, the largest possible for my application is 366 bits, but these are extremely rare.</li> <li>A large term represents an especially good number approximation, which means there are usually fewer terms overall for a large fraction.</li> <li>The worst possible continued fraction is the golden ratio, and a rational approximation of that with 366 bits of precision corresponds to roughly 525 1's in a row.</li> <li>Random rational numbers do not generally have large runs of the same number, but may have two to four in a row, with 1 again the most common.</li> </ol>  <p>So I have a limit on both the number of terms and the size of a term, with the number of terms roughly inversely proportional to their size.  So storing these terms in machine words or even bytes is usually very wasteful of space, and I still need to handle multi-word arithmetic in the worst case.  Given the roughly inverse relationship between term size and number of terms (which both also depend on the size of the fraction's numerator and denominator), I've been trying to find or come up with a good compression scheme so that I don't waste so much space storing the integer terms.</p>  <p>I've considered <a href="https://en.wikipedia.org/wiki/Huffman_coding" rel="nofollow">Huffman encoding</a>, as the speed of encoding and decoding is nice, but I'm not sure how to come up with the probabilities for the code values.  I have a vague intuition that <a href="https://en.wikipedia.org/wiki/Stern%E2%80%93Brocot_tree" rel="nofollow">Stern-Brocot Trees</a> might offer a hint, as they are <a href="https://en.wikipedia.org/wiki/Binary_tree" rel="nofollow">binary trees</a> with a direct relationship to continued fractions.</p>  <p>Does anyone know of a good compression approach for handling lots of small numbers with occasional huge ones where runs of the same number are typically short (but might be long in rare cases)?  In particular I need to be able to encode and decode fairly fast (say O(n*lg(n)) is the absolute worst speed I can tolerate, with O(n) or better preferable), and be able to get at the position of individual terms so that I know which term number I'm operating on (fourth term, fifth term, etc).</p>  <p>Also, I'm not interested in using any third party real number or continued fraction libraries.  I've looked at several and they are either insufficient or overkill for my needs, and I'd like the experience of coding this up myself for my own edification.</p>  <p><strong><em>Update</em></strong></p>  <p>I've also learned of <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Kuzmin_distribution" rel="nofollow">Gaussâ€“Kuzmin distribution</a> which gives the probability distribution of a particular continued fraction term of <code>k</code> for a random number uniformly distributed between 0 and 1.  It is </p>  <p><code>P(k) = -lg(1.0 - 1.0/((k + 1.0)**2)</code> in Python pseudo-code, where lg is the base 2 logarithm.  This is the limit as <code>k</code> approaches infinity, so my case is somewhat more restricted (though <code>2**366</code> is still huge).  "<a href="http://www.linas.org/math/entropy.pdf" rel="nofollow">The Entropy of Continued Fractions</a>" by Linas Vepstas gives the (information) entropy of Gauss-Kuzmin distribution as approximately 3.43 bits.  My maximum denominator is large enough that the information entropy of my continued fractions is probably close to that limit, though the graph in the linked article shows the limit is approached quite slowly and so is different for relatively small denominators.</p>