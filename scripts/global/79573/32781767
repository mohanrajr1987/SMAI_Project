<p>We're building a system that migrates documents from a different data store into Drive. We'll be doing this for different clients on a regular basis. Therefore, we're interested in performance, because it impacts our customer's experience as well as our time to market in that we need to do testing, and waiting for files to load prolongs each testing cycle. </p>  <p>We have 3 areas of drive interaction</p>  <ol> <li>Create folders (there are many, potentially 30,000+)</li> <li>Upload files (similar in magnitude to the number of folders)</li> <li>Recursively delete a file structure</li> </ol>  <p>In both cases 1 and 2, we run into "User rate limit exceeded" errors with just 2 and 3 threads, respectively. We have an exponential backup policy as suggested that starts at 1 second, and retries 8 times. We're setting the quotaUser on all requests to a random uuid in an attempt to indicate to the server that we don't require user specific rate limiting - but this seems to have had not impact as compared to when we didn't set the quotaUser.</p>  <p>Number 3 currently uses batch queries. 1 and 2 currently use "normal" requests.</p>  <p>I'm looking for guidance on how best to improve the performance of this system.</p>