<p>I am trying to stream live video from my iPhone device to server using RTP. Using AVFoundation's AVCaptureVideoDataOutput, I was able to get CMSampleBuffer for video. I then feed these frames as they arrive into VideoToolBox's VTCompressionSessionEncodeFrame() and is able to get Encoded CMSampleBuffer.</p>  <p>Now to send these encoded Frames via RTP, I came across FFMPEG and found its built library for iOS device. (<a href="https://github.com/kewlbear/FFmpeg-iOS-build-script" rel="nofollow">https://github.com/kewlbear/FFmpeg-iOS-build-script</a>)</p>  <p>However I am not able to find any iOS example or sample code or any documentation that explains the process of sending the encoded frames via RTP for iOS apps. </p>  <p>Is there any existing example or documentation that can explain me how can I send the encoded CMSampleBuffers to server via RTP using FFMPEG.</p>  <p>Thanks in Advance :)</p>