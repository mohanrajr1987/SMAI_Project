<p>I have successfully been running WebRTC in my Android app for a while, using libjingle.so and PeerConnectionClient.java, etc., from Google's code library.  However, I am now running into a problem where a user starts a connection as audio only (i.e., an audio call), but then toggles video on.  I augmented the existing setVideoEnabled() in PeerConnectionClient as such:</p>  <pre><code> public void setVideoEnabled(final boolean enable) { executor.execute(new Runnable() {   @Override   public void run() {     renderVideo = enable;     if (localVideoTrack != null) {       localVideoTrack.setEnabled(renderVideo);     } else {       if (renderVideo) {           //AC: create a video track           String cameraDeviceName = VideoCapturerAndroid.getDeviceName(0);           String frontCameraDeviceName =                   VideoCapturerAndroid.getNameOfFrontFacingDevice();           if (numberOfCameras &gt; 1 &amp;&amp; frontCameraDeviceName != null) {               cameraDeviceName = frontCameraDeviceName;           }           Log.i(TAG, "Opening camera: " + cameraDeviceName);           videoCapturer = VideoCapturerAndroid.create(cameraDeviceName);           if (createVideoTrack(videoCapturer) != null) {                               mediaStream.addTrack(localVideoTrack);               localVideoTrack.setEnabled(renderVideo);               peerConnection.addStream(mediaStream);           } else {               Log.d(TAG, "Local video track is still null");           }       } else {         Log.d(TAG, "Local video track is null");       }     }     if (remoteVideoTrack != null) {       remoteVideoTrack.setEnabled(renderVideo);     } else {       Log.d(TAG,"Remote video track is null");     }   } }); </code></pre>  <p>}</p>  <p>This allows me successfully see a local inset of the device's video camera, but it doesn't send the video to the remove client.  I thought the peerConnection.addStream() call would do that, but perhaps I am missing something else?</p>