<p>I get a NullPointerException in a piece of code which can't throw it. I start thinking to have found a bug in JRE. I am using javac 1.8.0_51 as compiler, and the problem occurs both in jre 1.8.0_45 and the latest 1.8.0_60.</p>  <p>The line throwing the exception is inside a loop, which is inside a closure lambda function. We are running such closure in spark 1.4. The line is executed 1-2 million times, and I get the error not deterministically, with the same input, once every 3 or 4 run.</p>  <p>I'm pasting relevant piece of code here:</p>  <pre><code>        JavaRDD .... mapValues(iterable -&gt; {                 LocalDate[] dates = ...                 long[] dateDifferences = ...                  final double[] fooArray = new double[dates.length];                 final double[] barArray = new double[dates.length];                 for (Item item : iterable) {                     final LocalDate myTime = item.getMyTime();                     final int largerIndex = ...                     if (largerIndex == 0) {                         ...                     } else if (largerIndex &gt;= dates.length - 1) {                         ...                     } else {                         final LocalDate largerDate = dates[largerIndex];                         final long daysBetween = ...                         if (daysBetween == 0) {                             ...                         } else {                             double factor = ...                             // * * * NULL POINTER IN NEXT LINE * * * //                             fooArray[largerIndex - 1] += item.getFoo() * factor;                             fooArray[largerIndex] += item.getFoo() * (1 - factor);                             barArray[largerIndex - 1] += item.getBar() * factor;                             barArray[largerIndex] += item.getBar() * (1 - factor);                         }                     }                 }                 return new NewItem(fooArray, barArray);             })             ... </code></pre>  <p>I started analysing code and found that:</p>  <ul> <li>fooArray is never null since you have "new" few lines above</li> <li>largerIndex is primitive</li> <li>item is never null as it is already used few lines above</li> <li>getFoo() returns double with no unboxing</li> <li>factor is primitive</li> </ul>  <p>I can't run the same input locally and debug it: this is run on a spark cluster. So I added some debug println before the throwing line:</p>  <pre><code>System.out.println("largerIndex: " + largerIndex); System.out.println("foo: " + Arrays.toString(foo)); System.out.println("foo[1]: " + foo[1]); System.out.println("largerIndex-1: " + (largerIndex-1)); System.out.println("foo[largerIndex]: " + foo[largerIndex]); System.out.println("foo[largerIndex - 1]: " + foo[largerIndex - 1]); </code></pre>  <p>And this is the output:</p>  <pre><code>largerIndex: 2 foo: [0.0, 0.0, 0.0, 0.0, ...] foo[1]: 0.0 largerIndex-1: 1 foo[largerIndex]: 0.0 15/10/01 12:36:11 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 17162, host13): java.lang.NullPointerException     at my.class.lambda$mymethod$87560622$1(MyFile.java:150)     at my.other.class.$$Lambda$306/764841389.call(Unknown Source)     at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1027)     ... </code></pre>  <p>So <em>foo[largerIndex - 1]</em> is currently throwing the null-pointer. Note that also the following throws it:</p>  <pre><code>int idx = largerIndex - 1; foo[idx] += ...; </code></pre>  <p>But not the following:</p>  <pre><code>foo[1] += ....; </code></pre>  <p>I gave a look at bytecode in class file and found nothing strange. You correctly have the reference to foo and largerIndex in the stack before iconst_1, isub, and daload.</p>  <p>I'm just posting this to collect ideas before thinking to a jre bug. Does anyone of you experienced same class of problems using spark? or lambda function in general. Is it possible to run jvm with some debug flag to help me understand this strange behavior? Or should I file the issue to someone somewhere?</p>